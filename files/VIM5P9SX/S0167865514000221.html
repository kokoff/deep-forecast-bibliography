<!DOCTYPE html>
<!--[if IE 9]><html class="ie9" lang="en"><![endif]-->
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"><iframe src="javascript:false" title="" style="width: 0px; height: 0px; border: 0px none; display: none;"></iframe><script src="1086177924859222" async=""></script><script async="" src="fbevents.js"></script><script type="text/javascript" async="" src="geo2.js"></script>
      <title>A review of unsupervised feature learning and deep learning for time-series modeling - ScienceDirect</title>
      <meta data-react-helmet="true" name="citation_pii" content="S0167865514000221"><meta data-react-helmet="true" name="citation_issn" content="0167-8655"><meta data-react-helmet="true" name="citation_volume" content="42"><meta data-react-helmet="true" name="citation_lastpage" content="24"><meta data-react-helmet="true" name="citation_publisher" content="North-Holland"><meta data-react-helmet="true" name="citation_firstpage" content="11"><meta data-react-helmet="true" name="citation_journal_title" content="Pattern Recognition Letters"><meta data-react-helmet="true" name="citation_type" content="JOUR"><meta data-react-helmet="true" name="citation_doi" content="10.1016/j.patrec.2014.01.008"><meta data-react-helmet="true" name="dc.identifier" content="10.1016/j.patrec.2014.01.008"><meta data-react-helmet="true" name="citation_article_type" content="Short communication"><meta data-react-helmet="true" name="citation_title" content="A review of unsupervised feature learning and deep learning for time-series modeling"><meta data-react-helmet="true" name="citation_publication_date" content="2014/06/01"><meta data-react-helmet="true" name="citation_online_date" content="2014/01/28"><meta data-react-helmet="true" name="citation_pdf_url" content="/science/article/pii/S0167865514000221/pdfft?md5=47e86d593ea7b0a6e7b001a5ec7d1989&amp;pid=1-s2.0-S0167865514000221-main.pdf"><meta data-react-helmet="true" name="robots" content="INDEX,FOLLOW,NOARCHIVE,NOODP,NOYDIR"><meta data-react-helmet="true" name="viewport" content="initial-scale=1"><meta data-react-helmet="true" name="SDTech" content="Proudly brought to you by the SD Technology team in London, Dayton, and Amsterdam">
      <link data-react-helmet="true" rel="canonical" href="https://www.sciencedirect.com/science/article/pii/S0167865514000221">
      <link rel="shortcut icon" href="https://cdn.els-cdn.com/sd/favSD.ico" type="image/x-icon">
      <link rel="icon" href="https://cdn.els-cdn.com/sd/favSD.ico" type="image/x-icon">
      <link rel="stylesheet" href="arp.css">
      <link rel="stylesheet" href="style.css">
      <link rel="dns-prefetch" href="https://w.usabilla.com/">
      <link rel="dns-prefetch" href="https://www.deepdyve.com/">
      <link rel="dns-prefetch" href="https://smetrics.elsevier.com/">
      <script>
        window.pageTargeting = {"region":"us-east-1","platform":"sdtech","entitled":true};
        window.arp = {
          subscriptions: [],
          subscribe: function(cb) {
            var self = this;
            var i = this.subscriptions.push(cb) - 1;
            return function unsubscribe() {
              self.subscriptions.splice(i, 1);
            }
          },
        };
        window.optimizely = {
          trackEvent: function () {},
          activate: function () {},
          allExperiments: {},
        };
      </script>
      <script type="text/javascript" src="204774041.js"></script>
      <!-- begin mPulse embed code -->
      <script>
        (function(){
          if(window.BOOMR && window.BOOMR.version){return;}
          var dom,doc,where,iframe = document.createElement('iframe'),win = window;

          function boomerangSaveLoadTime(e) {
            win.BOOMR_onload=(e && e.timeStamp) || new Date().getTime();
          }
          if (win.addEventListener) {
            win.addEventListener("load", boomerangSaveLoadTime, false);
          } else if (win.attachEvent) {
            win.attachEvent("onload", boomerangSaveLoadTime);
          }

          iframe.src = "javascript:false";
          iframe.title = ""; iframe.role="presentation";
          (iframe.frameElement || iframe).style.cssText = "width:0;height:0;border:0;display:none;";
          where = document.getElementsByTagName('script')[0];
          where.parentNode.insertBefore(iframe, where);

          try {
            doc = iframe.contentWindow.document;
          } catch(e) {
            dom = document.domain;
            iframe.src="javascript:var d=document.open();d.domain='"+dom+"';void(0);";
            doc = iframe.contentWindow.document;
          }
          doc.open()._l = function() {
            var js = this.createElement("script");
            if(dom) this.domain = dom;
            js.id = "boomr-if-as";
            js.src = 'https://c.go-mpulse.net/boomerang/2FBN2-NKMGU-EJKY8-ZANKZ-SUJZF';
            BOOMR_lstart=new Date().getTime();
            this.body.appendChild(js);
          };
          doc.write('<body onload="document._l();">');
          doc.close();
        })();
      </script>
      <!-- end mPulse embed code -->
    <script src="satellite-5964b08664746d3292014dba.js"></script><script src="s-code-contents-9c0358adbc3b5986e210099b3bf1d427fc5bd286.js"></script><link rel="preload" href="integrator.js"><script type="text/javascript" src="integrator.js"></script><script src="pubads_impl_167.js" async=""></script><link rel="prefetch" href="https://tpc.googlesyndication.com/safeframe/1-0-13/html/container.html"><link rel="prefetch" href="https://securepubads.g.doubleclick.net/static/3p_cookie.html"><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style></head>
    <body><div style="display: none;" id="lightningjs-usabilla_live"><div><iframe id="lightningjs-frame-usabilla_live" frameborder="0"></iframe></div></div>
      <a class="sr-only sr-only-focusable" href="#react-root">Skip to main content</a>
      <!--[if lt IE 9]>
      <div id="ie8Warning" class="warning">
        <script>function ie8click() {
  const node = document.getElementById('ie8Warning');
  document.cookie = 'ie_warning_state=1';
  node.parentNode.removeChild(node);
}</script>
        <p>Please note that Internet Explorer version 8.x is not supported as of January 1, 2016.
        Please refer to <a href="https://blog.sciencedirect.com/posts/have-you-heard-spread-the-word-about-upgrading-your-browser">this blog post</a> for more information.</p>
        <a class="warning-close" onclick="ie8click()" title="Close IE warning">&times;</a>
      </div>
    <![endif]-->
      <header>
        <div class="ccs-component"><div class="header" role="banner"><div id="https-banner" class="hidden"><div class="message">View ScienceDirect over a secure connection: <a id="https-link" class="link" href="https://www.sciencedirect.com/science/article/pii/https">switch to HTTPS</a></div></div><div class="els-header"><div class="els-header-sub-nav-container " id="els-header-sub-nav-container"><div class="els-header-leftside"><div class="els-header-wordmark"><a href="https://www.sciencedirect.com/"><span aria-label="ScienceDirect" class="ccs-icon svg-wordmark-sciencedirect-orange els-main-title"><span class="svg-text">ScienceDirect</span></span></a></div></div><div class="els-header-sub-nav"><ul class="JournalsAndBooks els-header-navigation" role="navigation"><li class="els-header-navigation-section"><a id="ccs-journalsLink" class="link-inherit els-header-navigation-item " href="https://www.sciencedirect.com/science/journals">Journals</a></li><li class="els-header-navigation-section"><a id="ccs-booksLink" class="link-inherit els-header-navigation-item " href="https://www.sciencedirect.com/science/bookbshsrw">Books</a></li></ul><ul class="els-header-icons"><li class="els-header-icons-item els-header-user"><a aria-controls="ccs-profilePopup" aria-expanded="false" id="ccs-profilePopupLink" title="My account" class="link-inherit els-header-user-link" href="https://www.sciencedirect.com/user/login?returnURL=%2Fscience%2Farticle%2Fpii%2FS0167865514000221" aria-label="Profile actions"><span class="signIn_User loggedInUserName"><span id="els-header-user-name" class="els-header-user-name" role="presentation">My account</span><span class="els-header-user-icon-signout svg-person"></span></span></a><div class="SignInOut"><div class="ccs-signout-popup" role="dialog" id="ccs-profilePopup" aria-label="Profile action"><div class="ccs-signin-popup-body-border"><div class="LoggedInUser ccs-signout-body ccs-signout-popup-body"><h3 class="ChangeOrganization ccs-signout-organization"><strong>NESLI University of York - UK, General Access &nbsp;</strong></h3><ul><noscript></noscript></ul><noscript></noscript><ul><div><h3 class="ccs-signout-navigation-title" id="personal-details-desktop">Details &amp; settings</h3><ul aria-labelledby="personal-details-desktop" class="ccs-user_personal_details"><li class="ccs-personal_details"><a rel="nofollow" href="https://www.sciencedirect.com/science?_ob=RegistrationURL&amp;_method=display&amp;_type=shib&amp;_returnURL=%2Fscience%3F_ob%3DFederationURL%26_method%3Dvalidate%26_type%3Df%26_instId%3D530%26fedId%3D12%26uInstName%3DUniversity%2520of%2520York%26md5%3Ddd6b8f47b0213537976c7c3e21dd6517&amp;md5=a17c3e58b68bcd8d1a0d4a312b42d7a9">Personalize my account</a></li><noscript></noscript><li class="ccs-purchase_articles"><a rel="nofollow" href="https://www.sciencedirect.com/science?_ob=PurchaseHistoryURL&amp;_method=displayList&amp;md5=4c705547112eb1005576c22a146c22c8">Purchased articles</a></li></ul></div></ul><ul class="ccs-signout-navigation-list is-visible"><li class="ccs-sign_out"><a id="ccs-signout-id" rel="nofollow" href="https://www.sciencedirect.com/user/logout?targetURL=%2Fscience%2Farticle%2Fpii%2FS0167865514000221">Sign out</a></li></ul></div></div></div></div></li><li class="Help els-header-icons-item"><a id="ccs-help-link" class="link-inherit els-header-help" href="https://service.elsevier.com/app/home/supporthub/sciencedirect/" title="Help" target="_blank"><span aria-label="Help" class="ccs-icon svg-help"><span class="svg-text">Help</span></span></a></li></ul><div class="LibraryBanner" id="library-banner"><div class="customer-banner-container"><div class="customer-banner"><span>Brought to you by:</span><a target="_blank" href="http://www.york.ac.uk/library/" rel="nofollow"><span id="libraryBannerText">University of York</span></a></div></div><div class="els-header-library-close"><a class="ccs-icon svg-delete link-inherit" id="ccs-closeLibraryBanner" aria-label="Close library banner" aria-controls="library-banner" role="button" title="Close"></a></div></div><div id="mobile-menu-parent"><div class="HamburgerButton" id="hamburger-button"><button id="ccs-mobileHamburgerButton" aria-controls="" aria-expanded="false" aria-label="Menu" class="link"><span class="svg-hamburger-closed"></span></button></div><div aria-label="Mobile menu" class="MobileMenu" id="mobile-menu"><div class="mobile-menu-container" id="mobile-menu-container"><div class="mobilemenu-content"><div><div class="primary-mobile-menu"><span class="signIn_User loggedInUserName"><span id="els-header-user-name" class="els-header-user-name" role="presentation">My account</span><span class="els-header-user-icon-signout svg-person"></span></span><h3 class="ChangeOrganization ccs-signout-organization"><strong>NESLI University of York - UK, General Access &nbsp;</strong></h3><ul class="JournalsAndBooks els-header-navigation" role="navigation"><li class="els-header-navigation-section"><a id="mobileccs-journalsLink" class="link-inherit els-header-navigation-item " href="https://www.sciencedirect.com/science/journals">Journals</a></li><li class="els-header-navigation-section"><a id="mobileccs-booksLink" class="link-inherit els-header-navigation-item " href="https://www.sciencedirect.com/science/bookbshsrw">Books</a></li></ul></div><div class="secondary-mobile-menu"><ul><noscript></noscript></ul><noscript></noscript><ul><div><h3 class="ccs-signout-navigation-title" id="personal-details-desktop"><a href="#" aria-expanded="false" aria-controls="details-and-settings-mobile-list" class="link-inherit details-and-settings-mobile-link" id="details-and-settings-mobile-link">Details &amp; settings<span class="ccs-icon svg-sdfe-arrow-down-white"></span></a></h3><ul aria-labelledby="personal-details-desktop" class="ccs-user_personal_details" id="details-and-settings-mobile-list"><li class="ccs-personal_details"><a class="link-inherit" rel="nofollow" href="https://www.sciencedirect.com/science?_ob=RegistrationURL&amp;_method=display&amp;_type=shib&amp;_returnURL=%2Fscience%3F_ob%3DFederationURL%26_method%3Dvalidate%26_type%3Df%26_instId%3D530%26fedId%3D12%26uInstName%3DUniversity%2520of%2520York%26md5%3Ddd6b8f47b0213537976c7c3e21dd6517&amp;md5=a17c3e58b68bcd8d1a0d4a312b42d7a9">Personalize my account</a></li><noscript></noscript><li class="ccs-purchase_articles"><a class="link-inherit" rel="nofollow" href="https://www.sciencedirect.com/science?_ob=PurchaseHistoryURL&amp;_method=displayList&amp;md5=4c705547112eb1005576c22a146c22c8">Purchased articles</a></li></ul></div></ul><ul class="ccs-signout-navigation-list is-visible"><li class="ccs-sign_out"><a class="link-inherit" id="ccs-mobile-signout-id" rel="nofollow" href="https://www.sciencedirect.com/user/logout?targetURL=%2Fscience%2Farticle%2Fpii%2FS0167865514000221">Sign out</a></li></ul><li class="Help els-header-icons-item"><a id="mobileccs-help-link" class="link-inherit els-header-help" href="https://service.elsevier.com/app/home/supporthub/sciencedirect/" title="Help" target="_blank"><span aria-label="Help" class="ccs-icon svg-help"><span class="svg-text">Help</span></span></a></li></div></div></div></div></div></div></div></div></div></div></div>
      </header>
      <div id="react-root" class="react-root"><div data-reactroot="" class="App"><div class="page grid"><div class="row"><div class="col-p-3-3"><section><div class="Article"><!-- react-empty: 7 --><!-- react-empty: 8 --><!-- react-empty: 9 --><!-- react-empty: 10 --><!-- react-empty: 11 --><div class="sticky-outer-wrapper"><div class="sticky-inner-wrapper" style="position: relative; z-index: 1; transform: translate3d(0px, 0px, 0px);"><div class="Toolbar" role="region" aria-label="download options and search"><div class="container"><div class="hide-m hide-t cl-l-3-12">&nbsp;</div><div class="buttons pull-left pad-left"><button class="show-toc-button hide-l hide-d"><svg focusable="false" viewBox="0 0 104 128" height="24" width="19.5" class="icon icon-list"><path d="m2e1 95a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm0-3e1a9 9 0 0 1 -9 9 9 9 0 0 1 -9 -9 9 9 0 0 1 9 -9 9 9 0 0 1 9 9zm14 55h68v1e1h-68zm0-3e1h68v1e1h-68zm0-3e1h68v1e1h-68z"></path></svg><span class="label">Outline</span></button><div class="PdfDownloadButton"><a id="pdfLink" class="pdf-download-btn-link" href="#" aria-controls="Download PDF options"><i class="icon-pdf"></i><span class="pdf-download-label hide-m hide-t">Download PDF</span><span class="pdf-download-label-short hide-l hide-d">Download</span></a></div><div class="ExportCitation"><button class="button ExportCitationButton button-anchor"><span class="button-text">Export</span><svg focusable="false" viewBox="0 0 92 128" height="24" width="17.25" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button></div></div><div class="pull-right pad-right hide-m"><form class="QuickSearch" action="/search" method="get"><input class="query" aria-label="Search ScienceDirect" name="qs" placeholder="Search ScienceDirect" type="text"><button class="button button-primary" type="submit" aria-label="Submit search"><span class="button-text"><svg focusable="false" viewBox="0 0 100 128" height="20" width="18.75" class="icon icon-search"><path d="m19.22 76.91c-5.84-5.84-9.05-13.6-9.05-21.85s3.21-16.01 9.05-21.85c5.84-5.83 13.59-9.05 21.85-9.05 8.25 0 16.01 3.22 21.84 9.05 5.84 5.84 9.05 13.6 9.05 21.85s-3.21 16.01-9.05 21.85c-5.83 5.83-13.59 9.05-21.84 9.05-8.26 0-16.01-3.22-21.85-9.05zm80.33 29.6l-26.32-26.32c5.61-7.15 8.68-15.9 8.68-25.13 0-10.91-4.25-21.17-11.96-28.88-7.72-7.71-17.97-11.96-28.88-11.96s-21.17 4.25-28.88 11.96c-7.72 7.71-11.97 17.97-11.97 28.88s4.25 21.17 11.97 28.88c7.71 7.71 17.97 11.96 28.88 11.96 9.23 0 17.98-3.07 25.13-8.68l26.32 26.32 7.03-7.03"></path></svg></span></button><a class="advanced-search-link" href="https://www.sciencedirect.com/science/search">Advanced</a><input name="origin" value="article" type="hidden"><input name="zone" value="qSearch" type="hidden"></form></div></div></div></div></div><div class="article-wrapper"><div class="hide-m hide-t cl-l-3-12"><div class="TableOfContents" lang="en"><!-- react-empty: 41 --><div class="Outline"><h2>Outline</h2><ol><li><a href="#ab005" title="Abstract"><!-- react-text: 264 -->Abstract<!-- /react-text --></a></li><li><a href="#kg005" title="Keywords"><!-- react-text: 267 -->Keywords<!-- /react-text --></a></li><li><a href="#s0005" title="1. Introduction and background"><!-- react-text: 270 -->1<!-- /react-text --><!-- react-text: 271 -->. <!-- /react-text --><!-- react-text: 272 -->Introduction and background<!-- /react-text --></a></li><li><a href="#s0010" title="2. Properties of time-series data"><!-- react-text: 275 -->2<!-- /react-text --><!-- react-text: 276 -->. <!-- /react-text --><!-- react-text: 277 -->Properties of time-series data<!-- /react-text --></a></li><li><a href="#s0015" title="3. Unsupervised feature learning and deep learning"><!-- react-text: 280 -->3<!-- /react-text --><!-- react-text: 281 -->. <!-- /react-text --><!-- react-text: 282 -->Unsupervised feature learning and deep learning<!-- /react-text --></a></li><li><a href="#s0070" title="4. Classical time-series problems"><!-- react-text: 285 -->4<!-- /react-text --><!-- react-text: 286 -->. <!-- /react-text --><!-- react-text: 287 -->Classical time-series problems<!-- /react-text --></a></li><li><a href="#s0115" title="5. Conclusion"><!-- react-text: 290 -->5<!-- /react-text --><!-- react-text: 291 -->. <!-- /react-text --><!-- react-text: 292 -->Conclusion<!-- /react-text --></a></li><li><a href="#bi005" title="References"><!-- react-text: 295 -->References<!-- /react-text --></a></li></ol><button class="button button-anchor"><span class="button-text">Show full outline</span><svg focusable="false" viewBox="0 0 92 128" height="20" width="17.25" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="Figures"><h2><!-- react-text: 303 -->Figures (<!-- /react-text --><!-- react-text: 304 -->11<!-- /react-text --><!-- react-text: 305 -->)<!-- /react-text --></h2><ol><li><a href="#f0005"><div><img alt="Fig. 1. A 2-layer RBM for static data" src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr1.sml" style="max-width: 219px; max-height: 85px;"></div></a></li><li><a href="#f0010"><div><img alt="Fig. 2. A 2-layer conditional RBM for time-series data" src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr2.sml" style="max-width: 131px; max-height: 163px;"></div></a></li><li><a href="#f0015"><div><img alt="Fig. 3. A 1-layer auto-encoder for static time-series input" src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr3.sml" style="max-width: 177px; max-height: 164px;"></div></a></li><li><a href="#f0020"><div><img alt="Fig. 4. A Recurrent Neural Network (RNN)" src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr4.sml" style="max-width: 159px; max-height: 164px;"></div></a></li><li><a href="#f0025"><div><img alt="Fig. 5. A 2-layer convolutional neural network" src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr5.sml" style="max-width: 219px; max-height: 88px;"></div></a></li><li><a href="#f0030"><div><img alt="Fig. 6. Four images from the KTH action recognition data set of a person running at…" src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr6.sml" style="max-width: 219px; max-height: 59px;"></div></a></li></ol><button class="button button-anchor"><span class="button-text">Show all figures</span><svg focusable="false" viewBox="0 0 92 128" height="20" width="17.25" class="icon icon-navigate-down"><path d="m1 51l7-7 38 38 38-38 7 7-45 45z"></path></svg></button><div class="PageDivider"></div></div><div class="Tables"><h2><!-- react-text: 338 -->Tables (<!-- /react-text --><!-- react-text: 339 -->2<!-- /react-text --><!-- react-text: 340 -->)<!-- /react-text --></h2><ol><li><a href="#t0005" title="A summary of commonly used models for feature learning."><svg focusable="false" viewBox="0 0 98 128" height="24" width="18.375" class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg><!-- react-text: 346 -->Table 1<!-- /react-text --></a></li><li><a href="#t0010" title="A summary of commonly used time-series problems."><svg focusable="false" viewBox="0 0 98 128" height="24" width="18.375" class="icon icon-table"><path d="m54 68h32v32h-32v-32zm-42 0h32v32h-32v-32zm0-42h32v32h-32v-32zm42 0h32v32h-32v-32zm-52 84h94v-94h-94v94z"></path></svg><!-- react-text: 351 -->Table 2<!-- /react-text --></a></li></ol><div class="PageDivider"></div></div><!-- react-empty: 45 --><!-- react-text: 46 -->&nbsp;<!-- /react-text --></div></div><article class="cl-m-3-3 cl-t-6-9 cl-l-6-12 pad-left pad-right" role="main" lang="en"><noscript><div class="alert-message-container" role="alert"><div class="alert-message-body" aria-hidden="true"><span class="Icon IconAlert" style="display: inline-block;"><svg fill="currentColor" tabindex="-1" focusable="false" height="24" width="24" style="width: 100%; height: 100%;"><path d="M11.84 4.63c-.77.05-1.42.6-1.74 1.27-1.95 3.38-3.9 6.75-5.85 10.13-.48.83-.24 1.99.53 2.56.7.6 1.66.36 2.5.41 3.63 0 7.27.01 10.9-.01 1.13-.07 2.04-1.28 1.76-2.39-.1-.58-.56-1.02-.81-1.55-1.85-3.21-3.69-6.43-5.55-9.64-.42-.52-1.06-.83-1.74-.79z" fill="#f80"></path><path d="M11 8h2v5h-2zM11 14h2v2h-2z"></path></svg></span><!-- react-text: 55 -->JavaScript is disabled on your browser. Please enable JavaScript to use all the features on this page.<!-- /react-text --></div></div></noscript><div class="Publication"><div class="publication-brand"><a href="https://www.sciencedirect.com/science/journal/01678655"><img class="publication-brand-image hide-m" src="elsevier-non-solus.png" alt="Elsevier"></a></div><div class="publication-volume"><h2 class="publication-title"><span class="size-xl"><a class="publication-title-link" title="Go to Pattern Recognition Letters on ScienceDirect" href="https://www.sciencedirect.com/science/journal/01678655">Pattern Recognition Letters</a></span></h2><div><span class="size-m"><a title="Go to table of contents for this volume/issue" href="https://www.sciencedirect.com/science/journal/01678655/42/supp/C"><span>Volume 42</span></a><!-- react-text: 68 -->, <!-- /react-text --><!-- react-text: 69 -->1 June 2014<!-- /react-text --><!-- react-text: 70 -->, Pages 11-24<!-- /react-text --></span></div></div><div class="publication-cover"><div class="publication-cover-align"><a href="https://www.sciencedirect.com/science/journal/01678655/42/supp/C"><img src="1-s2.gif" alt="Pattern Recognition Letters" class="publication-cover-image hide-m"></a></div></div></div><div class="Head"><!-- react-empty: 76 --><!-- react-empty: 77 --><h1 class="article-title"><span class="title-text">A review of unsupervised feature learning and deep learning for time-series modeling</span><a name="baep-article-footnote-id12" href="#aep-article-footnote-id12" class="workspace-trigger label">☆</a></h1><!-- react-empty: 81 --><!-- react-empty: 82 --><!-- react-empty: 83 --><!-- react-empty: 84 --><!-- react-empty: 85 --><!-- react-empty: 86 --></div><div class="Banner"><div class="wrapper truncated"><div class="AuthorGroups"><div class="author-group"><span class="sr-only">Author links open overlay panel</span><a class="author size-m workspace-trigger" name="bau005" href="#%21"><span class="content"><span class="text given-name">Martin</span><span class="text surname">Längkvist</span><svg focusable="false" viewBox="0 0 106 128" height="24" width="19.875" class="icon icon-person"><path d="m11.07 1.2e2l0.84-9.29c1.97-18.79 23.34-22.93 41.09-22.93 17.74 0 39.11 4.13 41.08 22.84l0.84 9.38h10.04l-0.93-10.34c-2.15-20.43-20.14-31.66-51.03-31.66s-48.89 11.22-51.05 31.73l-0.91 10.27h10.03m41.93-102.29c-9.72 0-18.24 8.69-18.24 18.59 0 13.67 7.84 23.98 18.24 23.98s18.24-10.31 18.24-23.98c0-9.9-8.52-18.59-18.24-18.59zm0 52.29c-15.96 0-28-14.48-28-33.67 0-15.36 12.82-28.33 28-28.33s28 12.97 28 28.33c0 19.19-12.04 33.67-28 33.67"></path></svg><svg focusable="false" viewBox="0 0 102 128" height="24" width="19.125" class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a><a class="author size-m workspace-trigger" name="bau010" href="#%21"><span class="content"><span class="text given-name">Lars</span><span class="text surname">Karlsson</span><svg focusable="false" viewBox="0 0 102 128" height="24" width="19.125" class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a><a class="author size-m workspace-trigger" name="bau015" href="#%21"><span class="content"><span class="text given-name">Amy</span><span class="text surname">Loutfi</span><svg focusable="false" viewBox="0 0 102 128" height="24" width="19.125" class="icon icon-envelope"><path d="m55.8 57.2c-1.78 1.31-5.14 1.31-6.9 0l-31.32-23.2h69.54l-31.32 23.19zm-55.8-24.78l42.94 32.62c2.64 1.95 6.02 2.93 9.4 2.93s6.78-0.98 9.42-2.93l40.24-30.7v-10.34h-102zm92 56.48l-18.06-22.74-8.04 5.95 17.38 21.89h-64.54l18.38-23.12-8.04-5.96-19.08 24.02v-37.58l-1e1 -8.46v61.1h102v-59.18l-1e1 8.46v35.62"></path></svg></span></a></div></div></div><!-- react-empty: 112 --><button class="ExpandingButton show-details"><svg viewBox="0 0 9 9" class="icon-expand"><path d="M5 7H4V5H2V4h2V2h1v2h2v1H5z"></path><path d="M0 0v9h9V0zm1 1h7v7H1z"></path></svg><!-- react-text: 117 -->Show more<!-- /react-text --></button></div><div class="DoiLink"><a class="doi" href="https://doi.org/10.1016/j.patrec.2014.01.008" target="_blank" aria-label="Persistent link using digital object identifier" title="Persistent link using digital object identifier">https://doi.org/10.1016/j.patrec.2014.01.008</a><a class="rights-and-content" target="_blank" href="https://s100.copyright.com/AppDispatchServlet?publisherName=ELS&amp;contentID=S0167865514000221&amp;orderBeanReset=true">Get rights and content</a></div><!-- react-empty: 121 --><section class="ReferencedArticles"></section><section class="ReferencedArticles"></section><div class="PageDivider"></div><div class="Abstracts"><div class="abstract author" id="ab005" lang="en"><h2 class="section-title">Abstract</h2><div id="as005"><p id="sp005">This
 paper gives a review of the recent developments in deep learning and 
unsupervised feature learning for time-series problems. While these 
techniques have shown promise for modeling static data, such as computer
 vision, applying them to time-series data is gaining increasing 
attention. This paper overviews the particular challenges present in 
time-series data and provides a review of the works that have either 
applied time-series data to unsupervised feature learning algorithms or 
alternatively have contributed to modifications of feature learning 
algorithms to take into account the challenges present in time-series 
data.</p></div></div></div><div class="Keywords"><div id="kg005" class="keywords-section"><h2 class="section-title">Keywords</h2><div id="k0005" class="keyword"><span>Time-series</span></div><div id="k0010" class="keyword"><span>Unsupervised feature learning</span></div><div id="k0015" class="keyword"><span>Deep learning</span></div></div></div><!-- react-text: 139 --><!-- /react-text --><!-- react-empty: 140 --><div class="Body" id="body-mathjax-container"><!-- react-empty: 353 --><!-- react-empty: 354 --><div><section id="s0005"><h2><!-- react-text: 358 -->1<!-- /react-text --><!-- react-text: 359 -->. <!-- /react-text --><!-- react-text: 360 -->Introduction and background<!-- /react-text --></h2><p id="p0005"><!-- react-text: 362 -->Time
 is a natural element that is always present when the human brain is 
learning tasks like language, vision and motion. Most real-world data 
has a temporal component, whether it is measurements of natural 
processes (weather, sound waves) or man-made (stock market, robotics). 
Analysis of time-series data has been the subject of active research for
 decades <!-- /react-text --><a name="bb0330" href="#b0330" class="workspace-trigger">[66,26]</a><!-- react-text: 364 --> and is considered by Yang and Wu <!-- /react-text --><a name="bb0655" href="#b0655" class="workspace-trigger">[131]</a><!-- react-text: 366 -->
 as one of the top 10 challenging problems in data mining due to its 
unique properties. Traditional approaches for modeling sequential data 
include the estimation of parameters from an assumed time-series model, 
such as autoregressive models <!-- /react-text --><a name="bb0415" href="#b0415" class="workspace-trigger">[83]</a><!-- react-text: 368 --> and Linear Dynamical Systems (LDS) <!-- /react-text --><a name="bb0410" href="#b0410" class="workspace-trigger">[82]</a><!-- react-text: 370 -->, and the popular Hidden Markov Model (HMM) <!-- /react-text --><a name="bb0515" href="#b0515" class="workspace-trigger">[103]</a><!-- react-text: 372 -->.
 The estimated parameters can then be used as features in a classifier 
to perform classification. However, more complex, high-dimensional, and 
noisy real-world time-series data cannot be described with analytical 
equations with parameters to solve since the dynamics are either too 
complex or unknown <!-- /react-text --><a name="bb0595" href="#b0595" class="workspace-trigger">[119]</a><!-- react-text: 374 -->
 and traditional shallow methods, which contain only a small number of 
non-linear operations, do not have the capacity to accurately model such
 complex data.<!-- /react-text --></p><p id="p0010"><!-- react-text: 376 -->In
 order to better model complex real-world data, one approach is to 
develop robust features that capture the relevant information. However, 
developing domain-specific features for each task is expensive, 
time-consuming, and requires expertise of the data. The alternative is 
to use unsupervised feature learning <!-- /react-text --><a name="bb0040" href="#b0040" class="workspace-trigger">[8,5,29]</a><!-- react-text: 378 -->
 in order to learn a layer of feature representations from unlabeled 
data. This has the advantage that the unlabeled data, which is plentiful
 and easy to obtain, is utilized and that the features are learned from 
the data instead of being hand-crafted. Another benefit is that these 
layers of feature representations can be stacked to create deep 
networks, which are more capable of modeling complex structures in the 
data. Deep networks have been used to achieve state-of-the-art results 
on a number of benchmark data sets and for solving difficult AI tasks. 
However, much focus in the feature learning community has been on 
developing models for static data and not so much on time-series data.<!-- /react-text --></p><p id="p0015"><!-- react-text: 380 -->In
 this paper we review the variety of feature learning algorithms that 
has been developed to explicitly capture temporal relationships as well 
as the various time-series problems that they have been used on. The 
properties of time-series data will be discussed in Section <!-- /react-text --><a name="bs0010" href="#s0010" class="workspace-trigger">2</a><!-- react-text: 382 --> followed by an introduction to unsupervised feature learning and deep learning in Section <!-- /react-text --><a name="bs0015" href="#s0015" class="workspace-trigger">3</a><!-- react-text: 384 -->. An overview of some common time-series problems and previous work using deep learning is given in Section <!-- /react-text --><a name="bs0070" href="#s0070" class="workspace-trigger">4</a><!-- react-text: 386 -->. Finally, conclusions are given in Section <!-- /react-text --><a name="bs0115" href="#s0115" class="workspace-trigger">5</a><!-- react-text: 388 -->.<!-- /react-text --></p></section><section id="s0010"><h2><!-- react-text: 391 -->2<!-- /react-text --><!-- react-text: 392 -->. <!-- /react-text --><!-- react-text: 393 -->Properties of time-series data<!-- /react-text --></h2><p id="p0020">Time-series
 data consists of sampled data points taken from a continuous, 
real-valued process over time. There are a number of characteristics of 
time-series data that make it different from other types of data.</p><p id="p0025"><!-- react-text: 396 -->Firstly,
 the sampled time-series data often contain much noise and have high 
dimensionality. To deal with this, signal processing techniques such as 
dimensionality reduction techniques, wavelet analysis or filtering can 
be applied to remove some of the noise and reduce the dimensionality. 
The use of feature extraction has a number of advantages <!-- /react-text --><a name="bb0485" href="#b0485" class="workspace-trigger">[97]</a><!-- react-text: 398 -->.
 However, valuable information could be lost and the choice of features 
and signal processing techniques may require expertise of the data.<!-- /react-text --></p><p id="p0030"><!-- react-text: 400 -->The
 second characteristics of time-series data is that it is not certain 
that there are enough information available to understand the process. 
For example, in electronic nose data, where an array of sensors with 
various selectivity for a number of gases are combined to identify a 
particular smell, there is no guarantee that the selection of sensors 
actually are able to identify the target odour. In financial data when 
observing a single stock, which only measures a small aspect of a 
complex system, there is most likely not enough information in order to 
predict the future <!-- /react-text --><a name="bb0150" href="#b0150" class="workspace-trigger">[30]</a><!-- react-text: 402 -->.<!-- /react-text --></p><p id="p0035"><!-- react-text: 404 -->Further, time-series have an explicit dependency on the time variable. Given an input <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">x</mi><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo stretchy="false" is="true">)</mo></mrow></math><!-- react-text: 411 --> at time <!-- /react-text --><em>t</em><!-- react-text: 413 -->, the model predicts <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">y</mi><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo stretchy="false" is="true">)</mo></mrow></math><!-- react-text: 420 -->,
 but an identical input at a later time could be associated with a 
different prediction. To solve this problem, the model either has to 
include more data input from the past or must have a memory of past 
inputs. For long-term dependencies the first approach could make the 
input size too large for the model to handle. Another challenge is that 
the length of the time-dependencies could be unknown.<!-- /react-text --></p><p id="p0040">Many
 time-series are also non-stationary, meaning that the characteristics 
of the data, such as mean, variance, and frequency, changes over time. 
For some time-series data, the change in frequency is so relevant to the
 task that it is more beneficial to work in the frequency-domain than in
 the time-domain.</p><p id="p0045">Finally, there is a difference 
between time-series data and other types of data when it comes to 
invariance. In other domains, for example computer vision, it is 
important to have features that are invariant to translations, 
rotations, and scale. Most features used for time-series need to be 
invariant to translations in time.</p><p id="p0050">In conclusion, 
time-series data is high-dimensional and complex with unique properties 
that make them challenging to analyze and model. There is a large 
interest in representing the time-series data in order to reduce the 
dimensionality and extract relevant information. The key for any 
successful application lies in choosing the right representation. 
Various time-series problems contain different degrees of the properties
 discussed in this section and prior knowledge or assumptions about 
these properties is often infused in the chosen model or feature 
representation. There is an increasing interest in learning the 
representation from unlabeled data instead of using hand-designed 
features. Unsupervised feature learning have shown to be successful at 
learning layers of feature representations for static data sets and can 
be combined with deep networks to create more powerful learning models. 
However, the feature learning for time-series data have to be modified 
in order to adjust for the characteristics of time-series data in order 
to capture the temporal information as well.</p></section><section id="s0015"><h2><!-- react-text: 426 -->3<!-- /react-text --><!-- react-text: 427 -->. <!-- /react-text --><!-- react-text: 428 -->Unsupervised feature learning and deep learning<!-- /react-text --></h2><p id="p0055">This
 section presents both models that are used for unsupervised feature 
learning and models and techniques that are used for modeling temporal 
relations. The advantage of learning features from unlabeled data is 
that the plentiful unlabeled data can be utilized and that potentially 
better features than hand-crafted features can be learned. Both these 
advantages reduce the need for expertise of the data.</p><section id="s0020"><h3><!-- react-text: 432 -->3.1<!-- /react-text --><!-- react-text: 433 -->. <!-- /react-text --><!-- react-text: 434 -->Restricted Boltzmann Machine<!-- /react-text --></h3><div><p id="p0060"><!-- react-text: 437 -->The Restricted Boltzmann Machines (RBM) <!-- /react-text --><a name="bb0265" href="#b0265" class="workspace-trigger">[53,49,76]</a><!-- react-text: 439 --> is a generative probabilistic model between input units (visible), <!-- /react-text --><strong>x</strong><!-- react-text: 441 -->, and latent units (hidden), <!-- /react-text --><strong>h</strong><!-- react-text: 443 -->, see <!-- /react-text --><a name="bf0005" href="#f0005" class="workspace-trigger">Fig. 1</a><!-- react-text: 445 -->. The visible and hidden units are connected with a weight matrix, <!-- /react-text --><strong>W</strong><!-- react-text: 447 --> and have bias vectors <!-- /react-text --><strong>c</strong><!-- react-text: 449 --> and <!-- /react-text --><strong>b</strong><!-- react-text: 451 -->,
 respectively. There are no connections among the visible and hidden 
units. The RBM can be used to model static data. The energy function and
 the joint distribution for a given visible and hidden vector is defined
 as:<!-- /react-text --><span class="display"><div id="e0080" class="formula"><span class="label">(1)</span><math class="math"><mi is="true">E</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">x</mi><mtext is="true">,</mtext><mi is="true" mathvariant="bold">h</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><msup is="true"><mrow is="true"><mi is="true" mathvariant="bold">h</mi></mrow><mrow is="true"><mi is="true">T</mi></mrow></msup><mi is="true" mathvariant="bold">Wx</mi><mo is="true">+</mo><msup is="true"><mrow is="true"><mi is="true" mathvariant="bold">b</mi></mrow><mrow is="true"><mi is="true">T</mi></mrow></msup><mi is="true" mathvariant="bold">h</mi><mo is="true">+</mo><msup is="true"><mrow is="true"><mi is="true" mathvariant="bold">c</mi></mrow><mrow is="true"><mi is="true">T</mi></mrow></msup><mi is="true" mathvariant="bold">v</mi></math></div></span><span class="display"><div id="e0085" class="formula"><span class="label">(2)</span><math class="math"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">x</mi><mtext is="true">,</mtext><mi is="true" mathvariant="bold">h</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">Z</mi></mrow></mfrac><msup is="true"><mrow is="true"><mi is="true" mathvariant="normal">exp</mi></mrow><mrow is="true"><mi is="true">E</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">x</mi><mtext is="true">,</mtext><mi is="true" mathvariant="bold">h</mi><mo stretchy="false" is="true">)</mo></mrow></msup></math></div></span><!-- react-text: 509 -->where <!-- /react-text --><em>Z</em><!-- react-text: 511 -->
 is the partition function that ensures that the distribution is 
normalized. For binary visible and hidden units, the probability that 
hidden unit <!-- /react-text --><math class="math"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">h</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></mrow></math><!-- react-text: 519 --> is activated given visible vector <!-- /react-text --><em>x</em><!-- react-text: 521 --> and the probability that visible unit <!-- /react-text --><math class="math"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math><!-- react-text: 529 --> is activated given hidden vector <!-- /react-text --><em>h</em><!-- react-text: 531 --> are given by:<!-- /react-text --><span class="display"><div id="e0090" class="formula"><span class="label">(3)</span><math class="math"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">h</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true" mathvariant="bold">x</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">σ</mi><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">+</mo><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi></mrow></munder></mstyle><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ij</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></mfenced></mrow></math></div></span><span class="display"><div id="e0095" class="formula"><span class="label">(4)</span><math class="math"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true" mathvariant="bold">h</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">σ</mi><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">+</mo><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi></mrow></munder></mstyle><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ij</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">h</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></mrow></mfenced></mrow></math></div></span><!-- react-text: 614 -->where <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">σ</mi><mo stretchy="false" is="true">(</mo><mo is="true">·</mo><mo stretchy="false" is="true">)</mo></mrow></math><!-- react-text: 621 --> is the activation function. The logistic function, <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">σ</mi><mo stretchy="false" is="true">(</mo><mi is="true">x</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">+</mo><msup is="true"><mrow is="true"><mi is="true">e</mi></mrow><mrow is="true"><mo is="true">-</mo><mi is="true">x</mi></mrow></msup></mrow></mfrac></mrow></math><!-- react-text: 641 -->,
 is a common choice for the activation function. The parameters W, b, 
and v, are trained to minimize the reconstruction error using 
contrastive divergence <!-- /react-text --><a name="bb0250" href="#b0250" class="workspace-trigger">[50]</a><!-- react-text: 643 -->. The learning rule for the RBM is:<!-- /react-text --><span class="display"><div id="e0005" class="formula"><span class="label">(5)</span><math class="math"><mfrac is="true"><mrow is="true"><mi is="true">∂</mi><mi is="true" mathvariant="normal">log</mi><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">x</mi><mo stretchy="false" is="true">)</mo></mrow><mrow is="true"><mi is="true">∂</mi><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ij</mi></mrow></msub></mrow></mfrac><mo is="true">≈</mo><msub is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">h</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></mrow></mfenced></mrow><mrow is="true"><mi is="true" mathvariant="italic">data</mi></mrow></msub><mo is="true">-</mo><msub is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">h</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></mrow></mfenced></mrow><mrow is="true"><mi is="true" mathvariant="italic">recon</mi></mrow></msub></math></div></span><!-- react-text: 697 -->where <!-- /react-text --><math class="math"><mrow is="true"><mrow is="true"><mfenced open="〈" close="〉" is="true"><mrow is="true"><mo is="true">·</mo></mrow></mfenced></mrow></mrow></math><!-- react-text: 704 -->
 is the average value over all training samples. Several RBMs can be 
stacked to produce a deep belief network (DBN). In a deep network, the 
activation of the hidden units in the first layer is the input to the 
second layer.<!-- /react-text --></p><figure id="f0005"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr1.jpg" alt="A 2-layer RBM for static data" height="148"><ol class="links-for-figure"><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr1_lrg.jpg" target="__blank"><!-- react-text: 711 -->Download high-res image<!-- /react-text --><!-- react-text: 712 --> (73KB)<!-- /react-text --></a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr1.jpg" target="__blank">Download full-size image</a></li></ol></span><div class="captions"><span id="cn005"><p id="sp010"><span class="label">Fig. 1</span><!-- react-text: 719 -->. <!-- /react-text --><!-- react-text: 720 -->A 2-layer RBM for static data. The visible units <!-- /react-text --><em>x</em><!-- react-text: 722 --> are fully connected to the first hidden layer <!-- /react-text --><math class="math"><mrow is="true"><msup is="true"><mrow is="true"><mi is="true">h</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msup></mrow></math><!-- react-text: 730 -->.<!-- /react-text --></p></span></div></figure></div></section><section id="s0025"><h3><!-- react-text: 733 -->3.2<!-- /react-text --><!-- react-text: 734 -->. <!-- /react-text --><!-- react-text: 735 -->Conditional RBM<!-- /react-text --></h3><div><p id="p0065"><!-- react-text: 738 -->An extension of RBM that models multivariate time-series data is the conditional RBM (cRBM), see <!-- /react-text --><a name="bf0010" href="#f0010" class="workspace-trigger">Fig. 2</a><!-- react-text: 740 -->. A similar model is the Temporal RBM <!-- /react-text --><a name="bb0570" href="#b0570" class="workspace-trigger">[114]</a><!-- react-text: 742 -->.
 The cRBM consists of auto-regressive weights that model short-term 
temporal structures, and connections between past visible units to the 
current hidden units. The bias vectors in a cRBM depend on previous 
visible units and are defined as:<!-- /react-text --><span class="display"><div id="e0100" class="formula"><span class="label">(6)</span><math class="math"><msubsup is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mo is="true">∗</mo></mrow></msubsup><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">+</mo><mstyle displaystyle="true" is="true"><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">n</mi></mrow></munderover></mstyle><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mi is="true">x</mi><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo is="true">-</mo><mi is="true">i</mi><mo stretchy="false" is="true">)</mo></math></div></span><span class="display"><div id="e0105" class="formula"><span class="label">(7)</span><math class="math"><msubsup is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo is="true">∗</mo></mrow></msubsup><mo is="true">=</mo><msub is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">+</mo><mstyle displaystyle="true" is="true"><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi><mo is="true">=</mo><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">n</mi></mrow></munderover></mstyle><msub is="true"><mrow is="true"><mi is="true">A</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mi is="true">x</mi><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo is="true">-</mo><mi is="true">i</mi><mo stretchy="false" is="true">)</mo></math></div></span><!-- react-text: 821 -->where <!-- /react-text --><math class="math"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">A</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math><!-- react-text: 829 --> is the auto-regressive connections between visible units at time <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">t</mi><mo is="true">-</mo><mi is="true">i</mi></mrow></math><!-- react-text: 835 --> and current visible units, <!-- /react-text --><math class="math"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math><!-- react-text: 843 --> is the weight matrix connecting visible layer at time <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">t</mi><mo is="true">-</mo><mi is="true">i</mi></mrow></math><!-- react-text: 849 --> to the current hidden units. The model order is defined by the constant <!-- /react-text --><em>n</em><!-- react-text: 851 -->. The probabilities for going up or down a layer are:<!-- /react-text --><span class="display"><div id="e0110" class="formula"><span class="label">(8)</span><math class="math"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">h</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true" mathvariant="bold">x</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">σ</mi><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">+</mo><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi></mrow></munder></mstyle><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ij</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">+</mo><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">k</mi></mrow></munder></mstyle><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi></mrow></munder></mstyle><msub is="true"><mrow is="true"><mi is="true">B</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ijk</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo is="true">-</mo><mi is="true">k</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></math></div></span><span class="display"><div id="e0115" class="formula"><span class="label">(9)</span><math class="math"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo stretchy="false" is="true">|</mo><mi is="true" mathvariant="bold">h</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">σ</mi><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">+</mo><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi></mrow></munder></mstyle><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ij</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">h</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">+</mo><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">k</mi></mrow></munder></mstyle><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi></mrow></munder></mstyle><msub is="true"><mrow is="true"><mi is="true">A</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ijk</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo is="true">-</mo><mi is="true">k</mi><mo stretchy="false" is="true">)</mo></mrow></mfenced></mrow></math></div></span><!-- react-text: 990 -->The parameters <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">θ</mi><mo is="true">=</mo><mo stretchy="false" is="true">{</mo><mi is="true">W</mi><mtext is="true">,</mtext><mi is="true">b</mi><mtext is="true">,</mtext><mi is="true">c</mi><mtext is="true">,</mtext><mi is="true">A</mi><mtext is="true">,</mtext><mi is="true">B</mi><mo stretchy="false" is="true">}</mo></mrow></math><!-- react-text: 1006 -->, are trained using contrastive divergence. Just like a RBM, the cRBM can also be used as a module to create deep networks.<!-- /react-text --></p><figure id="f0010"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr2.jpg" alt="A 2-layer conditional RBM for time-series data" height="424"><ol class="links-for-figure"><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr2_lrg.jpg" target="__blank"><!-- react-text: 1013 -->Download high-res image<!-- /react-text --><!-- react-text: 1014 --> (171KB)<!-- /react-text --></a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr2.jpg" target="__blank">Download full-size image</a></li></ol></span><div class="captions"><span id="cn010"><p id="sp015"><span class="label">Fig. 2</span><!-- react-text: 1021 -->. <!-- /react-text --><!-- react-text: 1022 -->A 2-layer conditional RBM for time-series data. The model order for the first and second layer is 3 and 2, respectively.<!-- /react-text --></p></span></div></figure></div></section><section id="s0030"><h3><!-- react-text: 1025 -->3.3<!-- /react-text --><!-- react-text: 1026 -->. <!-- /react-text --><!-- react-text: 1027 -->Gated RBM<!-- /react-text --></h3><p id="p0070"><!-- react-text: 1029 -->The Gated Restricted Boltzmann Machine (GRBM) <!-- /react-text --><a name="bb0440" href="#b0440" class="workspace-trigger">[88]</a><!-- react-text: 1031 --> is another extension of the RBM that models the transition between two input vectors. The GRBM models a weight tensor, <!-- /react-text --><math class="math"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ijk</mi></mrow></msub></mrow></math><!-- react-text: 1039 -->, between the input, <!-- /react-text --><strong>x</strong><!-- react-text: 1041 -->, the output, <!-- /react-text --><strong>y</strong><!-- react-text: 1043 -->, and latent variables, <!-- /react-text --><strong>z</strong><!-- react-text: 1045 -->. The energy function is defined as:<!-- /react-text --><span class="display"><div id="e0010" class="formula"><span class="label">(10)</span><math class="math"><mi is="true">E</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">y</mi><mtext is="true">,</mtext><mi is="true" mathvariant="bold">z</mi><mtext is="true">;</mtext><mi is="true" mathvariant="bold">x</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mo is="true">-</mo><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true" mathvariant="italic">ijk</mi></mrow></munder></mstyle><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ijk</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub><mo is="true">-</mo><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">k</mi></mrow></munder></mstyle><msub is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub><mo is="true">-</mo><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi></mrow></munder></mstyle><msub is="true"><mrow is="true"><mi is="true">c</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub></math></div></span><!-- react-text: 1120 -->where <!-- /react-text --><strong>b</strong><!-- react-text: 1122 --> and <!-- /react-text --><strong>c</strong><!-- react-text: 1124 --> are the bias vectors for <!-- /react-text --><strong>x</strong><!-- react-text: 1126 --> and <!-- /react-text --><strong>y</strong><!-- react-text: 1128 -->, respectively. The conditional probability of the transformation and the output image given the input image is:<!-- /react-text --><span class="display"><div id="e0015" class="formula"><span class="label">(11)</span><math class="math"><mi is="true">p</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">y</mi><mtext is="true">,</mtext><mi is="true" mathvariant="bold">z</mi><mo stretchy="false" is="true">|</mo><mi is="true" mathvariant="bold">x</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mi is="true">Z</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">x</mi><mo stretchy="false" is="true">)</mo></mrow></mfrac><mi is="true" mathvariant="normal">exp</mi><mo stretchy="false" is="true">(</mo><mo is="true">-</mo><mi is="true">E</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">y</mi><mtext is="true">,</mtext><mi is="true" mathvariant="bold">z</mi><mtext is="true">;</mtext><mi is="true" mathvariant="bold">x</mi><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">)</mo></math></div></span><!-- react-text: 1162 -->where <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">Z</mi><mo stretchy="false" is="true">(</mo><mi is="true" mathvariant="bold">x</mi><mo stretchy="false" is="true">)</mo></mrow></math><!-- react-text: 1169 -->
 is the partition function. Luckily, this quantity does not need to be 
computed to perform inference or learning. The probability that hidden 
unit <!-- /react-text --><math class="math"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow></math><!-- react-text: 1177 --> is activated given <!-- /react-text --><strong>x</strong><!-- react-text: 1179 --> and <!-- /react-text --><strong>y</strong><!-- react-text: 1181 --> is given by:<!-- /react-text --><span class="display"><div id="e0020" class="formula"><span class="label">(12)</span><math class="math"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub><mo is="true">=</mo><mn is="true">1</mn><mo stretchy="false" is="true">|</mo><mi is="true" mathvariant="bold">x</mi><mtext is="true">,</mtext><mi is="true" mathvariant="bold">y</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">σ</mi><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true" mathvariant="italic">ij</mi></mrow></munder></mstyle><msub is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ijk</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">+</mo><msub is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub></mrow></mfenced></mrow></math></div></span><!-- react-text: 1232 -->Learning the parameters is performed with an approximation method of the gradient called contrastive divergence <!-- /react-text --><a name="bb0250" href="#b0250" class="workspace-trigger">[50]</a><!-- react-text: 1234 -->. Each latent variable <!-- /react-text --><math class="math"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">z</mi></mrow><mrow is="true"><mi is="true">k</mi></mrow></msub></mrow></math><!-- react-text: 1242 -->
 learns a simple transformation that together are combined the represent
 the full transformation. By fixating a learned transformation <!-- /react-text --><strong>z</strong><!-- react-text: 1244 --> and given an input image <!-- /react-text --><strong>x</strong><!-- react-text: 1246 -->, the output image <!-- /react-text --><strong>y</strong><!-- react-text: 1248 --> is the selected transformation applied to the input image. Similarly, for a fixed input image <!-- /react-text --><strong>x</strong><!-- react-text: 1250 -->, a given image <!-- /react-text --><strong>y</strong><!-- react-text: 1252 --> creates a RBM that learns the transformation <!-- /react-text --><strong>z</strong><!-- react-text: 1254 --> by reconstructing <!-- /react-text --><strong>y</strong><!-- react-text: 1256 -->. These properties could not be achieved with a regular RBM with input units simply being the concatenated images <!-- /react-text --><strong>x</strong><!-- react-text: 1258 --> and <!-- /react-text --><strong>y</strong><!-- react-text: 1260 -->
 since the latent variables would only learn the spatial information for
 that particular image pair and not the general transformation. The 
large number of parameters due to the weight tensor makes it impractical
 for large image sizes. A factored form of the three-way tensor has been
 proposed to reduce the number of parameters to learn <!-- /react-text --><a name="bb0445" href="#b0445" class="workspace-trigger">[89]</a><!-- react-text: 1262 -->.<!-- /react-text --></p></section><section id="s0035"><h3><!-- react-text: 1265 -->3.4<!-- /react-text --><!-- react-text: 1266 -->. <!-- /react-text --><!-- react-text: 1267 -->Auto-encoder<!-- /react-text --></h3><div><p id="p0075"><!-- react-text: 1270 -->A model that does not have a partition function is the auto-encoder <!-- /react-text --><a name="bb0535" href="#b0535" class="workspace-trigger">[107,7,4]</a><!-- react-text: 1272 -->, see <!-- /react-text --><a name="bf0015" href="#f0015" class="workspace-trigger">Fig. 3</a><!-- react-text: 1274 -->.
 The auto-encoder was first introduced as a dimensionality reduction 
algorithm. In fact, a basic linear auto-encoder learns essentially the 
same representation as a Principal Component Analysis (PCA). The layers 
of visible units, <!-- /react-text --><strong>x</strong><!-- react-text: 1276 -->, hidden units, <!-- /react-text --><strong>h</strong><!-- react-text: 1278 -->, and the reconstruction of the visible units, <!-- /react-text --><math class="math"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true" mathvariant="bold">x</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover></mrow></math><!-- react-text: 1286 -->, are connected via weight matrices <!-- /react-text --><math class="math"><mrow is="true"><msup is="true"><mrow is="true"><mi is="true" mathvariant="bold">W</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msup></mrow></math><!-- react-text: 1294 --> and <!-- /react-text --><math class="math"><mrow is="true"><msup is="true"><mrow is="true"><mi is="true" mathvariant="bold">W</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></mrow></math><!-- react-text: 1302 --> and the hidden layer and reconstruction layer have bias vectors <!-- /react-text --><math class="math"><mrow is="true"><msup is="true"><mrow is="true"><mi is="true" mathvariant="bold">b</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msup></mrow></math><!-- react-text: 1310 --> and <!-- /react-text --><math class="math"><mrow is="true"><msup is="true"><mrow is="true"><mi is="true" mathvariant="bold">b</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup></mrow></math><!-- react-text: 1318 -->, respectively. It is common in auto-encoders to have tied weights, that is, <!-- /react-text --><math class="math"><mrow is="true"><msup is="true"><mrow is="true"><mi is="true" mathvariant="bold">W</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">=</mo><msup is="true"><mrow is="true"><mo stretchy="false" is="true">(</mo><msup is="true"><mrow is="true"><mi is="true" mathvariant="bold">W</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msup><mo stretchy="false" is="true">)</mo></mrow><mrow is="true"><mi is="true">T</mi></mrow></msup></mrow></math><!-- react-text: 1338 -->. This works as a regularizer as it constrains the allowed parameter space and reduces the number of parameters to learn <!-- /react-text --><a name="bb0025" href="#b0025" class="workspace-trigger">[5]</a><!-- react-text: 1340 -->. The feed-forward activations are calculated as:<!-- /react-text --><span class="display"><div id="e0120" class="formula"><span class="label">(13)</span><math class="math"><msub is="true"><mrow is="true"><mi is="true">h</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">=</mo><mi is="true">σ</mi><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi></mrow></munder></mstyle><msubsup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ji</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msubsup><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mn is="true">1</mn></mrow></msubsup></mrow></mfenced></mrow></math></div></span><span class="display"><div id="e0125" class="formula"><span class="label">(14)</span><math class="math"><msub is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub><mo is="true">=</mo><mi is="true">σ</mi><mrow is="true"><mfenced open="(" close=")" is="true"><mrow is="true"><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi></mrow></munder></mstyle><msubsup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ij</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup><msub is="true"><mrow is="true"><mi is="true">h</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow></msub><mo is="true">+</mo><msubsup is="true"><mrow is="true"><mi is="true">b</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></msubsup></mrow></mfenced></mrow></math></div></span><!-- react-text: 1425 -->where <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">σ</mi><mo stretchy="false" is="true">(</mo><mo is="true">·</mo><mo stretchy="false" is="true">)</mo></mrow></math><!-- react-text: 1432 -->
 is the activation function. As with the RBM, a common choice is the 
logistic activation function. The cost function to be minimized is 
expressed as:<!-- /react-text --><span class="display"><div id="e0025" class="formula"><span class="label">(15)</span><math class="math"><mi is="true">J</mi><mo stretchy="false" is="true">(</mo><mi is="true">θ</mi><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mfrac is="true"><mrow is="true"><mn is="true">1</mn></mrow><mrow is="true"><mn is="true">2</mn><mi is="true">N</mi></mrow></mfrac><mstyle displaystyle="true" is="true"><munderover is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">n</mi></mrow><mrow is="true"><mi is="true">N</mi></mrow></munderover></mstyle><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi></mrow></munder></mstyle><msup is="true"><mrow is="true"><mo stretchy="false" is="true">(</mo><msubsup is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">)</mo></mrow></msubsup><mo is="true">-</mo><msup is="true"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">i</mi></mrow></msub></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover></mrow><mrow is="true"><mo stretchy="false" is="true">(</mo><mi is="true">n</mi><mo stretchy="false" is="true">)</mo></mrow></msup><mo stretchy="false" is="true">)</mo></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">+</mo><mfrac is="true"><mrow is="true"><mi is="true">λ</mi></mrow><mrow is="true"><mn is="true">2</mn></mrow></mfrac><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">l</mi></mrow></munder></mstyle><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">i</mi></mrow></munder></mstyle><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi></mrow></munder></mstyle><msup is="true"><mrow is="true"><mo stretchy="false" is="true">(</mo><msubsup is="true"><mrow is="true"><mi is="true">W</mi></mrow><mrow is="true"><mi is="true" mathvariant="italic">ij</mi></mrow><mrow is="true"><mi is="true">l</mi></mrow></msubsup><mo stretchy="false" is="true">)</mo></mrow><mrow is="true"><mn is="true">2</mn></mrow></msup><mo is="true">+</mo><mi is="true">β</mi><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">l</mi></mrow></munder></mstyle><mstyle displaystyle="true" is="true"><munder is="true"><mrow is="true"><mo is="true">∑</mo></mrow><mrow is="true"><mi is="true">j</mi></mrow></munder></mstyle><mi is="true" mathvariant="italic">KL</mi><mo stretchy="false" is="true">(</mo><mi is="true">ρ</mi><mo stretchy="false" is="true">|</mo><mo stretchy="false" is="true">|</mo><msubsup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mi is="true">l</mi></mrow></msubsup><mo stretchy="false" is="true">)</mo></math></div></span><!-- react-text: 1557 -->where <!-- /react-text --><math class="math"><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mi is="true">l</mi></mrow></msubsup></mrow></math><!-- react-text: 1567 --> is the mean activation for unit <!-- /react-text --><em>j</em><!-- react-text: 1569 --> in layer <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">l</mi><mtext is="true">,</mtext><mi is="true">ρ</mi></mrow></math><!-- react-text: 1575 --> is the desired mean activation, and <!-- /react-text --><em>N</em><!-- react-text: 1577 --> is the number of training examples. KL is the Kullback–Leibler (KL) divergence which is defined as <!-- /react-text --><math class="math"><mrow is="true"><mi is="true" mathvariant="italic">KL</mi><mo stretchy="false" is="true">(</mo><mi is="true">ρ</mi><mo stretchy="false" is="true">|</mo><mo stretchy="false" is="true">|</mo><msubsup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mi is="true">l</mi></mrow></msubsup><mo stretchy="false" is="true">)</mo><mo is="true">=</mo><mi is="true">ρ</mi><mi is="true" mathvariant="normal">log</mi><mfrac is="true"><mrow is="true"><mi is="true">ρ</mi></mrow><mrow is="true"><msubsup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mi is="true">l</mi></mrow></msubsup></mrow></mfrac><mo is="true">+</mo><mo stretchy="false" is="true">(</mo><mn is="true">1</mn><mo is="true">-</mo><mi is="true">ρ</mi><mo stretchy="false" is="true">)</mo><mi is="true" mathvariant="normal">log</mi><mfrac is="true"><mrow is="true"><mn is="true">1</mn><mo is="true">-</mo><mi is="true">ρ</mi></mrow><mrow is="true"><mn is="true">1</mn><mo is="true">-</mo><msubsup is="true"><mrow is="true"><mi is="true">p</mi></mrow><mrow is="true"><mi is="true">j</mi></mrow><mrow is="true"><mi is="true">l</mi></mrow></msubsup></mrow></mfrac></mrow></math><!-- react-text: 1629 -->.
 The first term is the square root error term that will minimize the 
reconstruction error. The second term is the L2 weight decay term that 
will keep the weight matrices close to zero. Finally, the third term is 
the sparsity penalty term and encourages each unit to only be partially 
activated as specified by the hyperparameter <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">ρ</mi></mrow></math><!-- react-text: 1633 -->.
 The inclusion of these regularization terms prevents the trivial 
learning of a 1-to-1 mapping of the input to the hidden units. A 
difference between auto-encoders and RBMs is that RBMs do not require 
such regularization because the use of stochastic binary hidden units 
acts as a very strong regularizer <!-- /react-text --><a name="bb0255" href="#b0255" class="workspace-trigger">[51]</a><!-- react-text: 1635 -->. However, it is not uncommon to introduce an extra sparsity constraint for RBMs <!-- /react-text --><a name="bb0380" href="#b0380" class="workspace-trigger">[76]</a><!-- react-text: 1637 -->.<!-- /react-text --></p><figure id="f0015"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr3.jpg" alt="A 1-layer auto-encoder for static time-series input" height="333"><ol class="links-for-figure"><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr3_lrg.jpg" target="__blank"><!-- react-text: 1644 -->Download high-res image<!-- /react-text --><!-- react-text: 1645 --> (169KB)<!-- /react-text --></a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr3.jpg" target="__blank">Download full-size image</a></li></ol></span><div class="captions"><span id="cn015"><p id="sp020"><span class="label">Fig. 3</span><!-- react-text: 1652 -->. <!-- /react-text --><!-- react-text: 1653 -->A 1-layer auto-encoder for static time-series input. The input is the concatenation of current and past frames of visible data <!-- /react-text --><em>x</em><!-- react-text: 1655 -->. The reconstruction of <!-- /react-text --><em>x</em><!-- react-text: 1657 --> is denoted <!-- /react-text --><math class="math"><mrow is="true"><mover accent="true" is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mo is="true">ˆ</mo></mrow></mover></mrow></math><!-- react-text: 1665 -->.<!-- /react-text --></p></span></div></figure></div></section><section id="s0040"><h3><!-- react-text: 1668 -->3.5<!-- /react-text --><!-- react-text: 1669 -->. <!-- /react-text --><!-- react-text: 1670 -->Recurrent neural network<!-- /react-text --></h3><div><p id="p0080"><!-- react-text: 1673 -->A model that have been used for modeling sequential data is the Recurrent Neural Network (RNN) <!-- /react-text --><a name="bb0285" href="#b0285" class="workspace-trigger">[57]</a><!-- react-text: 1675 -->. Generally, an RNN is obtained from the feedforward network by connecting the neurons’ output to their inputs, see <!-- /react-text --><a name="bf0020" href="#f0020" class="workspace-trigger">Fig. 4</a><!-- react-text: 1677 -->.
 The short-term time-dependency is modeled by the hidden-to-hidden 
connections without using any time delay-taps. They are usually trained 
iteratively via a procedure known as backpropagation-through-time 
(BPTT). RNNs can be seen as very deep networks with shared parameters at
 each layer when unfolded in time. This results in the problem of 
vanishing gradients <!-- /react-text --><a name="bb0510" href="#b0510" class="workspace-trigger">[102]</a><!-- react-text: 1679 --> and has motivated the exploration of second-order methods for deep architectures <!-- /react-text --><a name="bb0430" href="#b0430" class="workspace-trigger">[86]</a><!-- react-text: 1681 --> and unsupervised pre-training. An overview of strategies for training RNNs is provided by Sutskever <!-- /react-text --><a name="bb0565" href="#b0565" class="workspace-trigger">[113]</a><!-- react-text: 1683 -->. A popular extension is the use of the purpose-built Long-short term memory cell <!-- /react-text --><a name="bb0270" href="#b0270" class="workspace-trigger">[54]</a><!-- react-text: 1685 --> that better finds long-term dependencies.<!-- /react-text --></p><figure id="f0020"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr4.jpg" alt="A Recurrent Neural Network (RNN)" height="370"><ol class="links-for-figure"><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr4_lrg.jpg" target="__blank"><!-- react-text: 1692 -->Download high-res image<!-- /react-text --><!-- react-text: 1693 --> (176KB)<!-- /react-text --></a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr4.jpg" target="__blank">Download full-size image</a></li></ol></span><div class="captions"><span id="cn020"><p id="sp025"><span class="label">Fig. 4</span><!-- react-text: 1700 -->. <!-- /react-text --><!-- react-text: 1701 -->A Recurrent Neural Network (RNN). The input <!-- /react-text --><em>x</em><!-- react-text: 1703 --> is transformed to the output representation <!-- /react-text --><em>y</em><!-- react-text: 1705 --> via the hidden units <!-- /react-text --><em>h</em><!-- react-text: 1707 -->.
 The hidden units have connections from the input values of the current 
time frame and the hidden units from the previous time frame.<!-- /react-text --></p></span></div></figure></div></section><section id="s0045"><h3><!-- react-text: 1710 -->3.6<!-- /react-text --><!-- react-text: 1711 -->. <!-- /react-text --><!-- react-text: 1712 -->Deep learning<!-- /react-text --></h3><p id="p0085"><!-- react-text: 1714 -->The
 models presented in this section use a non-linear activation function 
on the hidden units. This non-linearity enables a more expressive model 
that can learn more abstract representations when multiple modules are 
stacked on top of each other to form a deep network (if linear features 
would be stacked the result would still be a linear operation). The goal
 of a deep network is to build features at the lower layers that will 
disentangle the factors of variations in the input data and then combine
 these representations at the higher layers. It has been proposed that a
 deep network will generalize better because it has a more compact 
representation <!-- /react-text --><a name="bb0370" href="#b0370" class="workspace-trigger">[74]</a><!-- react-text: 1716 -->.
 However, the difficulty with training multiple layers of hidden units 
lies in the problem of vanishing gradients when the error signal is 
backpropagated <!-- /react-text --><a name="bb0045" href="#b0045" class="workspace-trigger">[9]</a><!-- react-text: 1718 -->.
 This can be solved by doing unsupervised greedy layer-wise pre-training
 of each layer. This acts as an unusual form of regularization <!-- /react-text --><a name="bb0145" href="#b0145" class="workspace-trigger">[29]</a><!-- react-text: 1720 --> that avoids poor local minima and gives a better initialization than a random initialization <!-- /react-text --><a name="bb0025" href="#b0025" class="workspace-trigger">[5]</a><!-- react-text: 1722 -->.
 However, the importance of parameter initialization is not as crucial 
as other factors such as input connections and architecture <!-- /react-text --><a name="bb0540" href="#b0540" class="workspace-trigger">[108]</a><!-- react-text: 1724 -->.<!-- /react-text --></p></section><section id="s0050"><h3><!-- react-text: 1727 -->3.7<!-- /react-text --><!-- react-text: 1728 -->. <!-- /react-text --><!-- react-text: 1729 -->Convolution and pooling<!-- /react-text --></h3><div><p id="p0090"><!-- react-text: 1732 -->A
 technique that is particularly interesting for high-dimensional data, 
such as images and time-series data, is convolution. In a convolutional 
setting, the hidden units are not fully connected to the input but 
instead divided into locally connected segments, see <!-- /react-text --><a name="bf0025" href="#f0025" class="workspace-trigger">Fig. 5</a><!-- react-text: 1734 -->. Convolution has been applied to both RBMs and auto-encoders to create convolutional RBMs (convRBM) <!-- /react-text --><a name="bb0390" href="#b0390" class="workspace-trigger">[78,77]</a><!-- react-text: 1736 --> and convolutional auto-encoders (convAE) <!-- /react-text --><a name="bb0435" href="#b0435" class="workspace-trigger">[87]</a><!-- react-text: 1738 -->.
 A Time-Delay Neural Network (TDNN) is a specialization of Artificial 
Neural Networks (ANN) that exploits the time structure of the input by 
performing convolutions on overlapping windows.<!-- /react-text --></p><figure id="f0025"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr5.jpg" alt="A 2-layer convolutional neural network" height="153"><ol class="links-for-figure"><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr5_lrg.jpg" target="__blank"><!-- react-text: 1745 -->Download high-res image<!-- /react-text --><!-- react-text: 1746 --> (118KB)<!-- /react-text --></a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr5.jpg" target="__blank">Download full-size image</a></li></ol></span><div class="captions"><span id="cn025"><p id="sp030"><span class="label">Fig. 5</span><!-- react-text: 1753 -->. <!-- /react-text --><!-- react-text: 1754 -->A 2-layer convolutional neural network.<!-- /react-text --></p></span></div></figure></div><p id="p0095"><!-- react-text: 1756 -->A
 common operator used together with convolution is pooling, which 
combines nearby values in input or feature space through a max, average 
or histogram operator. The purpose of pooling is to achieve invariance 
to small local distortions and reduce the dimensionality of the feature 
space. The work by Lee et al. <!-- /react-text --><a name="bb0385" href="#b0385" class="workspace-trigger">[77]</a><!-- react-text: 1758 --> introduces probabilistic max-pooling in the context of convolutional RBMs. The Space–Time DBN (ST-DBN) <!-- /react-text --><a name="bb0065" href="#b0065" class="workspace-trigger">[13]</a><!-- react-text: 1760 -->
 uses convolutional RBMs together with a spatial pooling layer and a 
temporal pooling layer to build invariant features from spatio-temporal 
data.<!-- /react-text --></p></section><section id="s0055"><h3><!-- react-text: 1763 -->3.8<!-- /react-text --><!-- react-text: 1764 -->. <!-- /react-text --><!-- react-text: 1765 -->Temporal coherence<!-- /react-text --></h3><p id="p0100"><!-- react-text: 1767 -->There
 are a number of other ways besides the architectural structure that can
 be used to capture temporal coherence in data. One way is to introduce a
 smoothness penalty on the hidden variables in the regularization. This 
is done by minimizing the changes in the hidden unit activations from 
one frame to the next by <!-- /react-text --><math class="math"><mrow is="true"><mi is="true" mathvariant="normal">min</mi><mo stretchy="false" is="true">|</mo><mi is="true">h</mi><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo stretchy="false" is="true">)</mo><mo is="true">-</mo><mi is="true">h</mi><mo stretchy="false" is="true">(</mo><mi is="true">t</mi><mo is="true">-</mo><mn is="true">1</mn><mo stretchy="false" is="true">)</mo><mo stretchy="false" is="true">|</mo></mrow></math><!-- react-text: 1784 -->.
 The motivation behind this is that for sequential data the hidden unit 
activations should not change much if the time-dependent data is fed to 
the model in a chronological order. Other strategies include penalizing 
the squared difference, slow feature analysis <!-- /react-text --><a name="bb0640" href="#b0640" class="workspace-trigger">[128]</a><!-- react-text: 1786 -->,
 or as a function of other factors, for example the change in the input 
data in order to adapt to both slow and rapid changing input data.<!-- /react-text --></p><p id="p0105"><!-- react-text: 1788 -->Temporal
 coherence is related to invariant feature representations since both 
methods want to achieve small changes in the feature representation for 
small changes in the input data. It is suggested in <!-- /react-text --><a name="bb0260" href="#b0260" class="workspace-trigger">[52]</a><!-- react-text: 1790 -->
 that the pose parameters and affine transformations should be modeled 
instead of using invariant feature representations. In that case, 
temporal coherence should be over a group of numbers, such as the 
position and pose of the object rather than a single scalar. This could 
for example be achieved using a structured sparsity penalty <!-- /react-text --><a name="bb0325" href="#b0325" class="workspace-trigger">[65]</a><!-- react-text: 1792 -->.<!-- /react-text --></p></section><section id="s0060"><h3><!-- react-text: 1795 -->3.9<!-- /react-text --><!-- react-text: 1796 -->. <!-- /react-text --><!-- react-text: 1797 -->Hidden Markov Model<!-- /react-text --></h3><p id="p0110"><!-- react-text: 1799 -->The Hidden Markov Model (HMM) <!-- /react-text --><a name="bb0515" href="#b0515" class="workspace-trigger">[103]</a><!-- react-text: 1801 -->
 is a popular model for modeling sequential data and is defined by two 
probability distributions. The first one is the transition distribution <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">t</mi><mo is="true">-</mo><mn is="true">1</mn></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math><!-- react-text: 1820 -->, which defines the probability of going from one hidden state <!-- /react-text --><em>y</em><!-- react-text: 1822 --> to the next hidden state. The second one is the observation distribution <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">P</mi><mo stretchy="false" is="true">(</mo><msub is="true"><mrow is="true"><mi is="true">x</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo stretchy="false" is="true">|</mo><msub is="true"><mrow is="true"><mi is="true">y</mi></mrow><mrow is="true"><mi is="true">t</mi></mrow></msub><mo stretchy="false" is="true">)</mo></mrow></math><!-- react-text: 1839 -->, which defines the relation between observed <!-- /react-text --><em>x</em><!-- react-text: 1841 --> values and hidden <!-- /react-text --><em>y</em><!-- react-text: 1843 -->
 states. One assumption is that these distributions are stationary. 
However, the main problem with HMMs are that they require a discrete 
state space, often have unrealistic independence assumptions, and have a
 limited representational capacity of their hidden states <!-- /react-text --><a name="bb0470" href="#b0470" class="workspace-trigger">[94]</a><!-- react-text: 1845 -->. HMMs require <!-- /react-text --><math class="math"><mrow is="true"><msup is="true"><mrow is="true"><mn is="true">2</mn></mrow><mrow is="true"><mi is="true">N</mi></mrow></msup></mrow></math><!-- react-text: 1853 --> hidden states in order to model <!-- /react-text --><em>N</em><!-- react-text: 1855 --> bits of information about the past history.<!-- /react-text --></p></section><section id="s0065"><h3><!-- react-text: 1858 -->3.10<!-- /react-text --><!-- react-text: 1859 -->. <!-- /react-text --><!-- react-text: 1860 -->Summary<!-- /react-text --></h3><div><p id="p0115"><a name="bt0005" href="#t0005" class="workspace-trigger">Table 1</a><!-- react-text: 1864 -->
 gives a summary of the briefly presented models in this section. The 
first column indicates whether the model is capable of capturing 
temporal relations. A model that captures temporal relations does so by 
having a memory of past inputs. The memory of a model, indicated in the 
second column, means how many steps back in time an input have on the 
current frame. Without the temporal order, any permutation of the 
feature sequence would yield the same distribution <!-- /react-text --><a name="bb0280" href="#b0280" class="workspace-trigger">[56]</a><!-- react-text: 1866 -->.
 The implementation of a memory is performed differently between the 
models. In a cRBM, delay taps are used to create a short-term dependency
 on past visible units. The long-term dependency comes from modeling 
subsequent layers. This means that the length of the memory for a cRBM 
is increased for each added layer. The model order for a cRBM in one 
layer is typically below 5 for input sizes around 50. A decrease in the 
input size would allow a higher model order. In an RNN, hidden units in 
the current time frame are affected by the state of the hidden units in 
the previous time frame. This can create a ripple effect with a duration
 of potentially infinite time frames. On the other hand, this ripple 
effect can be prevented by using a forget gate <!-- /react-text --><a name="bb0185" href="#b0185" class="workspace-trigger">[37]</a><!-- react-text: 1868 -->. The use of Long-short term memory <!-- /react-text --><a name="bb0270" href="#b0270" class="workspace-trigger">[54]</a><!-- react-text: 1870 --> or hessian-free optimizer <!-- /react-text --><a name="bb0430" href="#b0430" class="workspace-trigger">[86]</a><!-- react-text: 1872 -->
 can produce recurrent networks that has a memory of over 100 time 
steps. The Gated RBM and the convolutional GRBM models transitions 
between pairs of input vectors so the memory for these models is 2. The 
Space–Time DBN <!-- /react-text --><a name="bb0065" href="#b0065" class="workspace-trigger">[13]</a><!-- react-text: 1874 --> models 6 sequences of outputs from the spatial pooling layer, which is a longer memory than GRBM, but using a lower input size.<!-- /react-text --></p><div class="tables frame-topbot rowsep-0 colsep-0" id="t0005"><div class="captions"><span id="cn060"><p id="sp065"><span class="label">Table 1</span><!-- react-text: 1880 -->. <!-- /react-text --><!-- react-text: 1881 -->A summary of commonly used models for feature learning.<!-- /react-text --></p></span></div><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col" class="align-left">Method</th><th scope="col" class="align-left">Temporal relation</th><th scope="col" class="align-left">Memory</th><th scope="col" class="align-left">Typical input size</th><th scope="col" class="align-left">Generative</th></tr></thead><tbody><tr class="valign-top"><td class="align-left">RBM</td><td class="align-left">–</td><td class="align-left">–</td><td class="align-left">10–1000</td><td class="align-left">✓</td></tr><tr class="valign-top"><td class="align-left">AE</td><td class="align-left">–</td><td class="align-left">–</td><td class="align-left">10–1000</td><td class="align-left">–</td></tr><tr class="valign-top"><td class="align-left">RNN</td><td class="align-left">✓</td><td class="align-left">1–100</td><td class="align-left">50–1000</td><td class="align-left">✓</td></tr><tr class="valign-top"><td class="align-left">cRBM</td><td class="align-left">✓</td><td class="align-left">2–5</td><td class="align-left">50</td><td class="align-left">✓</td></tr><tr class="valign-top"><td class="align-left">TDNN</td><td class="align-left">✓</td><td class="align-left">2–5</td><td class="align-left">5–50</td><td class="align-left">–</td></tr><tr class="valign-top"><td class="align-left">ANN</td><td class="align-left">–</td><td class="align-left">–</td><td class="align-left">10–1000</td><td class="align-left">–</td></tr><tr class="valign-top"><td class="align-left">GRBM</td><td class="align-left">✓</td><td class="align-left">2</td><td class="align-left"><!-- react-text: 1933 -->&lt;64<!-- /react-text --><!-- react-text: 1934 -->&nbsp;<!-- /react-text --><!-- react-text: 1935 -->×<!-- /react-text --><!-- react-text: 1936 -->&nbsp;<!-- /react-text --><!-- react-text: 1937 -->64<!-- /react-text --></td><td class="align-left">✓</td></tr><tr class="valign-top"><td class="align-left">ConvGRBM</td><td class="align-left">✓</td><td class="align-left">2</td><td class="align-left"><!-- react-text: 1944 -->&gt;64<!-- /react-text --><!-- react-text: 1945 -->&nbsp;<!-- /react-text --><!-- react-text: 1946 -->×<!-- /react-text --><!-- react-text: 1947 -->&nbsp;<!-- /react-text --><!-- react-text: 1948 -->64<!-- /react-text --></td><td class="align-left">✓</td></tr><tr class="valign-top"><td class="align-left">ConvRBM</td><td class="align-left">–</td><td class="align-left">–</td><td class="align-left"><!-- react-text: 1955 -->&gt;64<!-- /react-text --><!-- react-text: 1956 -->&nbsp;<!-- /react-text --><!-- react-text: 1957 -->×<!-- /react-text --><!-- react-text: 1958 -->&nbsp;<!-- /react-text --><!-- react-text: 1959 -->64<!-- /react-text --></td><td class="align-left">✓</td></tr><tr class="valign-top"><td class="align-left">ConvAE</td><td class="align-left">–</td><td class="align-left">–</td><td class="align-left"><!-- react-text: 1966 -->&gt;64<!-- /react-text --><!-- react-text: 1967 -->&nbsp;<!-- /react-text --><!-- react-text: 1968 -->×<!-- /react-text --><!-- react-text: 1969 -->&nbsp;<!-- /react-text --><!-- react-text: 1970 -->64<!-- /react-text --></td><td class="align-left">-</td></tr><tr class="valign-top"><td class="align-left">ST-DBN</td><td class="align-left">✓</td><td class="align-left">2–6</td><td class="align-left"><!-- react-text: 1977 -->10<!-- /react-text --><!-- react-text: 1978 -->&nbsp;<!-- /react-text --><!-- react-text: 1979 -->×<!-- /react-text --><!-- react-text: 1980 -->&nbsp;<!-- /react-text --><!-- react-text: 1981 -->10<!-- /react-text --></td><td class="align-left">✓</td></tr></tbody></table></div></div></div><p id="p0120"><!-- react-text: 1984 -->The last column in <!-- /react-text --><a name="bt0005" href="#t0005" class="workspace-trigger">Table 1</a><!-- react-text: 1986 -->
 indicates if the model is generative (as opposed to discriminative). A 
generative model can generate observable data given a hidden 
representation and this ability is mostly used for generating synthetic 
data of future time steps. Even though the auto-encoder is not 
generative, a probabilistic interpretation can be made using 
auto-encoder scoring <!-- /react-text --><a name="bb0315" href="#b0315" class="workspace-trigger">[63,10]</a><!-- react-text: 1988 -->.<!-- /react-text --></p><p id="p0125"><!-- react-text: 1990 -->For
 selecting a model for a particular problem, a number of questions 
should be taken into consideration: (1) Use a generative or 
discriminative model? (2) What are the properties of the data? and (3) 
How large is the input size? A generative model is preferred if the 
trained model should be used for synthesizing new data or prediction 
tasks where partial input data (data at <!-- /react-text --><math class="math"><mrow is="true"><mi is="true">t</mi><mo is="true">+</mo><mn is="true">1</mn></mrow></math><!-- react-text: 1996 -->)
 need to be reconstructed. If the task is to do classification, a 
discriminative model is sufficient. A discriminative model will attempt 
to model the training data even if that data is noisy while a generative
 model will simply assign a low probability for outliers. This makes a 
generative model more robust for noisy inputs and a better outlier 
detector. There is also the factor of training time. Generative models 
use Gibbs sampling to approximate the derivatives for each parameter 
update while a discriminative model calculates the exact gradients in 
one iteration. However, if the simulation time is an issue, it is a good
 idea to look for hardware solutions or the choice of optimization 
method before considering which method is the fastest. When the 
combination of input size, model parameters, and number of training 
examples in one training batch is large, the training time could be 
decreased by performing the parameter updates on a GPU instead of the 
CPU. For large-scale problems, i.e., the number of training examples is 
large, it is recommended to use stochastic gradient descent instead of 
L-BFGS or conjugate gradient descent as optimization method <!-- /react-text --><a name="bb0075" href="#b0075" class="workspace-trigger">[15]</a><!-- react-text: 1998 -->.
 Furthermore, if the data has a temporal structure it is not recommended
 to treat the input data as a feature vector since this will discard the
 temporal information. Instead, a model that inherently models temporal 
relations or incorporates temporal coherence (by regularization or 
temporal pooling) in a static model is a better approach. For 
high-dimensional problems, like images which have a pictorial structure,
 it may be appropriate to use convolution. The use of pooling further 
decreases the number of dimensions and introduces invariance for small 
translations of the input data.<!-- /react-text --></p></section></section><section id="s0070"><h2><!-- react-text: 2001 -->4<!-- /react-text --><!-- react-text: 2002 -->. <!-- /react-text --><!-- react-text: 2003 -->Classical time-series problems<!-- /react-text --></h2><div><p id="p0130"><!-- react-text: 2006 -->In
 this section we will highlight some common time-series problems and the
 models that have been used to address them in the literature. We will 
focus on complex problems that require the use of models with hidden 
variables for feature representation and where the representations are 
fully or partially learned from unlabeled data. A summary of the 
classical time-series problems that will be presented in this section is
 given in <!-- /react-text --><a name="bt0010" href="#t0010" class="workspace-trigger">Table 2</a><!-- react-text: 2008 -->.<!-- /react-text --></p><div class="tables frame-topbot rowsep-0 colsep-0" id="t0010"><div class="captions"><span id="cn065"><p id="sp070"><span class="label">Table 2</span><!-- react-text: 2014 -->. <!-- /react-text --><!-- react-text: 2015 -->A summary of commonly used time-series problems.<!-- /react-text --></p></span></div><div class="groups"><table><thead><tr class="rowsep-1 valign-top"><th scope="col" class="align-left">Problem</th><th scope="col" class="align-left">Multi-variate</th><th scope="col" class="align-left">Raw data</th><th scope="col" class="align-left">Frequency rich</th><th scope="col" class="align-left">Common features</th><th scope="col" class="align-left">Common method</th><th scope="col" class="align-left">Benchmark set</th></tr></thead><tbody><tr class="valign-top"><td class="align-left">Stock prediction</td><td class="align-left">–</td><td class="align-left">✓</td><td class="align-left">–</td><td class="align-left">–</td><td class="align-left">ANN</td><td class="align-left">DJIA</td></tr><tr class="valign-top"><td class="align-left">Video</td><td class="align-left">✓</td><td class="align-left">✓</td><td class="align-left">–</td><td class="align-left">SIFT, HOG</td><td class="align-left">ConvRBM</td><td class="align-left">KTH</td></tr><tr class="valign-top"><td class="align-left">Speech Recognition</td><td class="align-left">–</td><td class="align-left">(✓)</td><td class="align-left">✓</td><td class="align-left">MFCC</td><td class="align-left">RBM, RNN</td><td class="align-left">TIMIT</td></tr><tr class="valign-top"><td class="align-left">Music recognition</td><td class="align-left">✓</td><td class="align-left">–</td><td class="align-left">✓</td><td class="align-left">Chroma, MFCC</td><td class="align-left">ConvRBM</td><td class="align-left">GTZAN</td></tr><tr class="valign-top"><td class="align-left">Motion capture</td><td class="align-left">✓</td><td class="align-left">✓</td><td class="align-left">–</td><td class="align-left">–</td><td class="align-left">cRBM</td><td class="align-left">CMU</td></tr><tr class="valign-top"><td class="align-left">E-nose</td><td class="align-left">✓</td><td class="align-left">✓</td><td class="align-left">–</td><td class="align-left">Many</td><td class="align-left">TDNN</td><td class="align-left">–</td></tr><tr class="valign-top"><td class="align-left">Physiological data</td><td class="align-left">✓</td><td class="align-left">(✓)</td><td class="align-left">✓</td><td class="align-left">Many, spectogram</td><td class="align-left">RBM, AE</td><td class="align-left">PhysioNET</td></tr></tbody></table></div></div></div><section id="s0075"><h3><!-- react-text: 2086 -->4.1<!-- /react-text --><!-- react-text: 2087 -->. <!-- /react-text --><!-- react-text: 2088 -->Videos<!-- /react-text --></h3><div><p id="p0135"><!-- react-text: 2091 -->Video
 data are series of images over time (spatio-temporal data) and can 
therefore be viewed as high-dimensional time-series data. <!-- /react-text --><a name="bf0030" href="#f0030" class="workspace-trigger">Fig. 6</a><!-- react-text: 2093 --> shows a sequence of images from the KTH activity recognition data set.<!-- /react-text --><a name="bfn1" href="#fn1" class="workspace-trigger"><sup>1</sup></a><!-- react-text: 2096 -->
 The traditional approach to modeling video streams is to treat each 
individual static image and detecting interesting points using common 
feature detectors such as SIFT <!-- /react-text --><a name="bb0405" href="#b0405" class="workspace-trigger">[81]</a><!-- react-text: 2098 --> or HOG <!-- /react-text --><a name="bb0120" href="#b0120" class="workspace-trigger">[24]</a><!-- react-text: 2100 -->. These features are domain-specific for static images and are not easily extended to other domains such as video <!-- /react-text --><a name="bb0365" href="#b0365" class="workspace-trigger">[73]</a><!-- react-text: 2102 -->.<!-- /react-text --></p><figure id="f0030"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr6.jpg" alt="Four images from the KTH action recognition data set of a person running at…" height="144"><ol class="links-for-figure"><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr6_lrg.jpg" target="__blank"><!-- react-text: 2109 -->Download high-res image<!-- /react-text --><!-- react-text: 2110 --> (172KB)<!-- /react-text --></a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr6.jpg" target="__blank">Download full-size image</a></li></ol></span><div class="captions"><span id="cn030"><p id="sp035"><span class="label">Fig. 6</span><!-- react-text: 2117 -->. <!-- /react-text --><!-- react-text: 2118 -->Four
 images from the KTH action recognition data set of a person running at 
frame 100, 105, 110, and 115. The KTH data set also contains videos of 
walking, jogging, boxing, hand waving, and handclapping.<!-- /react-text --></p></span></div></figure></div><p id="p0140"><!-- react-text: 2120 -->The approach taken by Stavens and Thrun <!-- /react-text --><a name="bb0555" href="#b0555" class="workspace-trigger">[111]</a><!-- react-text: 2122 -->
 learns its own domain-optimized features instead of using pre-defined 
features, but still from static images. A better approach to modeling 
videos is to learn image transitions instead of working with static 
images. A Gated Restricted Boltzmann Machine (GRBM) <!-- /react-text --><a name="bb0440" href="#b0440" class="workspace-trigger">[88]</a><!-- react-text: 2124 --> has been used for this purpose where the input, <!-- /react-text --><em>x</em><!-- react-text: 2126 -->, of the GRBM is the full image in one time frame and the output <!-- /react-text --><em>y</em><!-- react-text: 2128 -->
 is the full image in the subsequent time frame. However, since the 
network is fully connected to the image the method does not scale well 
to larger images and local transformations at multiple locations must be
 re-learned.<!-- /react-text --></p><p id="p0145"><!-- react-text: 2130 -->A convolutional version of the GRBM using probabilistic max-pooling is presented by Taylor et al. <!-- /react-text --><a name="bb0580" href="#b0580" class="workspace-trigger">[116]</a><!-- react-text: 2132 -->.
 The use of convolution reduces the number of parameters to learn, 
allows for larger input sizes, and better handles the local affine 
transformations that can appear anywhere in the image. The model was 
validated on synthetic data and a number of benchmark data sets, 
including the KTH activity recognition data set.<!-- /react-text --></p><p id="p0150"><!-- react-text: 2134 -->The work by Le et al. <!-- /react-text --><a name="bb0365" href="#b0365" class="workspace-trigger">[73]</a><!-- react-text: 2136 --> presents an unsupervised spatio-temporal feature learning method using an extension of Independent Subspace Analysis (ISA) <!-- /react-text --><a name="bb0300" href="#b0300" class="workspace-trigger">[60]</a><!-- react-text: 2138 -->.
 The extensions include hierarchical (stacked) convolutional ISA modules
 together with pooling. A disadvantage of ISA is that it does not scale 
well to large input sizes. The inclusion of convolution and stacking 
solves this problem by learning on smaller patches of input data. The 
method is validated on a number of benchmark sets, including KTH. One 
advantage of the method is that the use of ISA reduces the need for 
tweaking many of the hyperparameters seen in RBM-based methods, such as 
learning rate, weight decay, convergence parameters, etc.<!-- /react-text --></p><p id="p0155"><!-- react-text: 2140 -->Modeling temporal relations in video have also been done using temporal pooling. The work by Chen and de Freitas <!-- /react-text --><a name="bb0065" href="#b0065" class="workspace-trigger">[13]</a><!-- react-text: 2142 -->
 uses convolutional RBMs as building blocks for spatial pooling and then
 performs temporal pooling on the spatial pooling units. The method is 
called Space–Time Deep Belief Network (ST-DBN). The ST-DBN allows for 
invariance and statistical dependencies in both space and time. The 
method achieved superior performance on applications such as action 
recognition and video denoising when compared to a standard 
convolutional DBN.<!-- /react-text --></p><p id="p0160"><!-- react-text: 2144 -->The use of temporal coherence for modeling videos is done by Zou et al. <!-- /react-text --><a name="bb0675" href="#b0675" class="workspace-trigger">[135]</a><!-- react-text: 2146 -->,
 where an auto-encoder with a L1-cost on the temporal difference on the 
pooling units is used to learn features that improve object recognition 
on still images. The work by Hyvärinen <!-- /react-text --><a name="bb0290" href="#b0290" class="workspace-trigger">[58]</a><!-- react-text: 2148 --> also uses temporal information as a criterion for learning representations.<!-- /react-text --></p><p id="p0165">The
 use of deep learning, feature learning, and convolution with pooling 
has propelled the advances in video processing. Modeling streams of 
video is a natural continuation for deep learning algorithms since they 
have already been shown to be successful at building useful features 
from static images. By focusing on learning temporal features in videos,
 the performance on static images can be improved, which motivates the 
need for continuing developing deep learning algorithms that capture 
temporal relations. The early attempts at extending deep learning 
algorithms to video data was done by modeling the transition between two
 frames. The use of temporal pooling extends the time-dependencies a 
model can learn beyond a single frame transition. However, the 
time-dependency that has been modeled is still just a few frames. A 
possible future direction for video processing is to look at models that
 can learn longer time-dependencies.</p></section><section id="s0080"><h3><!-- react-text: 2152 -->4.2<!-- /react-text --><!-- react-text: 2153 -->. <!-- /react-text --><!-- react-text: 2154 -->Stock market prediction<!-- /react-text --></h3><div><p id="p0170"><!-- react-text: 2157 -->Stock
 market data are highly complex and difficult to predict, even for human
 experts, due to a number of external factors, e.g., politics, global 
economy, and trader expectation. The trends in stock market data tend to
 be nonlinear, uncertain, and non-stationary. <!-- /react-text --><a name="bf0035" href="#f0035" class="workspace-trigger">Fig. 7</a><!-- react-text: 2159 --> shows the Dow Jones Industrial Average (DJOI) over a decade. According to the Efficient Market Hypothesis (EMH) <!-- /react-text --><a name="bb0150" href="#b0150" class="workspace-trigger">[30]</a><!-- react-text: 2161 -->,
 stock market prices follow a random walk pattern, meaning that a stock 
has the same probability to go up as it has to go down, resulting in 
that predictions can not have more than 50% accuracy <!-- /react-text --><a name="bb0605" href="#b0605" class="workspace-trigger">[121]</a><!-- react-text: 2163 -->.
 The EMH state that stock prices are largely driven by “news” rather 
than present and past prices. However, it has also been argued that 
stock market prices do not follow a random walk and that they can be 
predicted <!-- /react-text --><a name="bb0420" href="#b0420" class="workspace-trigger">[84]</a><!-- react-text: 2165 -->.
 The landscape for acquiring both news and stock information looks very 
different today than it did decades ago. As an example, it has been 
shown that predicted stock prices can be improved if further information
 is extracted from online social media, such as Twitter feeds <!-- /react-text --><a name="bb0070" href="#b0070" class="workspace-trigger">[14]</a><!-- react-text: 2167 --> and online chat activity <!-- /react-text --><a name="bb0215" href="#b0215" class="workspace-trigger">[43]</a><!-- react-text: 2169 -->.<!-- /react-text --></p><figure id="f0035"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr7.jpg" alt="Dow Jones Industrial Average (DJOI) over a period of 10years" height="391"><ol class="links-for-figure"><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr7_lrg.jpg" target="__blank"><!-- react-text: 2176 -->Download high-res image<!-- /react-text --><!-- react-text: 2177 --> (182KB)<!-- /react-text --></a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr7.jpg" target="__blank">Download full-size image</a></li></ol></span><div class="captions"><span id="cn035"><p id="sp040"><span class="label">Fig. 7</span><!-- react-text: 2184 -->. <!-- /react-text --><!-- react-text: 2185 -->Dow Jones Industrial Average (DJOI) over a period of 10<!-- /react-text --><!-- react-text: 2186 -->&nbsp;<!-- /react-text --><!-- react-text: 2187 -->years.<!-- /react-text --></p></span></div></figure></div><p id="p0175"><!-- react-text: 2189 -->One model that has emerged and shown to be suitable for stock market prediction is the artificial neural network (ANN) <!-- /react-text --><a name="bb0015" href="#b0015" class="workspace-trigger">[3]</a><!-- react-text: 2191 -->.
 This is due to its ability to handle non-linear complex systems. A 
survey of ANNs applied to stock market prediction is given in <!-- /react-text --><a name="bb0395" href="#b0395" class="workspace-trigger">[79]</a><!-- react-text: 2193 -->. However, most approaches of ANN applied to stock prediction have given unsatisfactory results <!-- /react-text --><a name="bb0005" href="#b0005" class="workspace-trigger">[1]</a><!-- react-text: 2195 -->. Neural networks with feedback have also been tried, such as recurrent versions of TDNN <!-- /react-text --><a name="bb0335" href="#b0335" class="workspace-trigger">[67]</a><!-- react-text: 2197 -->, wavelet transformed features with an RNN <!-- /react-text --><a name="bb0275" href="#b0275" class="workspace-trigger">[55]</a><!-- react-text: 2199 -->, and echo state networks <!-- /react-text --><a name="bb0400" href="#b0400" class="workspace-trigger">[80]</a><!-- react-text: 2201 -->. Many of these methods are applied directly on the raw data, while other papers focus more on the feature selection step <!-- /react-text --><a name="bb0605" href="#b0605" class="workspace-trigger">[121]</a><!-- react-text: 2203 -->.<!-- /react-text --></p><p id="p0180"><!-- react-text: 2205 -->In
 summary, it can be concluded that there is still room to improve 
existing techniques for making safe and accurate stock prediction 
systems. If additional information from sources that affect the stock 
market can be measured and obtained, such as general public opinions 
from social media <!-- /react-text --><a name="bb0070" href="#b0070" class="workspace-trigger">[14]</a><!-- react-text: 2207 -->, trading volume <!-- /react-text --><a name="bb0670" href="#b0670" class="workspace-trigger">[134]</a><!-- react-text: 2209 -->,
 market specific domain knowledge, and political and economical factors,
 it can be combined together with the stock price data to achieve higher
 stock price predictions <!-- /react-text --><a name="bb0005" href="#b0005" class="workspace-trigger">[1]</a><!-- react-text: 2211 -->.
 The limited success of applying small, one layer neural networks for 
stock market prediction and the realization that there is a need to add 
more information to make better predictions indicate that a future 
direction for stock market prediction is to apply the combined data to 
more powerful models that are able to handle such complex, 
high-dimensional data. Deep learning methods for multivariate 
time-series fit this description and provide new interesting approach 
for the financial field and a new challenging application for the deep 
learning community, which to the authors knowledge has not yet been 
tried.<!-- /react-text --></p></section><section id="s0085"><h3><!-- react-text: 2214 -->4.3<!-- /react-text --><!-- react-text: 2215 -->. <!-- /react-text --><!-- react-text: 2216 -->Speech recognition<!-- /react-text --></h3><div><p id="p0185"><!-- react-text: 2219 -->Speech recognition is one area where deep learning has made significant progress <!-- /react-text --><a name="bb0240" href="#b0240" class="workspace-trigger">[48]</a><!-- react-text: 2221 -->. The problem of speech recognition can be divided into a variety of sub-problems, such as speaker identification <!-- /react-text --><a name="bb0385" href="#b0385" class="workspace-trigger">[77]</a><!-- react-text: 2223 -->, gender identification <!-- /react-text --><a name="bb0390" href="#b0390" class="workspace-trigger">[78,101]</a><!-- react-text: 2225 -->, speech-to-text <!-- /react-text --><a name="bb0160" href="#b0160" class="workspace-trigger">[32]</a><!-- react-text: 2227 --> and acoustic modeling. The raw input data is single channel and highly time and frequency dependent, see <!-- /react-text --><a name="bf0040" href="#f0040" class="workspace-trigger">Fig. 8</a><!-- react-text: 2229 -->.
 A common approach is to use pre-set features that are designed for 
speech processing such as Mel-frequency cepstral coefficients (MFCC).<!-- /react-text --></p><figure id="f0040"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr8.jpg" alt="Raw acoustic signal of the utterance of the sentence “The quick brown fox jumps…" height="355"><ol class="links-for-figure"><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr8_lrg.jpg" target="__blank"><!-- react-text: 2236 -->Download high-res image<!-- /react-text --><!-- react-text: 2237 --> (186KB)<!-- /react-text --></a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr8.jpg" target="__blank">Download full-size image</a></li></ol></span><div class="captions"><span id="cn040"><p id="sp045"><span class="label">Fig. 8</span><!-- react-text: 2244 -->. <!-- /react-text --><!-- react-text: 2245 -->Raw acoustic signal of the utterance of the sentence “The quick brown fox jumps over the lazy dog”.<!-- /react-text --></p></span></div></figure></div><p id="p0190"><!-- react-text: 2247 -->For decades, Hidden Markov Models (HMMs) <!-- /react-text --><a name="bb0515" href="#b0515" class="workspace-trigger">[103]</a><!-- react-text: 2249 -->
 have been the state-of-the-art technique for speech recognition. A 
common method for discretization of the input data for speech that is 
required by the HMM is to use Gaussian mixture models (GMM). More 
recently however, the Restricted Boltzmann Machines (RBM) have shown to 
be an adequate alternative for replacing the GMM in the discretization 
step. A classification error of 20.7% on the TIMIT speech recognition 
data set<!-- /react-text --><a name="bfn2" href="#fn2" class="workspace-trigger"><sup>2</sup></a><!-- react-text: 2252 --> was achieved by Mohamed et al. <!-- /react-text --><a name="bb0465" href="#b0465" class="workspace-trigger">[93]</a><!-- react-text: 2254 --> by training a RBM on MFCC features. A similar setup has been used for large vocabulary speech recognition by Dahl et al. <!-- /react-text --><a name="bb0110" href="#b0110" class="workspace-trigger">[22]</a><!-- react-text: 2256 -->. A convolutional deep belief networks was applied by Lee et al. <!-- /react-text --><a name="bb0390" href="#b0390" class="workspace-trigger">[78]</a><!-- react-text: 2258 --> to audio data and evaluated on various audio classification tasks.<!-- /react-text --></p><p id="p0195"><!-- react-text: 2260 -->A number of variations on the RBM have also been tried on speech data. The mean-covariance RBM (mcRBM) <!-- /react-text --><a name="bb0525" href="#b0525" class="workspace-trigger">[105,106]</a><!-- react-text: 2262 --> achieved a classification error of 20.5% on the TIMIT data set by Dahl et al. <!-- /react-text --><a name="bb0115" href="#b0115" class="workspace-trigger">[23]</a><!-- react-text: 2264 -->. A conditional RBM (cRBM) was modified by Mohamed and Hinton <!-- /react-text --><a name="bb0470" href="#b0470" class="workspace-trigger">[94]</a><!-- react-text: 2266 -->
 by including connections from future instead of only having connections
 from the past, which presumably gave better classification because the 
near future is more relevant than the more distant past.<!-- /react-text --></p><p id="p0200"><!-- react-text: 2268 -->Earlier, a Time-Delay Neural Network (TDNN) has been used for speech recognition <!-- /react-text --><a name="bb0625" href="#b0625" class="workspace-trigger">[125]</a><!-- react-text: 2270 --> and a review of TDNN architectures for speech recognition is given by Sugiyama et al. <!-- /react-text --><a name="bb0560" href="#b0560" class="workspace-trigger">[112]</a><!-- react-text: 2272 -->.
 However, it has been suggested that convolution over the frequency 
instead of the time is better since the HMM on top models the temporal 
information.<!-- /react-text --></p><p id="p0205"><!-- react-text: 2274 -->The recent work by Graves et al. <!-- /react-text --><a name="bb0205" href="#b0205" class="workspace-trigger">[41]</a><!-- react-text: 2276 --> uses a deep Long Short-term Memory Recurrent Neural Network (RNN) <!-- /react-text --><a name="bb0270" href="#b0270" class="workspace-trigger">[54]</a><!-- react-text: 2278 -->
 to achieve a classification error of 17.7% on the TIMIT data set, which
 is the best result to date. One difference between the approaches of 
RBM-HMM and RNN is that the RNN can be used as an ‘end-to-end’ model 
because it replaces a combination of different techniques that are 
currently used in sequence modeling, such as the HMM. However, both 
these approaches still rely on pre-defined features as input.<!-- /react-text --></p><p id="p0210"><!-- react-text: 2280 -->While
 using features such as MFCCs that collapse high dimensional speech 
sound waves into low dimensional encodings have been successful in 
speech recognition systems, such low dimensional encodings may lose some
 relevant information. On the other hand, there are approaches that 
build their own features instead of using pre-defined features. The work
 by Jaitly and Hinton <!-- /react-text --><a name="bb0305" href="#b0305" class="workspace-trigger">[61]</a><!-- react-text: 2282 -->
 used raw speech as input to a RBM and achieved a classification error 
of 21.8% on the TIMIT data set. Another approach that uses raw data is 
learning the auditory codes using spiking population code <!-- /react-text --><a name="bb0550" href="#b0550" class="workspace-trigger">[110]</a><!-- react-text: 2284 -->.
 In this model, each spike encodes the precise time position and 
magnitude of a localized, time varying kernel function. The learned 
representations (basis vectors) show a striking resemblance to the 
cochlear filters in the auditory cortex.<!-- /react-text --></p><p id="p0215"><!-- react-text: 2286 -->Similarly sparse coding for audio classification is used by Grosse et al. <!-- /react-text --><a name="bb0210" href="#b0210" class="workspace-trigger">[42]</a><!-- react-text: 2288 -->.
 The authors used features as input and a shift-invariant sparse coding 
model that reconstructs a time-series input using all the basis 
functions in all possible shifts. The model was evaluated on speaker 
identification and music genre classification.<!-- /react-text --></p><p id="p0220"><!-- react-text: 2290 -->A multimodal framework was explored by Ngiam et al. <!-- /react-text --><a name="bb0490" href="#b0490" class="workspace-trigger">[98]</a><!-- react-text: 2292 --> where video data of spoken digits and letters where combined with the audio data to improve the classification.<!-- /react-text --></p><p id="p0225"><!-- react-text: 2294 -->In
 conclusion, there have been a lot of recent improvements to the 
previous dominance of the features-GMM-HMM structure that has been used 
in speech recognition. First, there is a trend towards replacing GMM 
with a feature learning model such as deep belief networks or sparse 
coding. Second, there is a trend towards replacing HMM with other 
alternatives. One of them is the conditional random field (CRF) <!-- /react-text --><a name="bb0340" href="#b0340" class="workspace-trigger">[68]</a><!-- react-text: 2296 --> that have been shown to outperform HMM, see for example the work by van Kasteren et al. <!-- /react-text --><a name="bb0320" href="#b0320" class="workspace-trigger">[64]</a><!-- react-text: 2298 --> and Bengio and Frasconi <!-- /react-text --><a name="bb0030" href="#b0030" class="workspace-trigger">[6]</a><!-- react-text: 2300 -->. However, to date, the best reported result is replacing both parts of GMM-HMM with RNN <!-- /react-text --><a name="bb0205" href="#b0205" class="workspace-trigger">[41]</a><!-- react-text: 2302 -->.
 A next possible step for speech processing would be to replace the 
pre-made features with algorithms that build even better features from 
raw data.<!-- /react-text --></p></section><section id="s0090"><h3><!-- react-text: 2305 -->4.4<!-- /react-text --><!-- react-text: 2306 -->. <!-- /react-text --><!-- react-text: 2307 -->Music recognition<!-- /react-text --></h3><p id="p0230"><!-- react-text: 2309 -->Music
 recognition is similar to speech recognition with the exception that 
the data can be multivariate and either presented as raw acoustic 
signals or by discrete chords. In music recognition, a number of 
sub-problems are considered, such as music annotation (genre, chord, 
instrument, mood classification), music retrieval (text-based content 
search, content-based similarity retrieval, organization), and tempo 
identification. For music recognition, a commonly used set of features 
are MFCCs, chroma, constant-Q spectrograms (CQT) <!-- /react-text --><a name="bb0545" href="#b0545" class="workspace-trigger">[109]</a><!-- react-text: 2311 -->, local contrast normalization (LCN) <!-- /react-text --><a name="bb0375" href="#b0375" class="workspace-trigger">[75]</a><!-- react-text: 2313 -->, or Compressive Sampling (CS) <!-- /react-text --><a name="bb0090" href="#b0090" class="workspace-trigger">[18]</a><!-- react-text: 2315 -->.
 However, there is an increasing interest in learning the features from 
the data instead of using highly engineered features based on acoustic 
knowledge. A widely used data set for music genre recognition is GTZAN.<!-- /react-text --><a name="bfn3" href="#fn3" class="workspace-trigger"><sup>3</sup></a><!-- react-text: 2318 -->
 Even though it is possible to solve many tasks on text-based meta-data,
 such as user data (playlists, song history, social structure), there is
 still a need for content-based analysis. The reasons for this is that 
manual labeling is inefficient due to the large amount of music content 
and some tasks require the well-trained ear of an expert, e.g., chord 
recognition.<!-- /react-text --></p><p id="p0235"><!-- react-text: 2320 -->The work by Humphrey et al. <!-- /react-text --><a name="bb0280" href="#b0280" class="workspace-trigger">[56]</a><!-- react-text: 2322 -->
 gives a review and future directions for music recognition. In this 
work, three deficiencies are identified: hand-crafted features are 
sub-optimal and unsustainable to develop for each task, shallow 
architectures are fundamentally limited, and short-time analysis cannot 
encode a musically meaningful structure. To handle these deficiencies it
 is proposed to learn features automatically, apply deep architectures, 
and model longer time-dependencies than the current use of data in 
milliseconds.<!-- /react-text --></p><p id="p0240"><!-- react-text: 2324 -->The work by Nam et al. <!-- /react-text --><a name="bb0480" href="#b0480" class="workspace-trigger">[96]</a><!-- react-text: 2326 -->
 addresses the first deficiency by presenting a processing pipeline for 
automatically learning features for music recognition. The model follows
 the structure of a high-dimensional single layer network with 
max-pooling separately after learning the features <!-- /react-text --><a name="bb0105" href="#b0105" class="workspace-trigger">[21]</a><!-- react-text: 2328 -->.
 The input data is taken from multiple audio frames and fed into three 
different feature learning algorithms, namely K-means clustering, sparse
 coding, and RBM. The learned features gave better performance compared 
to MFCC, regardless of the feature learning algorithm.<!-- /react-text --></p><p id="p0245"><!-- react-text: 2330 -->Sparse coding have been used by Grosse et al. <!-- /react-text --><a name="bb0210" href="#b0210" class="workspace-trigger">[42]</a><!-- react-text: 2332 --> for learning features for music genre recognition. The work by Henaff et al. <!-- /react-text --><a name="bb0230" href="#b0230" class="workspace-trigger">[46]</a><!-- react-text: 2334 -->
 used Predictive Sparse Decomposition (PSD), which is similar to sparse 
coding, and achieved an accuracy of 83.4% on the GTZAN data. In this 
work, the features are automatically learned from CTQ spectograms in an 
unsupervised manner. The learned features capture information about 
which chords are being played in a particular frame and produce 
comparable results to hand-crafted features for the task of genre 
recognition. A limitation, however, is that it ignores temporal 
dependencies between frames.<!-- /react-text --></p><p id="p0250"><!-- react-text: 2336 -->Convolutional DBNs were used by Lee et al. <!-- /react-text --><a name="bb0390" href="#b0390" class="workspace-trigger">[78]</a><!-- react-text: 2338 --> to learn features from speech and music spectrograms and from engineered features by Dieleman <!-- /react-text --><a name="bb0125" href="#b0125" class="workspace-trigger">[25]</a><!-- react-text: 2340 -->. The work by Hamel and Eck <!-- /react-text --><a name="bb0225" href="#b0225" class="workspace-trigger">[45]</a><!-- react-text: 2342 --> also uses convolutional DBN to achieve an accuracy of 84.3% on the GTZAN dataset.<!-- /react-text --></p><p id="p0255"><!-- react-text: 2344 -->Self-taught
 learning have also been used for music genre classification. The 
self-taught learning framework attempts to use unlabeled data that does 
not share the labels of the classification task to improve 
classification performance <!-- /react-text --><a name="bb0520" href="#b0520" class="workspace-trigger">[104,62]</a><!-- react-text: 2346 -->. Self-taught learning and sparse coding are used by Markov and Matsui <!-- /react-text --><a name="bb0425" href="#b0425" class="workspace-trigger">[85]</a><!-- react-text: 2348 --> where unlabeled data from other music genres other than in the classification task was used to train the model.<!-- /react-text --></p><p id="p0260"><!-- react-text: 2350 -->In
 conclusion, there are many works that use unsupervised feature learning
 methods for music recognition. The motivation for using deep networks 
is that music itself is structured hierarchically by a combination of 
chords, melodies and rhythms that creates motives, phrases, sections and
 finally entire pieces <!-- /react-text --><a name="bb0280" href="#b0280" class="workspace-trigger">[56]</a><!-- react-text: 2352 -->.
 Just like in speech recognition, the input data is often in some form 
of spectrograms. Many works leave the natural step of learning features 
from raw data as future work <!-- /react-text --><a name="bb0475" href="#b0475" class="workspace-trigger">[95]</a><!-- react-text: 2354 -->. Still, as proposed by Humphrey et al. <!-- /react-text --><a name="bb0280" href="#b0280" class="workspace-trigger">[56]</a><!-- react-text: 2356 -->,
 even though convolutional networks have given good results on 
time-frequency representations of audio, there is room for discovering 
new and better models.<!-- /react-text --></p></section><section id="s0095"><h3><!-- react-text: 2359 -->4.5<!-- /react-text --><!-- react-text: 2360 -->. <!-- /react-text --><!-- react-text: 2361 -->Motion capture data<!-- /react-text --></h3><p id="p0265"><!-- react-text: 2363 -->Modeling
 human motion has several applications such as tracking, activity 
recognition, style and content separation, person identification, 
computer animation, and synthesis of new motion data. Motion capture 
data is collected from recordings of movements from several points on 
the body of a human actor. These points can be captured by cameras that 
either track the position of strategically placed markers (usually at 
joint centers) or uses vision-based algorithms for tracking points of 
interest <!-- /react-text --><a name="bb0190" href="#b0190" class="workspace-trigger">[38]</a><!-- react-text: 2365 -->.
 The points are represented as 3D Cartesian coordinates over time and 
are used to form a skeletal structure with constant limb lengths by 
translating the points to relative joint angles. The joint angles can be
 expressed in Euler angles, 4D quaternions, or exponential map 
parameterization <!-- /react-text --><a name="bb0200" href="#b0200" class="workspace-trigger">[40]</a><!-- react-text: 2367 -->
 and can have 1–3 degrees of freedom (DOF) each. The full data set 
consists of the orientation and translation of the root and all relative
 joint angles for each time frame as well as the constant skeleton 
model. The data is noisy, high-dimensional, and multivariate with 
complex nonlinear relationships. It has a lower frequency compared to 
speech and music data and some of the signals may be task-redundant.<!-- /react-text --></p><p id="p0270"><!-- react-text: 2369 -->Some of the traditional approaches include the work by Brand and Hertzmann <!-- /react-text --><a name="bb0080" href="#b0080" class="workspace-trigger">[16]</a><!-- react-text: 2371 -->,
 which models both the style and content of human motion using Hidden 
Markov Models (HMMs). The different styles were learned from unlabeled 
data and the trained model was used to synthesize motion data. A linear 
dynamical systems was used by Chiappa et al. <!-- /react-text --><a name="bb0100" href="#b0100" class="workspace-trigger">[20]</a><!-- react-text: 2373 -->
 to model three different motions of a human performing the task of 
holding a cup that has a ball attached to it with a string and then try 
to catch the ball into the cup (game of Balero). A Bayesian mixture of 
linear Gaussian state-space models (LGSSM) was trained with data from a 
human learner and used to generate new motions that was clustered and 
simulated on a robotic manipulator.<!-- /react-text --></p><div><p id="p0275"><!-- react-text: 2376 -->Both HMMs and linear dynamical systems are limited by their ability to model complex full-body motions. The work by Wang et al. <!-- /react-text --><a name="bb0635" href="#b0635" class="workspace-trigger">[127]</a><!-- react-text: 2378 --> uses Gaussian Processes to model three styles of locomotive motion (walk, run, stride) from the CMU motion capture data set,<!-- /react-text --><a name="bfn4" href="#fn4" class="workspace-trigger"><sup>4</sup></a><!-- react-text: 2381 --> see <!-- /react-text --><a name="bf0045" href="#f0045" class="workspace-trigger">Fig. 9</a><!-- react-text: 2383 -->.
 The CMU data set have also been used to generate motion capture from 
just a few initialization frames with a Temporal RBM (TRBM) <!-- /react-text --><a name="bb0570" href="#b0570" class="workspace-trigger">[114]</a><!-- react-text: 2385 --> and a conditional RBM (cRBM) <!-- /react-text --><a name="bb0590" href="#b0590" class="workspace-trigger">[118]</a><!-- react-text: 2387 -->.
 Better modeling and smoother transition between different styles of 
motions was achieved by adding a second hidden layer to the cRBM, using 
the Recurrent TRBM <!-- /react-text --><a name="bb0575" href="#b0575" class="workspace-trigger">[115]</a><!-- react-text: 2389 -->, and using the factored conditional RBM (fcRBM) <!-- /react-text --><a name="bb0585" href="#b0585" class="workspace-trigger">[117]</a><!-- react-text: 2391 -->. The work by Längkvist and Loutfi <!-- /react-text --><a name="bb0360" href="#b0360" class="workspace-trigger">[72]</a><!-- react-text: 2393 -->
 restructures an auto-encoder to resemble a cRBM but is used to perform 
classification on the CMU motion capture data instead of generating new 
sequences. The drawbacks with general-purpose models such as Gaussian 
Processes and cRBM are that prior information about motion is not 
utilized and they have a costly approximation sampling procedure.<!-- /react-text --></p><figure id="f0045"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr9.jpg" alt="A sequence of human motion from the CMU motion capture data set" height="592"><ol class="links-for-figure"><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr9_lrg.jpg" target="__blank"><!-- react-text: 2400 -->Download high-res image<!-- /react-text --><!-- react-text: 2401 --> (913KB)<!-- /react-text --></a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr9.jpg" target="__blank">Download full-size image</a></li></ol></span><div class="captions"><span id="cn045"><p id="sp050"><span class="label">Fig. 9</span><!-- react-text: 2408 -->. <!-- /react-text --><!-- react-text: 2409 -->A sequence of human motion from the CMU motion capture data set.<!-- /react-text --></p></span></div></figure></div><p id="p0280"><!-- react-text: 2411 -->An
 unsupervised hierarchical model that is specifically designed for 
modeling locomotion styles was developed by Pan and Torresani <!-- /react-text --><a name="bb0500" href="#b0500" class="workspace-trigger">[100]</a><!-- react-text: 2413 -->
 and builds on the Hierarchical Bayesian Continuous Profile Model 
(HB-CPM). A Dynamic Factor Graph (DFG), which is an extension of factor 
graphs, was introduced by Mirowski and LeCun <!-- /react-text --><a name="bb0450" href="#b0450" class="workspace-trigger">[90]</a><!-- react-text: 2415 -->
 and used on motion capture data to fill in missing data. The advantage 
of DFG is that it has a constant partition function which avoids the 
costly approximation sampling procedure that is used in a cRBM.<!-- /react-text --></p><p id="p0285"><!-- react-text: 2417 -->In
 summary, analyzing and synthesizing motion capture data is a 
challenging task and it encourages researchers to further improve 
learning algorithms for dealing with complex, multivariate time-series 
data. A motivation for using deep learning algorithms for motion capture
 data is that it has been suggested that human motion is composed of 
elementary building blocks (motion templates) and any complex motion is 
constructed from a library of these previously learned motion templates <!-- /react-text --><a name="bb0155" href="#b0155" class="workspace-trigger">[31]</a><!-- react-text: 2419 -->.
 Deep networks can, in an unsupervised manner, learn these motion 
templates from raw data and use them to form complex human motions. 
Motion capture data also provides an interesting platform for feature 
learning from raw data since there is no commonly used feature set for 
motion capture data. Therefore, the success of applying deep learning 
algorithms to motion data can inspire learning features from raw data in
 other time-series problems as well.<!-- /react-text --></p></section><section id="s0100"><h3><!-- react-text: 2422 -->4.6<!-- /react-text --><!-- react-text: 2423 -->. <!-- /react-text --><!-- react-text: 2424 -->Electronic nose data<!-- /react-text --></h3><div><p id="p0290"><!-- react-text: 2427 -->Machine olfaction <!-- /react-text --><a name="bb0495" href="#b0495" class="workspace-trigger">[99,33]</a><!-- react-text: 2429 -->
 is a field that seeks to quantify and analyze odours using an 
electronic nose (e-nose). An e-nose is composed of an array of selective
 gas sensors together with pattern recognition techniques. <!-- /react-text --><a name="bf0050" href="#f0050" class="workspace-trigger">Fig. 10</a><!-- react-text: 2431 -->
 shows the readings from an e-nose sensor array. The number of sensors 
in the array typically ranges from 4–30 sensors and are therefore, just 
like motion capture data, multivariate and may contain redundant 
signals. The data is also unintuitive and there is a lack of expert 
knowledge that can guide the design of features. E-noses are mostly used
 in practice for industrial applications such as measuring food, 
beverage <!-- /react-text --><a name="bb0175" href="#b0175" class="workspace-trigger">[35]</a><!-- react-text: 2433 -->, and air quality <!-- /react-text --><a name="bb0660" href="#b0660" class="workspace-trigger">[132]</a><!-- react-text: 2435 -->, gas identification, and gas source localization <!-- /react-text --><a name="bb0055" href="#b0055" class="workspace-trigger">[11]</a><!-- react-text: 2437 -->, but also has medical applications such as bacteria identification <!-- /react-text --><a name="bb0140" href="#b0140" class="workspace-trigger">[28]</a><!-- react-text: 2439 --> and diagnosis <!-- /react-text --><a name="bb0170" href="#b0170" class="workspace-trigger">[34]</a><!-- react-text: 2441 -->.<!-- /react-text --></p><figure id="f0050"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr10.jpg" alt="Normalized data from an array of electronic nose sensors" height="299"><ol class="links-for-figure"><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr10_lrg.jpg" target="__blank"><!-- react-text: 2448 -->Download high-res image<!-- /react-text --><!-- react-text: 2449 --> (429KB)<!-- /react-text --></a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr10.jpg" target="__blank">Download full-size image</a></li></ol></span><div class="captions"><span id="cn050"><p id="sp055"><span class="label">Fig. 10</span><!-- react-text: 2456 -->. <!-- /react-text --><!-- react-text: 2457 -->Normalized data from an array of electronic nose sensors.<!-- /react-text --></p></span></div></figure></div><p id="p0295"><!-- react-text: 2459 -->The
 traditional approach of analyzing e-nose data involves extracting 
information in the static and dynamic phases of the signals <!-- /react-text --><a name="bb0220" href="#b0220" class="workspace-trigger">[44]</a><!-- react-text: 2461 -->
 for the use of static pattern analysis techniques (PCA, discriminant 
function analysis, cluster analysis and neural networks). Some commonly 
used features are the static sensor response, transient derivatives <!-- /react-text --><a name="bb0600" href="#b0600" class="workspace-trigger">[120]</a><!-- react-text: 2463 -->, area under the curve <!-- /react-text --><a name="bb0085" href="#b0085" class="workspace-trigger">[17]</a><!-- react-text: 2465 -->, model parameter identification <!-- /react-text --><a name="bb0615" href="#b0615" class="workspace-trigger">[123]</a><!-- react-text: 2467 -->, and dynamic analysis <!-- /react-text --><a name="bb0235" href="#b0235" class="workspace-trigger">[47]</a><!-- react-text: 2469 -->.<!-- /react-text --></p><p id="p0300"><!-- react-text: 2471 -->A popular approach for modeling e-nose data is the Time-Delay Neural Networks (TDNN) <!-- /react-text --><a name="bb0625" href="#b0625" class="workspace-trigger">[125]</a><!-- react-text: 2473 -->. It has been used for identifying the smell of spices <!-- /react-text --><a name="bb0665" href="#b0665" class="workspace-trigger">[133]</a><!-- react-text: 2475 -->, ternary mixtures <!-- /react-text --><a name="bb0620" href="#b0620" class="workspace-trigger">[124]</a><!-- react-text: 2477 -->, optimum fermentation time for black tea <!-- /react-text --><a name="bb0060" href="#b0060" class="workspace-trigger">[12]</a><!-- react-text: 2479 -->, and vintages of wine <!-- /react-text --><a name="bb0650" href="#b0650" class="workspace-trigger">[130]</a><!-- react-text: 2481 -->. An RNN have been used for odour localization with a mobile robot <!-- /react-text --><a name="bb0135" href="#b0135" class="workspace-trigger">[27]</a><!-- react-text: 2483 -->.<!-- /react-text --></p><p id="p0305"><!-- react-text: 2485 -->The work by Vembu et al. <!-- /react-text --><a name="bb0615" href="#b0615" class="workspace-trigger">[123]</a><!-- react-text: 2487 -->
 compares the gas discrimination and localization between three 
approaches: SVM on raw data, SVM on features extracted from 
auto-regressive and linear dynamical systems, and finally a SVMs with 
kernels specialized for structured data <!-- /react-text --><a name="bb0180" href="#b0180" class="workspace-trigger">[36]</a><!-- react-text: 2489 -->.
 The SVM with built-in time-aware kernels performed better than 
techniques that used feature extraction, even though the features 
captured temporal information.<!-- /react-text --></p><p id="p0310"><!-- react-text: 2491 -->More recently, an auto-encoder, RBM, and cRBM have been used for bacteria identification <!-- /react-text --><a name="bb0355" href="#b0355" class="workspace-trigger">[71]</a><!-- react-text: 2493 --> and fast classification of meat spoilage markers <!-- /react-text --><a name="bb0345" href="#b0345" class="workspace-trigger">[69]</a><!-- react-text: 2495 -->.<!-- /react-text --></p><p id="p0315">E-nose
 data introduces the challenge of improving models that can deal with 
redundant signals. It is not feasible to produce tailor-made sensors for
 each possible individual gas and combinations of gases of interest. 
Therefore the common approach is to use an array of sensors with 
different properties and leave the discrimination to the pattern 
analysis software. It is also not desirable to construct new feature 
sets for each e-nose application so a data-driven feature learning 
method is useful. The early works on e-nose data create feature vectors 
of simple features for each signal such as the static response or the 
slope of dynamic response and then feed it to a classifier. Recently, 
the use of dynamic models such as neural networks with tapped delays and
 SVMs with kernels for structured data have shown to improve the 
performance over static approaches. The next step is to continue this 
trend of using dynamical models that constructs robust features that can
 deal with noisy inputs in order to quantify and classify odors in more 
challenging open environments with many different simultaneous gas 
sources.</p></section><section id="s0105"><h3><!-- react-text: 2499 -->4.7<!-- /react-text --><!-- react-text: 2500 -->. <!-- /react-text --><!-- react-text: 2501 -->Physiological data<!-- /react-text --></h3><div><p id="p0320"><!-- react-text: 2504 -->With
 physiological data we consider recordings such as 
electroencephalography (EEG), magnetoencephalography (MEG), 
electrocardiography (ECG), and wearable sensors for health monitoring. <!-- /react-text --><a name="bf0055" href="#f0055" class="workspace-trigger">Fig. 11</a><!-- react-text: 2506 -->
 shows an example of how physiological data look like. The data can 
exist both as singular or multiple channels. The use of a feature 
learning algorithm is particularly beneficial in medical applications 
because acquiring a labeled medical data set is expensive since the data
 sets are often very large and require the labeling of an expert in the 
field.<!-- /react-text --></p><figure id="f0055"><span><img src="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr11.jpg" alt="Data from EEG (top two signals), EOG (third and fourth signal), and EMG (bottom…" height="375"><ol class="links-for-figure"><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr11_lrg.jpg" target="__blank"><!-- react-text: 2513 -->Download high-res image<!-- /react-text --><!-- react-text: 2514 --> (517KB)<!-- /react-text --></a></li><li><a href="https://ars.els-cdn.com/content/image/1-s2.0-S0167865514000221-gr11.jpg" target="__blank">Download full-size image</a></li></ol></span><div class="captions"><span id="cn055"><p id="sp060"><span class="label">Fig. 11</span><!-- react-text: 2521 -->. <!-- /react-text --><!-- react-text: 2522 -->Data
 from EEG (top two signals), EOG (third and fourth signal), and EMG 
(bottom signal), recorded with a polysomnograph during sleep.<!-- /react-text --></p></span></div></figure></div><p id="p0325"><!-- react-text: 2524 -->The work by Mirowski et al. <!-- /react-text --><a name="bb0460" href="#b0460" class="workspace-trigger">[92]</a><!-- react-text: 2526 -->
 compares convolutional networks with logistic regression and SVMs for 
epileptic seizure prediction from intracranial EEG signals. The features
 that are used are hand-engineered bi-variate features between channels 
that encode relationship between pairs of EEG channels. The result was 
that convolutional networks achieved only 1 false-alarm prediction from 
21 patients while the SVM had 10 false-alarms. TDNN and ICA has also 
been used for EEG-based prediction of epileptic seizures <!-- /react-text --><a name="bb0455" href="#b0455" class="workspace-trigger">[91]</a><!-- react-text: 2528 -->. The application of self-organizing maps (SOM) to analyze EMG data is presented by Tucker <!-- /react-text --><a name="bb0610" href="#b0610" class="workspace-trigger">[122]</a><!-- react-text: 2530 -->.<!-- /react-text --></p><p id="p0330"><!-- react-text: 2532 -->A
 RBM-based method that builds features from raw data for sleep stage 
classification from 4-channel polysomnography data has been proposed by 
Längkvist et al. <!-- /react-text --><a name="bb0350" href="#b0350" class="workspace-trigger">[70]</a><!-- react-text: 2534 -->. A similar setup was used by Wulsin et al. <!-- /react-text --><a name="bb0645" href="#b0645" class="workspace-trigger">[129]</a><!-- react-text: 2536 --> for modeling single channel EEG waveforms used for anomaly detection. A DBN is used by Wang and Shang <!-- /react-text --><a name="bb0630" href="#b0630" class="workspace-trigger">[126]</a><!-- react-text: 2538 -->
 to automatically extract features from raw unlabeled physiological data
 and achieves better classification than a feature-based approach. These
 recent works show that DBNs can be applied to raw physiological data to
 effectively learn relevant features.<!-- /react-text --></p><p id="p0335"><!-- react-text: 2540 -->A source separation method tailor-made to EEG and MEG signals is proposed by Hyvärinen et al. <!-- /react-text --><a name="bb0295" href="#b0295" class="workspace-trigger">[59]</a><!-- react-text: 2542 -->.
 The data is preprocessed by short-time Fourier transforms and then fed 
to an ICA. The work shows that temporal correlations are adequately 
taken into account. Independent Component Analysis (ICA) has provided to
 be a new tool to analyze time series and is a unifying framework that 
combines sparseness, temporal coherence, topography and complex cell 
pooling in a single model <!-- /react-text --><a name="bb0290" href="#b0290" class="workspace-trigger">[58]</a><!-- react-text: 2544 -->. A method for how to order the independent components for time-series is explored by Cheung and Xu <!-- /react-text --><a name="bb0095" href="#b0095" class="workspace-trigger">[19]</a><!-- react-text: 2546 -->.<!-- /react-text --></p><p id="p0340"><!-- react-text: 2548 -->Self-taught learning has been used with time-series data from wearable hand-motion sensors <!-- /react-text --><a name="bb0010" href="#b0010" class="workspace-trigger">[2]</a><!-- react-text: 2550 -->.<!-- /react-text --></p><p id="p0345">The
 field of physiological data is large and many different methods have 
been used. The characteristics of physiological data could be 
particularly interesting for the deep learning community because it can 
be used to explore the feasibility of learning features from raw data, 
which hopefully can inspire similar approaches in other time-series 
domains.</p></section><section id="s0110"><h3><!-- react-text: 2554 -->4.8<!-- /react-text --><!-- react-text: 2555 -->. <!-- /react-text --><!-- react-text: 2556 -->Summary<!-- /react-text --></h3><p id="p0350"><a name="bt0010" href="#t0010" class="workspace-trigger">Table 2</a><!-- react-text: 2559 -->
 gives a summary of the time-series problems that have been presented in
 this section. The first column indicates if the data is multivariate 
(or only contains one signal, univariate). Stock prediction is often 
viewed as a single channel problem, which explains the difficulties to 
produce accurate prediction systems, since stocks depend on a myriad of 
other factors, and arguably not at all on past values of the stock 
itself. For speech recognition, the use of multimodal sources can 
improve performance <!-- /react-text --><a name="bb0490" href="#b0490" class="workspace-trigger">[98]</a><!-- react-text: 2561 -->.<!-- /react-text --></p><p id="p0355"><!-- react-text: 2563 -->The
 second column shows which problems have attempted to create features 
purely from raw data. Only a few works have attempted this with speech 
recognition <!-- /react-text --><a name="bb0305" href="#b0305" class="workspace-trigger">[61,110]</a><!-- react-text: 2565 --> and physiological data <!-- /react-text --><a name="bb0645" href="#b0645" class="workspace-trigger">[129,70,126]</a><!-- react-text: 2567 -->.
 To the authors knowledge, learning features from raw data has not been 
attempted in music recognition. The process of constructing features 
from raw data has been well demonstrated for vision-tasks but is 
cautiously used for time-series problems. Models such as TDNN, cRBM and 
convolutional RBMs are well suited for being applied to raw data (or 
slightly pre-processed data).<!-- /react-text --></p><p id="p0360">The 
third column indicates which time-series problems have valuable 
information in the frequency-domain. For frequency-rich problems, it is 
uncommon to attempt to learn features from raw data. A reason for this 
is that current feature learning algorithms are yet not well-suited for 
learning features in the frequency-domain.</p><p id="p0365">The fourth 
column displays some common features that have been used in the 
literature. SIFT and HOG have been applied to videos even though those 
features are developed for static images. Chroma and MFCC have been 
applied to music recognition, even though they are develop for speech 
recognition. The e-nose community have tried a plethora of features. 
E-nose data is a relatively new field where a hand-crafted feature set 
have not been developed since this kind of data is complex and 
unintuitive. For physiological data, the used features are often a 
combination of application-specific features from previous works or 
hand-crafted features.</p><p id="p0370">The fifth column reports the 
most commonly used method (s), or current state-of-the-art, for each 
time-series problem. For stock prediction, the progress has stopped at 
classical neural networks. The current state-of-the-art augments 
additional information beside the stock data. For high-dimensional 
temporal data such as video and music recognition, the convolutional 
version of RBM have been successful. In recent years, the RBM have been 
used for speech recognition but the current state-of-the-art is achieved
 with an RNN. The cRBM introduced motion capture data to the deep 
learning community and it is an interesting problem to explore with 
other methods. Single layer neural networks with temporal capabilities 
have been used to model e-nose data and the use of deep networks is an 
interesting future direction for modeling e-nose data.</p><p id="p0375"><!-- react-text: 2572 -->And
 finally, the last column indicates a typical benchmark set for each 
problem. There is currently no well-known publicly available benchmark 
data set for e-nose data. For deep learning to enter the field of e-nose
 data it requires a large, well-organized data set that would benefit 
both communities. A data base of physiological data is available from 
PhysioNET <!-- /react-text --><a name="bb0195" href="#b0195" class="workspace-trigger">[39]</a><!-- react-text: 2574 -->.<!-- /react-text --></p></section></section><section id="s0115"><h2><!-- react-text: 2577 -->5<!-- /react-text --><!-- react-text: 2578 -->. <!-- /react-text --><!-- react-text: 2579 -->Conclusion<!-- /react-text --></h2><p id="p0380">Unsupervised
 feature learning and deep learning techniques have been successfully 
applied to a variety of domains. While much focus in deep learning and 
unsupervised feature learning have been in the computer vision domain, 
this paper has reviewed some of the successful applications of deep 
learning methods to the time-series domain. Some of these approaches 
have treated the input as static data but the most successful ones are 
those that have modified the deep learning models to better handle 
time-series data.</p><p id="p0385">The problem with processing 
time-series data as static input is that the importance of time is not 
captured. Modeling time-series faces many of the same challenges as 
modeling static data, such as coping with high-dimensional observations 
and nonlinear relationships between variables, however, by simply 
ignoring time and applying models of static data to time series one 
disregards much of the rich structure present in the data. When taking 
this approach, the context of the current input frame is lost and the 
only time-dependencies that are captured is within the input size. In 
order to capture long-term dependencies, the input size has to be 
increased, which can be impractical for multivariate signals or if the 
data has very long-term dependencies. The solution is to use a model 
that incorporates temporal coherence, performs temporal pooling, or 
models sequences of hidden unit activations.</p><p id="p0390">The choice
 of model and how the data should be presented to the model is highly 
dependent on the type of data. Within a chosen model there are 
additional design choices in terms of connectivity, architecture, and 
hyperparameters. For these reasons, even though many unsupervised 
feature learning models offer to relieve the user of having to come up 
with useful features for the current domain, there are still many 
challenges for applying them to time-series data. It is also worth 
noting that many works that construct useful features from the input 
data actually still use input data from pre-processed features.</p><p id="p0395">Deep
 learning methods offer better representation and classification on a 
multitude of time-series problems compared to shallow approaches when 
configured and trained properly. There is still room for improving the 
learning algorithms specifically for time-series data, e.g., performing 
signal selection that deals with redundant signals in multivariate input
 data. Another possible future direction is to develop models that 
change their internal architecture during learning or use model 
averaging in order to capture both short and long-term time 
dependencies. Further research in this area is needed to develop 
algorithms for time-series modeling that learn even better features and 
are easier and faster to train. Therefore, there is a need to focus less
 on the pre-processing pipeline for a specific time-series problem and 
focus more on learning better feature representations for a 
general-purpose algorithm for structured data, regardless of the 
application.</p></section></div><!-- react-empty: 2584 --><!-- react-empty: 2585 --></div><div class="related-content-links show-t-s hide-t hide-l hide-d"><div><div class="RelatedContentLink"><a class="related-content-link">Recommended articles</a></div><div class="RelatedContentLink"><a class="related-content-link">Citing articles (95)</a></div></div></div><!-- react-empty: 146 --><!-- react-empty: 147 --><div class="References"><section class="bibliography" id="bi005"><h2 class="section-title">References</h2><section class="bibliography-sec" id="bs005"><dl class="references"><dt class="label"><a href="#bb0005" id="ref-id-b0005">[1]</a></dt><dd class="reference" id="h0260"><div class="contribution"><!-- react-text: 2622 -->J.G.<!-- /react-text --><!-- react-text: 2623 --> <!-- /react-text --><!-- react-text: 2624 -->Agrawal<!-- /react-text --><!-- react-text: 2625 -->, <!-- /react-text --><!-- react-text: 2626 -->V.S.<!-- /react-text --><!-- react-text: 2627 --> <!-- /react-text --><!-- react-text: 2628 -->Chourasia<!-- /react-text --><!-- react-text: 2629 -->, <!-- /react-text --><!-- react-text: 2630 -->A.K.<!-- /react-text --><!-- react-text: 2631 --> <!-- /react-text --><!-- react-text: 2632 -->Mittra<!-- /react-text --><strong class="title"><!-- react-text: 2634 -->State-of-the-art in stock prediction techniques<!-- /react-text --></strong></div><div class="host"><!-- react-text: 2636 -->Int. J. Adv. Res. Electr. Electron. Instrum. Eng.<!-- /react-text --><!-- react-text: 2637 -->, <!-- /react-text --><!-- react-text: 2638 -->2<!-- /react-text --><!-- react-text: 2639 --> (<!-- /react-text --><!-- react-text: 2640 -->2013<!-- /react-text --><!-- react-text: 2641 -->)<!-- /react-text --><!-- react-text: 2642 -->, pp. <!-- /react-text --><!-- react-text: 2643 -->1360<!-- /react-text --><!-- react-text: 2644 -->-<!-- /react-text --><!-- react-text: 2645 -->1366<!-- /react-text --></div></dd><dt class="label"><a href="#bb0010" id="ref-id-b0010">[2]</a></dt><dd class="reference" id="h0265"><div class="contribution"><!-- react-text: 2650 -->O.<!-- /react-text --><!-- react-text: 2651 --> <!-- /react-text --><!-- react-text: 2652 -->Amft<!-- /react-text --><strong class="title"><!-- react-text: 2654 -->Self-taught learning for activity spotting in on-body motion sensor data<!-- /react-text --></strong></div><div class="host"><!-- react-text: 2656 -->ISWC 2011: Proceedings of the IEEE International Symposium on Wearable Computing<!-- /react-text --><!-- react-text: 2657 -->, <!-- /react-text --><!-- react-text: 2658 -->IEEE<!-- /react-text --><!-- react-text: 2659 --> (<!-- /react-text --><!-- react-text: 2660 -->2011<!-- /react-text --><!-- react-text: 2661 -->)<!-- /react-text --><!-- react-text: 2662 -->, pp. <!-- /react-text --><!-- react-text: 2663 -->83<!-- /react-text --><!-- react-text: 2664 -->-<!-- /react-text --><!-- react-text: 2665 -->86<!-- /react-text --></div></dd><dt class="label"><a href="#bb0015" id="ref-id-b0015">[3]</a></dt><dd class="reference" id="h0270"><div class="contribution"><!-- react-text: 2670 -->G.S.<!-- /react-text --><!-- react-text: 2671 --> <!-- /react-text --><!-- react-text: 2672 -->Atsalakis<!-- /react-text --><!-- react-text: 2673 -->, <!-- /react-text --><!-- react-text: 2674 -->K.P.<!-- /react-text --><!-- react-text: 2675 --> <!-- /react-text --><!-- react-text: 2676 -->Valavanis<!-- /react-text --><strong class="title"><!-- react-text: 2678 -->Surveying stock market forecasting techniques ĺc. Part ii: Soft computing methods<!-- /react-text --></strong></div><div class="host"><!-- react-text: 2680 -->Expert Syst. Appl.<!-- /react-text --><!-- react-text: 2681 -->, <!-- /react-text --><!-- react-text: 2682 -->36<!-- /react-text --><!-- react-text: 2683 --> (<!-- /react-text --><!-- react-text: 2684 -->2009<!-- /react-text --><!-- react-text: 2685 -->)<!-- /react-text --><!-- react-text: 2686 -->, pp. <!-- /react-text --><!-- react-text: 2687 -->5932<!-- /react-text --><!-- react-text: 2688 -->-<!-- /react-text --><!-- react-text: 2689 -->5941<!-- /react-text --></div></dd><dt class="label"><a href="#bb0020" id="ref-id-b0020">[4]</a></dt><dd class="reference" id="h0005"><span>Y. Bengio, Learning Deep Architectures for AI. Technical Report 1312. Dept. IRO, Universite de Montreal, 2007.</span></dd><dt class="label"><a href="#bb0025" id="ref-id-b0025">[5]</a></dt><dd class="reference" id="h0010"><span><!-- react-text: 2698 -->Y.
 Bengio, A. Courville, P. Vincent, Unsupervised Feature Learning and 
Deep Learning: A Review and New Perspectives. Technical Report, U. 
Montreal, 2012. Available from: <!-- /react-text --><a href="http://arxiv.org/abs/1206.5538" target="_blank">&lt;arXiv:1206.5538&gt;</a><!-- react-text: 2700 -->.<!-- /react-text --></span></dd><dt class="label"><a href="#bb0030" id="ref-id-b0030">[6]</a></dt><dd class="reference" id="h0275"><div class="contribution"><!-- react-text: 2705 -->Y.<!-- /react-text --><!-- react-text: 2706 --> <!-- /react-text --><!-- react-text: 2707 -->Bengio<!-- /react-text --><!-- react-text: 2708 -->, <!-- /react-text --><!-- react-text: 2709 -->P.<!-- /react-text --><!-- react-text: 2710 --> <!-- /react-text --><!-- react-text: 2711 -->Frasconi<!-- /react-text --><strong class="title"><!-- react-text: 2713 -->Input-output HMM’s for sequence processing<!-- /react-text --></strong></div><div class="host"><!-- react-text: 2715 -->IEEE Trans. Neural Networks<!-- /react-text --><!-- react-text: 2716 -->, <!-- /react-text --><!-- react-text: 2717 -->7<!-- /react-text --><!-- react-text: 2718 --> (<!-- /react-text --><!-- react-text: 2719 -->5<!-- /react-text --><!-- react-text: 2720 -->)<!-- /react-text --><!-- react-text: 2721 --> (<!-- /react-text --><!-- react-text: 2722 -->1996<!-- /react-text --><!-- react-text: 2723 -->)<!-- /react-text --><!-- react-text: 2724 -->, pp. <!-- /react-text --><!-- react-text: 2725 -->1231<!-- /react-text --><!-- react-text: 2726 -->-<!-- /react-text --><!-- react-text: 2727 -->1249<!-- /react-text --></div></dd><dt class="label"><a href="#bb0035" id="ref-id-b0035">[7]</a></dt><dd class="reference" id="h0280"><div class="contribution"><!-- react-text: 2732 -->Y.<!-- /react-text --><!-- react-text: 2733 --> <!-- /react-text --><!-- react-text: 2734 -->Bengio<!-- /react-text --><!-- react-text: 2735 -->, <!-- /react-text --><!-- react-text: 2736 -->P.<!-- /react-text --><!-- react-text: 2737 --> <!-- /react-text --><!-- react-text: 2738 -->Lamblin<!-- /react-text --><!-- react-text: 2739 -->, <!-- /react-text --><!-- react-text: 2740 -->D.<!-- /react-text --><!-- react-text: 2741 --> <!-- /react-text --><!-- react-text: 2742 -->Popovici<!-- /react-text --><!-- react-text: 2743 -->, <!-- /react-text --><!-- react-text: 2744 -->H.<!-- /react-text --><!-- react-text: 2745 --> <!-- /react-text --><!-- react-text: 2746 -->Larochelle<!-- /react-text --><strong class="title"><!-- react-text: 2748 -->Greedy layer-wise training of deep networks<!-- /react-text --></strong></div><div class="host"><!-- react-text: 2750 -->Adv. Neural Inf. Process. Syst.<!-- /react-text --><!-- react-text: 2751 -->, <!-- /react-text --><!-- react-text: 2752 -->19<!-- /react-text --><!-- react-text: 2753 --> (<!-- /react-text --><!-- react-text: 2754 -->2007<!-- /react-text --><!-- react-text: 2755 -->)<!-- /react-text --><!-- react-text: 2756 -->, p. <!-- /react-text --><!-- react-text: 2757 -->153<!-- /react-text --></div></dd><dt class="label"><a href="#bb0040" id="ref-id-b0040">[8]</a></dt><dd class="reference" id="h0285"><div class="contribution"><!-- react-text: 2762 -->Y.<!-- /react-text --><!-- react-text: 2763 --> <!-- /react-text --><!-- react-text: 2764 -->Bengio<!-- /react-text --><!-- react-text: 2765 -->, <!-- /react-text --><!-- react-text: 2766 -->Y.<!-- /react-text --><!-- react-text: 2767 --> <!-- /react-text --><!-- react-text: 2768 -->LeCun<!-- /react-text --><strong class="title"><!-- react-text: 2770 -->Scaling learning algorithms towards AI<!-- /react-text --></strong></div><div class="host"><!-- react-text: 2772 -->L.<!-- /react-text --><!-- react-text: 2773 --> <!-- /react-text --><!-- react-text: 2774 -->Bottou<!-- /react-text --><!-- react-text: 2775 -->, <!-- /react-text --><!-- react-text: 2776 -->O.<!-- /react-text --><!-- react-text: 2777 --> <!-- /react-text --><!-- react-text: 2778 -->Chapelle<!-- /react-text --><!-- react-text: 2779 -->, <!-- /react-text --><!-- react-text: 2780 -->D.<!-- /react-text --><!-- react-text: 2781 --> <!-- /react-text --><!-- react-text: 2782 -->DeCoste<!-- /react-text --><!-- react-text: 2783 -->, <!-- /react-text --><!-- react-text: 2784 -->J.<!-- /react-text --><!-- react-text: 2785 --> <!-- /react-text --><!-- react-text: 2786 -->Weston<!-- /react-text --><!-- react-text: 2787 --> (Eds.)<!-- /react-text --><!-- react-text: 2788 -->, <!-- /react-text --><!-- react-text: 2789 -->Large-Scale Kernel Machines<!-- /react-text --><!-- react-text: 2790 -->, <!-- /react-text --><!-- react-text: 2791 -->MIT Press<!-- /react-text --><!-- react-text: 2792 --> (<!-- /react-text --><!-- react-text: 2793 -->2007<!-- /react-text --><!-- react-text: 2794 -->)<!-- /react-text --></div></dd><dt class="label"><a href="#bb0045" id="ref-id-b0045">[9]</a></dt><dd class="reference" id="h0290"><div class="contribution"><!-- react-text: 2799 -->Y.<!-- /react-text --><!-- react-text: 2800 --> <!-- /react-text --><!-- react-text: 2801 -->Bengio<!-- /react-text --><!-- react-text: 2802 -->, <!-- /react-text --><!-- react-text: 2803 -->P.<!-- /react-text --><!-- react-text: 2804 --> <!-- /react-text --><!-- react-text: 2805 -->Simard<!-- /react-text --><!-- react-text: 2806 -->, <!-- /react-text --><!-- react-text: 2807 -->P.<!-- /react-text --><!-- react-text: 2808 --> <!-- /react-text --><!-- react-text: 2809 -->Frasconi<!-- /react-text --><strong class="title"><!-- react-text: 2811 -->Learning longterm dependencies with gradient descent is difficult<!-- /react-text --></strong></div><div class="host"><!-- react-text: 2813 -->IEEE Trans. Neural Networks<!-- /react-text --><!-- react-text: 2814 -->, <!-- /react-text --><!-- react-text: 2815 -->5<!-- /react-text --><!-- react-text: 2816 --> (<!-- /react-text --><!-- react-text: 2817 -->2<!-- /react-text --><!-- react-text: 2818 -->)<!-- /react-text --><!-- react-text: 2819 --> (<!-- /react-text --><!-- react-text: 2820 -->1994<!-- /react-text --><!-- react-text: 2821 -->)<!-- /react-text --><!-- react-text: 2822 -->, pp. <!-- /react-text --><!-- react-text: 2823 -->157<!-- /react-text --><!-- react-text: 2824 -->-<!-- /react-text --><!-- react-text: 2825 -->166<!-- /react-text --></div></dd><dt class="label"><a href="#bb0050" id="ref-id-b0050">[10]</a></dt><dd class="reference" id="h0015"><span>Y. Bengio, L. Yao, G. Alain, P. Vincent, Generalized denoising auto-encoders as generative models. CoRR abs/1305.6663, 2013.</span></dd><dt class="label"><a href="#bb0055" id="ref-id-b0055">[11]</a></dt><dd class="reference" id="h0295"><div class="contribution"><!-- react-text: 2834 -->V.H.<!-- /react-text --><!-- react-text: 2835 --> <!-- /react-text --><!-- react-text: 2836 -->Bennetts<!-- /react-text --><!-- react-text: 2837 -->, <!-- /react-text --><!-- react-text: 2838 -->A.J.<!-- /react-text --><!-- react-text: 2839 --> <!-- /react-text --><!-- react-text: 2840 -->Lilienthal<!-- /react-text --><!-- react-text: 2841 -->, <!-- /react-text --><!-- react-text: 2842 -->P.P.<!-- /react-text --><!-- react-text: 2843 --> <!-- /react-text --><!-- react-text: 2844 -->Neumann<!-- /react-text --><!-- react-text: 2845 -->, <!-- /react-text --><!-- react-text: 2846 -->M.<!-- /react-text --><!-- react-text: 2847 --> <!-- /react-text --><!-- react-text: 2848 -->Trincavelli<!-- /react-text --><strong class="title"><!-- react-text: 2850 -->Mobile robots for localizing gas emission sources on landfill sites: is bio-inspiration the way to go?<!-- /react-text --></strong></div><div class="host"><!-- react-text: 2852 -->Front. Neuroeng.<!-- /react-text --><!-- react-text: 2853 -->, <!-- /react-text --><!-- react-text: 2854 -->4<!-- /react-text --><!-- react-text: 2855 --> (<!-- /react-text --><!-- react-text: 2856 -->2011<!-- /react-text --><!-- react-text: 2857 -->)<!-- /react-text --></div></dd><dt class="label"><a href="#bb0060" id="ref-id-b0060">[12]</a></dt><dd class="reference" id="h0300"><div class="contribution"><!-- react-text: 2862 -->N.<!-- /react-text --><!-- react-text: 2863 --> <!-- /react-text --><!-- react-text: 2864 -->Bhattacharya<!-- /react-text --><!-- react-text: 2865 -->, <!-- /react-text --><!-- react-text: 2866 -->B.<!-- /react-text --><!-- react-text: 2867 --> <!-- /react-text --><!-- react-text: 2868 -->Tudu<!-- /react-text --><!-- react-text: 2869 -->, <!-- /react-text --><!-- react-text: 2870 -->A.<!-- /react-text --><!-- react-text: 2871 --> <!-- /react-text --><!-- react-text: 2872 -->Jana<!-- /react-text --><!-- react-text: 2873 -->, <!-- /react-text --><!-- react-text: 2874 -->D.<!-- /react-text --><!-- react-text: 2875 --> <!-- /react-text --><!-- react-text: 2876 -->Ghosh<!-- /react-text --><!-- react-text: 2877 -->, <!-- /react-text --><!-- react-text: 2878 -->R.<!-- /react-text --><!-- react-text: 2879 --> <!-- /react-text --><!-- react-text: 2880 -->Bandhopadhyaya<!-- /react-text --><!-- react-text: 2881 -->, <!-- /react-text --><!-- react-text: 2882 -->M.<!-- /react-text --><!-- react-text: 2883 --> <!-- /react-text --><!-- react-text: 2884 -->Bhuyan<!-- /react-text --><strong class="title"><!-- react-text: 2886 -->Preemptive identification of optimum fermentation time for black tea using electronic nose<!-- /react-text --></strong></div><div class="host"><!-- react-text: 2888 -->Sens. Actuators B: Chem.<!-- /react-text --><!-- react-text: 2889 -->, <!-- /react-text --><!-- react-text: 2890 -->131<!-- /react-text --><!-- react-text: 2891 --> (<!-- /react-text --><!-- react-text: 2892 -->2008<!-- /react-text --><!-- react-text: 2893 -->)<!-- /react-text --><!-- react-text: 2894 -->, pp. <!-- /react-text --><!-- react-text: 2895 -->110<!-- /react-text --><!-- react-text: 2896 -->-<!-- /react-text --><!-- react-text: 2897 -->116<!-- /react-text --></div></dd><dt class="label"><a href="#bb0065" id="ref-id-b0065">[13]</a></dt><dd class="reference" id="h0020"><span>Bo
 Chen, Jo-Anne Ting, B. Marlin, N. de Freitas, Deep learning of 
invariant spatio-temporal features from video, in: NIPS 2010 Deep 
Learning and Unsupervised Feature Learning Workshop, 2010.</span></dd><dt class="label"><a href="#bb0070" id="ref-id-b0070">[14]</a></dt><dd class="reference" id="h0305"><div class="contribution"><!-- react-text: 2906 -->J.<!-- /react-text --><!-- react-text: 2907 --> <!-- /react-text --><!-- react-text: 2908 -->Bollen<!-- /react-text --><!-- react-text: 2909 -->, <!-- /react-text --><!-- react-text: 2910 -->H.<!-- /react-text --><!-- react-text: 2911 --> <!-- /react-text --><!-- react-text: 2912 -->Mao<!-- /react-text --><!-- react-text: 2913 -->, <!-- /react-text --><!-- react-text: 2914 -->X.<!-- /react-text --><!-- react-text: 2915 --> <!-- /react-text --><!-- react-text: 2916 -->Zeng<!-- /react-text --><strong class="title"><!-- react-text: 2918 -->Twitter mood predicts the stock market<!-- /react-text --></strong></div><div class="host"><!-- react-text: 2920 -->J. Comput. Sci.<!-- /react-text --><!-- react-text: 2921 -->, <!-- /react-text --><!-- react-text: 2922 -->2<!-- /react-text --><!-- react-text: 2923 --> (<!-- /react-text --><!-- react-text: 2924 -->2011<!-- /react-text --><!-- react-text: 2925 -->)<!-- /react-text --><!-- react-text: 2926 -->, pp. <!-- /react-text --><!-- react-text: 2927 -->1<!-- /react-text --><!-- react-text: 2928 -->-<!-- /react-text --><!-- react-text: 2929 -->8<!-- /react-text --></div></dd><dt class="label"><a href="#bb0075" id="ref-id-b0075">[15]</a></dt><dd class="reference" id="h0310"><div class="contribution"><!-- react-text: 2934 -->L.<!-- /react-text --><!-- react-text: 2935 --> <!-- /react-text --><!-- react-text: 2936 -->Bottou<!-- /react-text --><strong class="title"><!-- react-text: 2938 -->Large-scale machine learning with stochastic gradient descent<!-- /react-text --></strong></div><div class="host"><!-- react-text: 2940 -->Y.<!-- /react-text --><!-- react-text: 2941 --> <!-- /react-text --><!-- react-text: 2942 -->Lechevallier<!-- /react-text --><!-- react-text: 2943 -->, <!-- /react-text --><!-- react-text: 2944 -->G.<!-- /react-text --><!-- react-text: 2945 --> <!-- /react-text --><!-- react-text: 2946 -->Saporta<!-- /react-text --><!-- react-text: 2947 --> (Eds.)<!-- /react-text --><!-- react-text: 2948 -->, <!-- /react-text --><!-- react-text: 2949 -->Proceedings of the 19th International Conference on Computational Statistics (COMPSTAT’2010)<!-- /react-text --><!-- react-text: 2950 -->, <!-- /react-text --><!-- react-text: 2951 -->Springer<!-- /react-text --><!-- react-text: 2952 -->, <!-- /react-text --><!-- react-text: 2953 -->Paris, France<!-- /react-text --><!-- react-text: 2954 --> (<!-- /react-text --><!-- react-text: 2955 -->2010<!-- /react-text --><!-- react-text: 2956 -->)<!-- /react-text --><!-- react-text: 2957 -->, pp. <!-- /react-text --><!-- react-text: 2958 -->177<!-- /react-text --><!-- react-text: 2959 -->-<!-- /react-text --><!-- react-text: 2960 -->187<!-- /react-text --></div></dd><dt class="label"><a href="#bb0080" id="ref-id-b0080">[16]</a></dt><dd class="reference" id="h0315"><div class="contribution"><!-- react-text: 2965 -->M.<!-- /react-text --><!-- react-text: 2966 --> <!-- /react-text --><!-- react-text: 2967 -->Brand<!-- /react-text --><!-- react-text: 2968 -->, <!-- /react-text --><!-- react-text: 2969 -->A.<!-- /react-text --><!-- react-text: 2970 --> <!-- /react-text --><!-- react-text: 2971 -->Hertzmann<!-- /react-text --><strong class="title"><!-- react-text: 2973 -->Style machines<!-- /react-text --></strong></div><div class="host"><!-- react-text: 2975 -->Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques<!-- /react-text --><!-- react-text: 2976 -->, <!-- /react-text --><!-- react-text: 2977 -->ACM Press/Addison-Wesley Publishing Co.<!-- /react-text --><!-- react-text: 2978 -->, <!-- /react-text --><!-- react-text: 2979 -->New York, NY, USA<!-- /react-text --><!-- react-text: 2980 --> (<!-- /react-text --><!-- react-text: 2981 -->2000<!-- /react-text --><!-- react-text: 2982 -->)<!-- /react-text --><!-- react-text: 2983 -->, pp. <!-- /react-text --><!-- react-text: 2984 -->183<!-- /react-text --><!-- react-text: 2985 -->-<!-- /react-text --><!-- react-text: 2986 -->192<!-- /react-text --></div></dd><dt class="label"><a href="#bb0085" id="ref-id-b0085">[17]</a></dt><dd class="reference" id="h0320"><div class="contribution"><!-- react-text: 2991 -->M.<!-- /react-text --><!-- react-text: 2992 --> <!-- /react-text --><!-- react-text: 2993 -->Carmona<!-- /react-text --><!-- react-text: 2994 -->, <!-- /react-text --><!-- react-text: 2995 -->J.<!-- /react-text --><!-- react-text: 2996 --> <!-- /react-text --><!-- react-text: 2997 -->Martinez<!-- /react-text --><!-- react-text: 2998 -->, <!-- /react-text --><!-- react-text: 2999 -->A.<!-- /react-text --><!-- react-text: 3000 --> <!-- /react-text --><!-- react-text: 3001 -->Zalacain<!-- /react-text --><!-- react-text: 3002 -->, <!-- /react-text --><!-- react-text: 3003 -->M.L.<!-- /react-text --><!-- react-text: 3004 --> <!-- /react-text --><!-- react-text: 3005 -->Rodriguez-Mendez<!-- /react-text --><!-- react-text: 3006 -->, <!-- /react-text --><!-- react-text: 3007 -->J.A.<!-- /react-text --><!-- react-text: 3008 --> <!-- /react-text --><!-- react-text: 3009 -->de Saja<!-- /react-text --><!-- react-text: 3010 -->, <!-- /react-text --><!-- react-text: 3011 -->G.L.<!-- /react-text --><!-- react-text: 3012 --> <!-- /react-text --><!-- react-text: 3013 -->Alonso<!-- /react-text --><strong class="title"><!-- react-text: 3015 -->Analysis of saffron volatile fraction by td–gc–ms and e-nose<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3017 -->Eur. Food Res. Technol.<!-- /react-text --><!-- react-text: 3018 -->, <!-- /react-text --><!-- react-text: 3019 -->223<!-- /react-text --><!-- react-text: 3020 --> (<!-- /react-text --><!-- react-text: 3021 -->2006<!-- /react-text --><!-- react-text: 3022 -->)<!-- /react-text --><!-- react-text: 3023 -->, pp. <!-- /react-text --><!-- react-text: 3024 -->96<!-- /react-text --><!-- react-text: 3025 -->-<!-- /react-text --><!-- react-text: 3026 -->101<!-- /react-text --></div></dd><dt class="label"><a href="#bb0090" id="ref-id-b0090">[18]</a></dt><dd class="reference" id="h0025"><span>K.
 Chang, J. Jang, C. Iliopoulos, Music genre classification via 
compressive sampling, in: Proceedings of the 11th International 
Conference on Music Information Retrieval (ISMIR), 2010, pp. 387–392.</span></dd><dt class="label"><a href="#bb0095" id="ref-id-b0095">[19]</a></dt><dd class="reference" id="h0325"><div class="contribution"><!-- react-text: 3035 -->Y.<!-- /react-text --><!-- react-text: 3036 --> <!-- /react-text --><!-- react-text: 3037 -->Cheung<!-- /react-text --><!-- react-text: 3038 -->, <!-- /react-text --><!-- react-text: 3039 -->L.<!-- /react-text --><!-- react-text: 3040 --> <!-- /react-text --><!-- react-text: 3041 -->Xu<!-- /react-text --><strong class="title"><!-- react-text: 3043 -->Independent component ordering in ica time series analysis<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3045 -->Neurocomputing<!-- /react-text --><!-- react-text: 3046 -->, <!-- /react-text --><!-- react-text: 3047 -->41<!-- /react-text --><!-- react-text: 3048 --> (<!-- /react-text --><!-- react-text: 3049 -->2001<!-- /react-text --><!-- react-text: 3050 -->)<!-- /react-text --><!-- react-text: 3051 -->, pp. <!-- /react-text --><!-- react-text: 3052 -->145<!-- /react-text --><!-- react-text: 3053 -->-<!-- /react-text --><!-- react-text: 3054 -->152<!-- /react-text --></div></dd><dt class="label"><a href="#bb0100" id="ref-id-b0100">[20]</a></dt><dd class="reference" id="h0030"><span>S.
 Chiappa, J. Kober, J. Peters, Using Bayesian dynamical systems for 
motion template libraries, in: Advances in Neural Information Processing
 Systems, vol. 21, 2009, 297–304.</span></dd><dt class="label"><a href="#bb0105" id="ref-id-b0105">[21]</a></dt><dd class="reference" id="h0330"><div class="contribution"><!-- react-text: 3063 -->A.<!-- /react-text --><!-- react-text: 3064 --> <!-- /react-text --><!-- react-text: 3065 -->Coates<!-- /react-text --><!-- react-text: 3066 -->, <!-- /react-text --><!-- react-text: 3067 -->H.<!-- /react-text --><!-- react-text: 3068 --> <!-- /react-text --><!-- react-text: 3069 -->Lee<!-- /react-text --><!-- react-text: 3070 -->, <!-- /react-text --><!-- react-text: 3071 -->A.Y.<!-- /react-text --><!-- react-text: 3072 --> <!-- /react-text --><!-- react-text: 3073 -->Ng<!-- /react-text --><strong class="title"><!-- react-text: 3075 -->An analysis of single-layer networks in unsupervised feature learning<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3077 -->Engineering<!-- /react-text --><!-- react-text: 3078 --> (<!-- /react-text --><!-- react-text: 3079 -->2010<!-- /react-text --><!-- react-text: 3080 -->)<!-- /react-text --><!-- react-text: 3081 -->, pp. <!-- /react-text --><!-- react-text: 3082 -->1<!-- /react-text --><!-- react-text: 3083 -->-<!-- /react-text --><!-- react-text: 3084 -->9<!-- /react-text --></div></dd><dt class="label"><a href="#bb0110" id="ref-id-b0110">[22]</a></dt><dd class="reference" id="h0335"><div class="contribution"><!-- react-text: 3089 -->G.<!-- /react-text --><!-- react-text: 3090 --> <!-- /react-text --><!-- react-text: 3091 -->Dahl<!-- /react-text --><!-- react-text: 3092 -->, <!-- /react-text --><!-- react-text: 3093 -->D.<!-- /react-text --><!-- react-text: 3094 --> <!-- /react-text --><!-- react-text: 3095 -->Yu<!-- /react-text --><!-- react-text: 3096 -->, <!-- /react-text --><!-- react-text: 3097 -->L.<!-- /react-text --><!-- react-text: 3098 --> <!-- /react-text --><!-- react-text: 3099 -->Deng<!-- /react-text --><!-- react-text: 3100 -->, <!-- /react-text --><!-- react-text: 3101 -->A.<!-- /react-text --><!-- react-text: 3102 --> <!-- /react-text --><!-- react-text: 3103 -->Acero<!-- /react-text --><strong class="title"><!-- react-text: 3105 -->Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3107 -->IEEE Transactions on Audio, Speech, and Language Processing<!-- /react-text --><!-- react-text: 3108 -->, <!-- /react-text --><!-- react-text: 3109 -->20<!-- /react-text --><!-- react-text: 3110 --> (<!-- /react-text --><!-- react-text: 3111 -->2012<!-- /react-text --><!-- react-text: 3112 -->)<!-- /react-text --><!-- react-text: 3113 -->, pp. <!-- /react-text --><!-- react-text: 3114 -->30<!-- /react-text --><!-- react-text: 3115 -->-<!-- /react-text --><!-- react-text: 3116 -->42<!-- /react-text --></div></dd><dt class="label"><a href="#bb0115" id="ref-id-b0115">[23]</a></dt><dd class="reference" id="h0340"><div class="contribution"><!-- react-text: 3121 -->G.E.<!-- /react-text --><!-- react-text: 3122 --> <!-- /react-text --><!-- react-text: 3123 -->Dahl<!-- /react-text --><!-- react-text: 3124 -->, <!-- /react-text --><!-- react-text: 3125 -->M.<!-- /react-text --><!-- react-text: 3126 --> <!-- /react-text --><!-- react-text: 3127 -->Ranzato<!-- /react-text --><!-- react-text: 3128 -->, <!-- /react-text --><!-- react-text: 3129 -->A.<!-- /react-text --><!-- react-text: 3130 --> <!-- /react-text --><!-- react-text: 3131 -->Mohamed<!-- /react-text --><!-- react-text: 3132 -->, <!-- /react-text --><!-- react-text: 3133 -->G.<!-- /react-text --><!-- react-text: 3134 --> <!-- /react-text --><!-- react-text: 3135 -->Hinton<!-- /react-text --><strong class="title"><!-- react-text: 3137 -->Phone recognition with the mean-covariance restricted Boltzmann machine<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3139 -->Adv. Neural Inf. Process. Syst.<!-- /react-text --><!-- react-text: 3140 -->, <!-- /react-text --><!-- react-text: 3141 -->23<!-- /react-text --><!-- react-text: 3142 --> (<!-- /react-text --><!-- react-text: 3143 -->2010<!-- /react-text --><!-- react-text: 3144 -->)<!-- /react-text --><!-- react-text: 3145 -->, pp. <!-- /react-text --><!-- react-text: 3146 -->469<!-- /react-text --><!-- react-text: 3147 -->-<!-- /react-text --><!-- react-text: 3148 -->477<!-- /react-text --></div></dd><dt class="label"><a href="#bb0120" id="ref-id-b0120">[24]</a></dt><dd class="reference" id="h0035"><span>N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, in: CVPR, 2005.</span></dd><dt class="label"><a href="#bb0125" id="ref-id-b0125">[25]</a></dt><dd class="reference" id="h0040"><span>S.
 Dieleman, P. Brakel, B. Schrauwen, Audio-based music classification 
with a pretrained convolutional network, in: The International Society 
for Music Information Retrieval (ISMIR), 2011.</span></dd><dt class="label"><a href="#bb0130" id="ref-id-b0130">[26]</a></dt><dd class="reference" id="h0345"><div class="contribution"><!-- react-text: 3161 -->T.G.<!-- /react-text --><!-- react-text: 3162 --> <!-- /react-text --><!-- react-text: 3163 -->Dietterich<!-- /react-text --><strong class="title"><!-- react-text: 3165 -->Machine learning for sequential data: a review<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3167 -->Structural, Syntactic, and Statistical Pattern Recognition<!-- /react-text --><!-- react-text: 3168 -->, <!-- /react-text --><!-- react-text: 3169 -->Springer-Verlag<!-- /react-text --><!-- react-text: 3170 --> (<!-- /react-text --><!-- react-text: 3171 -->2002<!-- /react-text --><!-- react-text: 3172 -->)<!-- /react-text --><!-- react-text: 3173 -->, pp. <!-- /react-text --><!-- react-text: 3174 -->15<!-- /react-text --><!-- react-text: 3175 -->-<!-- /react-text --><!-- react-text: 3176 -->30<!-- /react-text --></div></dd><dt class="label"><a href="#bb0135" id="ref-id-b0135">[27]</a></dt><dd class="reference" id="h0045"><span>T.
 Duckett, M. Axelsson, A. Saffiotti, Learning to locate an odour source 
with a mobile robot, in: IEEE International Conference on Robotics and 
Automation, 2001. Proceedings 2001 ICRA, vol. 4, 2001, pp. 4017–4022.</span></dd><dt class="label"><a href="#bb0140" id="ref-id-b0140">[28]</a></dt><dd class="reference" id="h0350"><div class="contribution"><!-- react-text: 3185 -->R.<!-- /react-text --><!-- react-text: 3186 --> <!-- /react-text --><!-- react-text: 3187 -->Dutta<!-- /react-text --><!-- react-text: 3188 -->, <!-- /react-text --><!-- react-text: 3189 -->E.<!-- /react-text --><!-- react-text: 3190 --> <!-- /react-text --><!-- react-text: 3191 -->Hines<!-- /react-text --><!-- react-text: 3192 -->, <!-- /react-text --><!-- react-text: 3193 -->J.<!-- /react-text --><!-- react-text: 3194 --> <!-- /react-text --><!-- react-text: 3195 -->Gardner<!-- /react-text --><!-- react-text: 3196 -->, <!-- /react-text --><!-- react-text: 3197 -->P.<!-- /react-text --><!-- react-text: 3198 --> <!-- /react-text --><!-- react-text: 3199 -->Boilot<!-- /react-text --><strong class="title"><!-- react-text: 3201 -->Bacteria classification using cyranose 320 electronic nose<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3203 -->Biomed. Eng.<!-- /react-text --><!-- react-text: 3204 -->, <!-- /react-text --><!-- react-text: 3205 -->1<!-- /react-text --><!-- react-text: 3206 --> (<!-- /react-text --><!-- react-text: 3207 -->2002<!-- /react-text --><!-- react-text: 3208 -->)<!-- /react-text --><!-- react-text: 3209 -->, p. <!-- /react-text --><!-- react-text: 3210 -->4<!-- /react-text --></div></dd><dt class="label"><a href="#bb0145" id="ref-id-b0145">[29]</a></dt><dd class="reference" id="h0355"><div class="contribution"><!-- react-text: 3215 -->D.<!-- /react-text --><!-- react-text: 3216 --> <!-- /react-text --><!-- react-text: 3217 -->Erhan<!-- /react-text --><!-- react-text: 3218 -->, <!-- /react-text --><!-- react-text: 3219 -->Y.<!-- /react-text --><!-- react-text: 3220 --> <!-- /react-text --><!-- react-text: 3221 -->Bengio<!-- /react-text --><!-- react-text: 3222 -->, <!-- /react-text --><!-- react-text: 3223 -->A.<!-- /react-text --><!-- react-text: 3224 --> <!-- /react-text --><!-- react-text: 3225 -->Courville<!-- /react-text --><!-- react-text: 3226 -->, <!-- /react-text --><!-- react-text: 3227 -->P.<!-- /react-text --><!-- react-text: 3228 --> <!-- /react-text --><!-- react-text: 3229 -->Manzagol<!-- /react-text --><!-- react-text: 3230 -->, <!-- /react-text --><!-- react-text: 3231 -->P.<!-- /react-text --><!-- react-text: 3232 --> <!-- /react-text --><!-- react-text: 3233 -->Vincent<!-- /react-text --><!-- react-text: 3234 -->, <!-- /react-text --><!-- react-text: 3235 -->S.<!-- /react-text --><!-- react-text: 3236 --> <!-- /react-text --><!-- react-text: 3237 -->Bengio<!-- /react-text --><strong class="title"><!-- react-text: 3239 -->Why does unsupervised pre-training help deep learning?<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3241 -->J. Mach. Learn. Res.<!-- /react-text --><!-- react-text: 3242 -->, <!-- /react-text --><!-- react-text: 3243 -->11<!-- /react-text --><!-- react-text: 3244 --> (<!-- /react-text --><!-- react-text: 3245 -->2010<!-- /react-text --><!-- react-text: 3246 -->)<!-- /react-text --><!-- react-text: 3247 -->, pp. <!-- /react-text --><!-- react-text: 3248 -->625<!-- /react-text --><!-- react-text: 3249 -->-<!-- /react-text --><!-- react-text: 3250 -->660<!-- /react-text --></div></dd><dt class="label"><a href="#bb0150" id="ref-id-b0150">[30]</a></dt><dd class="reference" id="h0360"><div class="contribution"><!-- react-text: 3255 -->E.F.<!-- /react-text --><!-- react-text: 3256 --> <!-- /react-text --><!-- react-text: 3257 -->Fama<!-- /react-text --><strong class="title"><!-- react-text: 3259 -->The behavior of stock-market prices<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3261 -->J. Bus.<!-- /react-text --><!-- react-text: 3262 -->, <!-- /react-text --><!-- react-text: 3263 -->1<!-- /react-text --><!-- react-text: 3264 --> (<!-- /react-text --><!-- react-text: 3265 -->1965<!-- /react-text --><!-- react-text: 3266 -->)<!-- /react-text --><!-- react-text: 3267 -->, pp. <!-- /react-text --><!-- react-text: 3268 -->34<!-- /react-text --><!-- react-text: 3269 -->-<!-- /react-text --><!-- react-text: 3270 -->105<!-- /react-text --></div></dd><dt class="label"><a href="#bb0155" id="ref-id-b0155">[31]</a></dt><dd class="reference" id="h0365"><div class="contribution"><!-- react-text: 3275 -->T.<!-- /react-text --><!-- react-text: 3276 --> <!-- /react-text --><!-- react-text: 3277 -->Flash<!-- /react-text --><!-- react-text: 3278 -->, <!-- /react-text --><!-- react-text: 3279 -->B.<!-- /react-text --><!-- react-text: 3280 --> <!-- /react-text --><!-- react-text: 3281 -->Hochner<!-- /react-text --><strong class="title"><!-- react-text: 3283 -->Motor primitives in vertebrates and invertebrates<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3285 -->Curr. Opin. Neurobiol.<!-- /react-text --><!-- react-text: 3286 -->, <!-- /react-text --><!-- react-text: 3287 -->15<!-- /react-text --><!-- react-text: 3288 --> (<!-- /react-text --><!-- react-text: 3289 -->6<!-- /react-text --><!-- react-text: 3290 -->)<!-- /react-text --><!-- react-text: 3291 --> (<!-- /react-text --><!-- react-text: 3292 -->2005<!-- /react-text --><!-- react-text: 3293 -->)<!-- /react-text --><!-- react-text: 3294 -->, pp. <!-- /react-text --><!-- react-text: 3295 -->660<!-- /react-text --><!-- react-text: 3296 -->-<!-- /react-text --><!-- react-text: 3297 -->666<!-- /react-text --></div></dd><dt class="label"><a href="#bb0160" id="ref-id-b0160">[32]</a></dt><dd class="reference" id="h0370"><div class="contribution"><!-- react-text: 3302 -->S.<!-- /react-text --><!-- react-text: 3303 --> <!-- /react-text --><!-- react-text: 3304 -->Furui<!-- /react-text --><!-- react-text: 3305 -->, <!-- /react-text --><!-- react-text: 3306 -->T.<!-- /react-text --><!-- react-text: 3307 --> <!-- /react-text --><!-- react-text: 3308 -->Kikuchi<!-- /react-text --><!-- react-text: 3309 -->, <!-- /react-text --><!-- react-text: 3310 -->Y.<!-- /react-text --><!-- react-text: 3311 --> <!-- /react-text --><!-- react-text: 3312 -->Shinnaka<!-- /react-text --><!-- react-text: 3313 -->, <!-- /react-text --><!-- react-text: 3314 -->C.<!-- /react-text --><!-- react-text: 3315 --> <!-- /react-text --><!-- react-text: 3316 -->Hori<!-- /react-text --><strong class="title"><!-- react-text: 3318 -->Speech-to-text and speech-to-speech summarization of spontaneous speech<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3320 -->IEEE Trans. Speech Audio Process.<!-- /react-text --><!-- react-text: 3321 -->, <!-- /react-text --><!-- react-text: 3322 -->12<!-- /react-text --><!-- react-text: 3323 --> (<!-- /react-text --><!-- react-text: 3324 -->2004<!-- /react-text --><!-- react-text: 3325 -->)<!-- /react-text --><!-- react-text: 3326 -->, pp. <!-- /react-text --><!-- react-text: 3327 -->401<!-- /react-text --><!-- react-text: 3328 -->-<!-- /react-text --><!-- react-text: 3329 -->408<!-- /react-text --></div></dd><dt class="label"><a href="#bb0165" id="ref-id-b0165">[33]</a></dt><dd class="reference" id="h0375"><div class="contribution"><!-- react-text: 3334 -->J.<!-- /react-text --><!-- react-text: 3335 --> <!-- /react-text --><!-- react-text: 3336 -->Gardner<!-- /react-text --><!-- react-text: 3337 -->, <!-- /react-text --><!-- react-text: 3338 -->P.<!-- /react-text --><!-- react-text: 3339 --> <!-- /react-text --><!-- react-text: 3340 -->Bartlett<!-- /react-text --><strong class="title"><!-- react-text: 3342 -->Electronic Noses, Principles and Applications<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3344 -->Oxford University Press<!-- /react-text --><!-- react-text: 3345 -->, <!-- /react-text --><!-- react-text: 3346 -->New York, NY, USA<!-- /react-text --><!-- react-text: 3347 --> (<!-- /react-text --><!-- react-text: 3348 -->1999<!-- /react-text --><!-- react-text: 3349 -->)<!-- /react-text --></div></dd><dt class="label"><a href="#bb0170" id="ref-id-b0170">[34]</a></dt><dd class="reference" id="h0380"><div class="contribution"><!-- react-text: 3354 -->J.W.<!-- /react-text --><!-- react-text: 3355 --> <!-- /react-text --><!-- react-text: 3356 -->Gardner<!-- /react-text --><!-- react-text: 3357 -->, <!-- /react-text --><!-- react-text: 3358 -->H.W.<!-- /react-text --><!-- react-text: 3359 --> <!-- /react-text --><!-- react-text: 3360 -->Shin<!-- /react-text --><!-- react-text: 3361 -->, <!-- /react-text --><!-- react-text: 3362 -->E.L.<!-- /react-text --><!-- react-text: 3363 --> <!-- /react-text --><!-- react-text: 3364 -->Hines<!-- /react-text --><strong class="title"><!-- react-text: 3366 -->An electronic nose system to diagnose illness<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3368 -->Sens. Actuators B: Chem.<!-- /react-text --><!-- react-text: 3369 -->, <!-- /react-text --><!-- react-text: 3370 -->70<!-- /react-text --><!-- react-text: 3371 --> (<!-- /react-text --><!-- react-text: 3372 -->2000<!-- /react-text --><!-- react-text: 3373 -->)<!-- /react-text --><!-- react-text: 3374 -->, pp. <!-- /react-text --><!-- react-text: 3375 -->19<!-- /react-text --><!-- react-text: 3376 -->-<!-- /react-text --><!-- react-text: 3377 -->24<!-- /react-text --></div></dd><dt class="label"><a href="#bb0175" id="ref-id-b0175">[35]</a></dt><dd class="reference" id="h0385"><div class="contribution"><!-- react-text: 3382 -->J.W.<!-- /react-text --><!-- react-text: 3383 --> <!-- /react-text --><!-- react-text: 3384 -->Gardner<!-- /react-text --><!-- react-text: 3385 -->, <!-- /react-text --><!-- react-text: 3386 -->H.W.<!-- /react-text --><!-- react-text: 3387 --> <!-- /react-text --><!-- react-text: 3388 -->Shin<!-- /react-text --><!-- react-text: 3389 -->, <!-- /react-text --><!-- react-text: 3390 -->E.L.<!-- /react-text --><!-- react-text: 3391 --> <!-- /react-text --><!-- react-text: 3392 -->Hines<!-- /react-text --><!-- react-text: 3393 -->, <!-- /react-text --><!-- react-text: 3394 -->C.S.<!-- /react-text --><!-- react-text: 3395 --> <!-- /react-text --><!-- react-text: 3396 -->Dow<!-- /react-text --><strong class="title"><!-- react-text: 3398 -->An electronic nose system for monitoring the quality of potable water<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3400 -->Sens. Actuators B: Chem.<!-- /react-text --><!-- react-text: 3401 -->, <!-- /react-text --><!-- react-text: 3402 -->69<!-- /react-text --><!-- react-text: 3403 --> (<!-- /react-text --><!-- react-text: 3404 -->2000<!-- /react-text --><!-- react-text: 3405 -->)<!-- /react-text --><!-- react-text: 3406 -->, pp. <!-- /react-text --><!-- react-text: 3407 -->336<!-- /react-text --><!-- react-text: 3408 -->-<!-- /react-text --><!-- react-text: 3409 -->341<!-- /react-text --></div></dd><dt class="label"><a href="#bb0180" id="ref-id-b0180">[36]</a></dt><dd class="reference" id="h0390"><div class="contribution"><!-- react-text: 3414 -->T.<!-- /react-text --><!-- react-text: 3415 --> <!-- /react-text --><!-- react-text: 3416 -->Gärtner<!-- /react-text --><strong class="title"><!-- react-text: 3418 -->A survey of kernels for structured data<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3420 -->SIGKDD Explor. Newslett.<!-- /react-text --><!-- react-text: 3421 -->, <!-- /react-text --><!-- react-text: 3422 -->5<!-- /react-text --><!-- react-text: 3423 --> (<!-- /react-text --><!-- react-text: 3424 -->2003<!-- /react-text --><!-- react-text: 3425 -->)<!-- /react-text --><!-- react-text: 3426 -->, pp. <!-- /react-text --><!-- react-text: 3427 -->49<!-- /react-text --><!-- react-text: 3428 -->-<!-- /react-text --><!-- react-text: 3429 -->58<!-- /react-text --></div></dd><dt class="label"><a href="#bb0185" id="ref-id-b0185">[37]</a></dt><dd class="reference" id="h0395"><div class="contribution"><!-- react-text: 3434 -->F.A.<!-- /react-text --><!-- react-text: 3435 --> <!-- /react-text --><!-- react-text: 3436 -->Gers<!-- /react-text --><!-- react-text: 3437 -->, <!-- /react-text --><!-- react-text: 3438 -->J.<!-- /react-text --><!-- react-text: 3439 --> <!-- /react-text --><!-- react-text: 3440 -->Schmidhuber<!-- /react-text --><!-- react-text: 3441 -->, <!-- /react-text --><!-- react-text: 3442 -->F.<!-- /react-text --><!-- react-text: 3443 --> <!-- /react-text --><!-- react-text: 3444 -->Cummins<!-- /react-text --><strong class="title"><!-- react-text: 3446 -->Learning to forget: continual prediction with LSTM<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3448 -->Neural Comput.<!-- /react-text --><!-- react-text: 3449 -->, <!-- /react-text --><!-- react-text: 3450 -->12<!-- /react-text --><!-- react-text: 3451 --> (<!-- /react-text --><!-- react-text: 3452 -->2000<!-- /react-text --><!-- react-text: 3453 -->)<!-- /react-text --><!-- react-text: 3454 -->, pp. <!-- /react-text --><!-- react-text: 3455 -->2451<!-- /react-text --><!-- react-text: 3456 -->-<!-- /react-text --><!-- react-text: 3457 -->2471<!-- /react-text --></div></dd><dt class="label"><a href="#bb0190" id="ref-id-b0190">[38]</a></dt><dd class="reference" id="h0400"><div class="contribution"><!-- react-text: 3462 -->M.<!-- /react-text --><!-- react-text: 3463 --> <!-- /react-text --><!-- react-text: 3464 -->Gleicher<!-- /react-text --><strong class="title"><!-- react-text: 3466 -->Animation from observation: motion capture and motion editing<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3468 -->SIGGRAPH Comput. Graph.<!-- /react-text --><!-- react-text: 3469 -->, <!-- /react-text --><!-- react-text: 3470 -->33<!-- /react-text --><!-- react-text: 3471 --> (<!-- /react-text --><!-- react-text: 3472 -->2000<!-- /react-text --><!-- react-text: 3473 -->)<!-- /react-text --><!-- react-text: 3474 -->, pp. <!-- /react-text --><!-- react-text: 3475 -->51<!-- /react-text --><!-- react-text: 3476 -->-<!-- /react-text --><!-- react-text: 3477 -->54<!-- /react-text --></div></dd><dt class="label"><a href="#bb0195" id="ref-id-b0195">[39]</a></dt><dd class="reference" id="h0405"><div class="contribution"><!-- react-text: 3482 -->A.L.<!-- /react-text --><!-- react-text: 3483 --> <!-- /react-text --><!-- react-text: 3484 -->Goldberger<!-- /react-text --><!-- react-text: 3485 -->, <!-- /react-text --><!-- react-text: 3486 -->L.A.N.<!-- /react-text --><!-- react-text: 3487 --> <!-- /react-text --><!-- react-text: 3488 -->Amaral<!-- /react-text --><!-- react-text: 3489 -->, <!-- /react-text --><!-- react-text: 3490 -->L.<!-- /react-text --><!-- react-text: 3491 --> <!-- /react-text --><!-- react-text: 3492 -->Glass<!-- /react-text --><!-- react-text: 3493 -->, <!-- /react-text --><!-- react-text: 3494 -->J.M.<!-- /react-text --><!-- react-text: 3495 --> <!-- /react-text --><!-- react-text: 3496 -->Hausdorff<!-- /react-text --><!-- react-text: 3497 -->, <!-- /react-text --><!-- react-text: 3498 -->P.C.<!-- /react-text --><!-- react-text: 3499 --> <!-- /react-text --><!-- react-text: 3500 -->Ivanov<!-- /react-text --><!-- react-text: 3501 -->, <!-- /react-text --><!-- react-text: 3502 -->R.G.<!-- /react-text --><!-- react-text: 3503 --> <!-- /react-text --><!-- react-text: 3504 -->Mark<!-- /react-text --><!-- react-text: 3505 -->, <!-- /react-text --><!-- react-text: 3506 -->J.E.<!-- /react-text --><!-- react-text: 3507 --> <!-- /react-text --><!-- react-text: 3508 -->Mietus<!-- /react-text --><!-- react-text: 3509 -->, <!-- /react-text --><!-- react-text: 3510 -->G.B.<!-- /react-text --><!-- react-text: 3511 --> <!-- /react-text --><!-- react-text: 3512 -->Moody<!-- /react-text --><!-- react-text: 3513 -->, <!-- /react-text --><!-- react-text: 3514 -->C.K.<!-- /react-text --><!-- react-text: 3515 --> <!-- /react-text --><!-- react-text: 3516 -->Peng<!-- /react-text --><!-- react-text: 3517 -->, <!-- /react-text --><!-- react-text: 3518 -->H.E.<!-- /react-text --><!-- react-text: 3519 --> <!-- /react-text --><!-- react-text: 3520 -->Stanley<!-- /react-text --><strong class="title"><!-- react-text: 3522 -->PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3524 -->Circulation<!-- /react-text --><!-- react-text: 3525 -->, <!-- /react-text --><!-- react-text: 3526 -->101<!-- /react-text --><!-- react-text: 3527 --> (<!-- /react-text --><!-- react-text: 3528 -->2000<!-- /react-text --><!-- react-text: 3529 -->)<!-- /react-text --><!-- react-text: 3530 -->, pp. <!-- /react-text --><!-- react-text: 3531 -->e215<!-- /react-text --><!-- react-text: 3532 -->-<!-- /react-text --><!-- react-text: 3533 -->e220<!-- /react-text --></div><div class="comment">Circulation Electronic Pages:</div><div class="host"><a href="http://circ.ahajournals.org/cgi/content/full/101/23/e215" target="_blank">&lt;http://circ.ahajournals.org/cgi/content/full/101/23/e215&gt;</a></div></dd><dt class="label"><a href="#bb0200" id="ref-id-b0200">[40]</a></dt><dd class="reference" id="h0415"><div class="contribution"><!-- react-text: 3541 -->F.S.<!-- /react-text --><!-- react-text: 3542 --> <!-- /react-text --><!-- react-text: 3543 -->Grassia<!-- /react-text --><strong class="title"><!-- react-text: 3545 -->Practical parameterization of rotations using the exponential map<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3547 -->J. Graph. Tools<!-- /react-text --><!-- react-text: 3548 -->, <!-- /react-text --><!-- react-text: 3549 -->3<!-- /react-text --><!-- react-text: 3550 --> (<!-- /react-text --><!-- react-text: 3551 -->1998<!-- /react-text --><!-- react-text: 3552 -->)<!-- /react-text --><!-- react-text: 3553 -->, pp. <!-- /react-text --><!-- react-text: 3554 -->29<!-- /react-text --><!-- react-text: 3555 -->-<!-- /react-text --><!-- react-text: 3556 -->48<!-- /react-text --></div></dd><dt class="label"><a href="#bb0205" id="ref-id-b0205">[41]</a></dt><dd class="reference" id="h0050"><span>A.
 Graves, A. Mohamed, G. Hinton, Speech recognition with deep recurrent 
neural networks, in: The 38th International Conference on Acoustics, 
Speech, and Signal Processing (ICASSP), 2013.</span></dd><dt class="label"><a href="#bb0210" id="ref-id-b0210">[42]</a></dt><dd class="reference" id="h0055"><span>R.
 Grosse, R. Raina, H. Kwong, A.Y. Ng, Shift-invariant sparse coding for 
audio classification, in: Conference on Uncertainty in Artificial 
Intelligence (UAI), 2007.</span></dd><dt class="label"><a href="#bb0215" id="ref-id-b0215">[43]</a></dt><dd class="reference" id="h0060"><span>D.
 Gruhl, R. Guha, R. Kumar, J. Novak, A. Tomkins, The predictive power of
 online chatter, in: Proceedings of the 11th ACM SIGKDD International 
Conference on Knowledge Discovery in Data Mining, 2005, pp. 78–87.</span></dd><dt class="label"><a href="#bb0220" id="ref-id-b0220">[44]</a></dt><dd class="reference" id="h0420"><div class="contribution"><!-- react-text: 3573 -->R.<!-- /react-text --><!-- react-text: 3574 --> <!-- /react-text --><!-- react-text: 3575 -->Gutierrez-Osuna<!-- /react-text --><strong class="title"><!-- react-text: 3577 -->Pattern analysis for machine olfaction: a review<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3579 -->IEEE Sens. J.<!-- /react-text --><!-- react-text: 3580 -->, <!-- /react-text --><!-- react-text: 3581 -->2<!-- /react-text --><!-- react-text: 3582 --> (<!-- /react-text --><!-- react-text: 3583 -->3<!-- /react-text --><!-- react-text: 3584 -->)<!-- /react-text --><!-- react-text: 3585 --> (<!-- /react-text --><!-- react-text: 3586 -->2002<!-- /react-text --><!-- react-text: 3587 -->)<!-- /react-text --><!-- react-text: 3588 -->, pp. <!-- /react-text --><!-- react-text: 3589 -->189<!-- /react-text --><!-- react-text: 3590 -->-<!-- /react-text --><!-- react-text: 3591 -->202<!-- /react-text --></div></dd><dt class="label"><a href="#bb0225" id="ref-id-b0225">[45]</a></dt><dd class="reference" id="h0065"><span>P.
 Hamel, D. Eck, Learning features from music audio with deep belief 
networks, in: 11th International Society for Music Information Retrieval
 Conference (ISMIR), 2010.</span></dd><dt class="label"><a href="#bb0230" id="ref-id-b0230">[46]</a></dt><dd class="reference" id="h0070"><span>M.
 Henaff, K. Jarrett, K. Kavukcuoglu, Y. LeCun, Unsupervised learning of 
sparse features for scalable audio classification, in: Proceedings of 
International Symposium on Music, Information Retrieval (ISMIR’11), 
2011.</span></dd><dt class="label"><a href="#bb0235" id="ref-id-b0235">[47]</a></dt><dd class="reference" id="h0425"><div class="contribution"><!-- react-text: 3604 -->E.<!-- /react-text --><!-- react-text: 3605 --> <!-- /react-text --><!-- react-text: 3606 -->Hines<!-- /react-text --><!-- react-text: 3607 -->, <!-- /react-text --><!-- react-text: 3608 -->E.<!-- /react-text --><!-- react-text: 3609 --> <!-- /react-text --><!-- react-text: 3610 -->Llobet<!-- /react-text --><!-- react-text: 3611 -->, <!-- /react-text --><!-- react-text: 3612 -->J.<!-- /react-text --><!-- react-text: 3613 --> <!-- /react-text --><!-- react-text: 3614 -->Gardner<!-- /react-text --><strong class="title"><!-- react-text: 3616 -->Electronic noses: a review of signal processing techniques<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3618 -->IEE Proc. Circuits Devices Syst.<!-- /react-text --><!-- react-text: 3619 -->, <!-- /react-text --><!-- react-text: 3620 -->146<!-- /react-text --><!-- react-text: 3621 --> (<!-- /react-text --><!-- react-text: 3622 -->1999<!-- /react-text --><!-- react-text: 3623 -->)<!-- /react-text --><!-- react-text: 3624 -->, pp. <!-- /react-text --><!-- react-text: 3625 -->297<!-- /react-text --><!-- react-text: 3626 -->-<!-- /react-text --><!-- react-text: 3627 -->310<!-- /react-text --></div></dd><dt class="label"><a href="#bb0240" id="ref-id-b0240">[48]</a></dt><dd class="reference" id="h0430"><div class="contribution"><!-- react-text: 3632 -->G.<!-- /react-text --><!-- react-text: 3633 --> <!-- /react-text --><!-- react-text: 3634 -->Hinton<!-- /react-text --><!-- react-text: 3635 -->, <!-- /react-text --><!-- react-text: 3636 -->L.<!-- /react-text --><!-- react-text: 3637 --> <!-- /react-text --><!-- react-text: 3638 -->Deng<!-- /react-text --><!-- react-text: 3639 -->, <!-- /react-text --><!-- react-text: 3640 -->D.<!-- /react-text --><!-- react-text: 3641 --> <!-- /react-text --><!-- react-text: 3642 -->Yu<!-- /react-text --><!-- react-text: 3643 -->, <!-- /react-text --><!-- react-text: 3644 -->G.<!-- /react-text --><!-- react-text: 3645 --> <!-- /react-text --><!-- react-text: 3646 -->Dahl<!-- /react-text --><!-- react-text: 3647 -->, <!-- /react-text --><!-- react-text: 3648 -->A.<!-- /react-text --><!-- react-text: 3649 --> <!-- /react-text --><!-- react-text: 3650 -->Mohamed<!-- /react-text --><!-- react-text: 3651 -->, <!-- /react-text --><!-- react-text: 3652 -->N.<!-- /react-text --><!-- react-text: 3653 --> <!-- /react-text --><!-- react-text: 3654 -->Jaitly<!-- /react-text --><!-- react-text: 3655 -->, <!-- /react-text --><!-- react-text: 3656 -->A.<!-- /react-text --><!-- react-text: 3657 --> <!-- /react-text --><!-- react-text: 3658 -->Senior<!-- /react-text --><!-- react-text: 3659 -->, <!-- /react-text --><!-- react-text: 3660 -->V.<!-- /react-text --><!-- react-text: 3661 --> <!-- /react-text --><!-- react-text: 3662 -->Vanhoucke<!-- /react-text --><!-- react-text: 3663 -->, <!-- /react-text --><!-- react-text: 3664 -->P.<!-- /react-text --><!-- react-text: 3665 --> <!-- /react-text --><!-- react-text: 3666 -->Nguyen<!-- /react-text --><!-- react-text: 3667 -->, <!-- /react-text --><!-- react-text: 3668 -->T.<!-- /react-text --><!-- react-text: 3669 --> <!-- /react-text --><!-- react-text: 3670 -->Sainath<!-- /react-text --><!-- react-text: 3671 -->, <!-- /react-text --><!-- react-text: 3672 -->B.<!-- /react-text --><!-- react-text: 3673 --> <!-- /react-text --><!-- react-text: 3674 -->Kingsbury<!-- /react-text --><strong class="title"><!-- react-text: 3676 -->Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3678 -->IEEE Signal Proc. Mag.<!-- /react-text --><!-- react-text: 3679 -->, <!-- /react-text --><!-- react-text: 3680 -->29<!-- /react-text --><!-- react-text: 3681 --> (<!-- /react-text --><!-- react-text: 3682 -->6<!-- /react-text --><!-- react-text: 3683 -->)<!-- /react-text --><!-- react-text: 3684 --> (<!-- /react-text --><!-- react-text: 3685 -->2012<!-- /react-text --><!-- react-text: 3686 -->)<!-- /react-text --><!-- react-text: 3687 -->, pp. <!-- /react-text --><!-- react-text: 3688 -->82<!-- /react-text --><!-- react-text: 3689 -->-<!-- /react-text --><!-- react-text: 3690 -->97<!-- /react-text --></div></dd><dt class="label"><a href="#bb0245" id="ref-id-b0245">[49]</a></dt><dd class="reference" id="h0435"><div class="contribution"><!-- react-text: 3695 -->G.<!-- /react-text --><!-- react-text: 3696 --> <!-- /react-text --><!-- react-text: 3697 -->Hinton<!-- /react-text --><!-- react-text: 3698 -->, <!-- /react-text --><!-- react-text: 3699 -->R.<!-- /react-text --><!-- react-text: 3700 --> <!-- /react-text --><!-- react-text: 3701 -->Salakhutdinov<!-- /react-text --><strong class="title"><!-- react-text: 3703 -->Reducing the dimensionality of data with neural networks<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3705 -->Science<!-- /react-text --><!-- react-text: 3706 -->, <!-- /react-text --><!-- react-text: 3707 -->313<!-- /react-text --><!-- react-text: 3708 --> (<!-- /react-text --><!-- react-text: 3709 -->5786<!-- /react-text --><!-- react-text: 3710 -->)<!-- /react-text --><!-- react-text: 3711 --> (<!-- /react-text --><!-- react-text: 3712 -->2006<!-- /react-text --><!-- react-text: 3713 -->)<!-- /react-text --><!-- react-text: 3714 -->, pp. <!-- /react-text --><!-- react-text: 3715 -->504<!-- /react-text --><!-- react-text: 3716 -->-<!-- /react-text --><!-- react-text: 3717 -->507<!-- /react-text --></div></dd><dt class="label"><a href="#bb0250" id="ref-id-b0250">[50]</a></dt><dd class="reference" id="h0440"><div class="contribution"><!-- react-text: 3722 -->G.E.<!-- /react-text --><!-- react-text: 3723 --> <!-- /react-text --><!-- react-text: 3724 -->Hinton<!-- /react-text --><strong class="title"><!-- react-text: 3726 -->Training products of experts by minimizing contrastive divergence<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3728 -->Neural Comput.<!-- /react-text --><!-- react-text: 3729 -->, <!-- /react-text --><!-- react-text: 3730 -->14<!-- /react-text --><!-- react-text: 3731 --> (<!-- /react-text --><!-- react-text: 3732 -->2002<!-- /react-text --><!-- react-text: 3733 -->)<!-- /react-text --><!-- react-text: 3734 -->, pp. <!-- /react-text --><!-- react-text: 3735 -->1771<!-- /react-text --><!-- react-text: 3736 -->-<!-- /react-text --><!-- react-text: 3737 -->1800<!-- /react-text --></div></dd><dt class="label"><a href="#bb0255" id="ref-id-b0255">[51]</a></dt><dd class="reference" id="h0690"><div class="contribution"><!-- react-text: 3742 -->G.E.<!-- /react-text --><!-- react-text: 3743 --> <!-- /react-text --><!-- react-text: 3744 -->Hinton<!-- /react-text --><strong class="title"><!-- react-text: 3746 -->A practical guide to training restricted boltzmann machines<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3748 -->G.<!-- /react-text --><!-- react-text: 3749 --> <!-- /react-text --><!-- react-text: 3750 -->Montavon<!-- /react-text --><!-- react-text: 3751 -->, <!-- /react-text --><!-- react-text: 3752 -->G.B.<!-- /react-text --><!-- react-text: 3753 --> <!-- /react-text --><!-- react-text: 3754 -->Orr<!-- /react-text --><!-- react-text: 3755 -->, <!-- /react-text --><!-- react-text: 3756 -->K.-R.<!-- /react-text --><!-- react-text: 3757 --> <!-- /react-text --><!-- react-text: 3758 -->Müller<!-- /react-text --><!-- react-text: 3759 --> (Eds.)<!-- /react-text --><!-- react-text: 3760 -->, <!-- /react-text --><!-- react-text: 3761 -->Neural Networks: Tricks of the Trade<!-- /react-text --><!-- react-text: 3762 -->, <!-- /react-text --><!-- react-text: 3763 -->Lecture Notes in Computer Science<!-- /react-text --><!-- react-text: 3764 -->, <!-- /react-text --><!-- react-text: 3765 -->vol. 7700<!-- /react-text --><!-- react-text: 3766 -->, <!-- /react-text --><!-- react-text: 3767 -->Springer<!-- /react-text --><!-- react-text: 3768 -->, <!-- /react-text --><!-- react-text: 3769 -->Berlin, Heidelberg<!-- /react-text --><!-- react-text: 3770 --> (<!-- /react-text --><!-- react-text: 3771 -->2010<!-- /react-text --><!-- react-text: 3772 -->)<!-- /react-text --><!-- react-text: 3773 -->, pp. <!-- /react-text --><!-- react-text: 3774 -->599<!-- /react-text --><!-- react-text: 3775 -->-<!-- /react-text --><!-- react-text: 3776 -->619<!-- /react-text --></div></dd><dt class="label"><a href="#bb0260" id="ref-id-b0260">[52]</a></dt><dd class="reference" id="h0080"><span>G.E.
 Hinton, A. Krizhevsky, S.D. Wang, Transforming auto-encoders, in: 
Proceedings of the 21th International Conference on Artificial Neural 
Networks, vol. Part I, 2011, pp. 44–51.</span></dd><dt class="label"><a href="#bb0265" id="ref-id-b0265">[53]</a></dt><dd class="reference" id="h0445"><div class="contribution"><!-- react-text: 3785 -->G.E.<!-- /react-text --><!-- react-text: 3786 --> <!-- /react-text --><!-- react-text: 3787 -->Hinton<!-- /react-text --><!-- react-text: 3788 -->, <!-- /react-text --><!-- react-text: 3789 -->S.<!-- /react-text --><!-- react-text: 3790 --> <!-- /react-text --><!-- react-text: 3791 -->Osindero<!-- /react-text --><!-- react-text: 3792 -->, <!-- /react-text --><!-- react-text: 3793 -->Y.W.<!-- /react-text --><!-- react-text: 3794 --> <!-- /react-text --><!-- react-text: 3795 -->Teh<!-- /react-text --><strong class="title"><!-- react-text: 3797 -->A fast learning algorithm for deep belief nets<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3799 -->Neural Comput.<!-- /react-text --><!-- react-text: 3800 -->, <!-- /react-text --><!-- react-text: 3801 -->18<!-- /react-text --><!-- react-text: 3802 --> (<!-- /react-text --><!-- react-text: 3803 -->2006<!-- /react-text --><!-- react-text: 3804 -->)<!-- /react-text --><!-- react-text: 3805 -->, pp. <!-- /react-text --><!-- react-text: 3806 -->1527<!-- /react-text --><!-- react-text: 3807 -->-<!-- /react-text --><!-- react-text: 3808 -->1554<!-- /react-text --></div></dd><dt class="label"><a href="#bb0270" id="ref-id-b0270">[54]</a></dt><dd class="reference" id="h0450"><div class="contribution"><!-- react-text: 3813 -->S.<!-- /react-text --><!-- react-text: 3814 --> <!-- /react-text --><!-- react-text: 3815 -->Hochreiter<!-- /react-text --><!-- react-text: 3816 -->, <!-- /react-text --><!-- react-text: 3817 -->J.<!-- /react-text --><!-- react-text: 3818 --> <!-- /react-text --><!-- react-text: 3819 -->Schmidhuber<!-- /react-text --><strong class="title"><!-- react-text: 3821 -->Long short-term memory<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3823 -->Neural Comput.<!-- /react-text --><!-- react-text: 3824 -->, <!-- /react-text --><!-- react-text: 3825 -->9<!-- /react-text --><!-- react-text: 3826 --> (<!-- /react-text --><!-- react-text: 3827 -->1997<!-- /react-text --><!-- react-text: 3828 -->)<!-- /react-text --><!-- react-text: 3829 -->, pp. <!-- /react-text --><!-- react-text: 3830 -->1735<!-- /react-text --><!-- react-text: 3831 -->-<!-- /react-text --><!-- react-text: 3832 -->1780<!-- /react-text --></div></dd><dt class="label"><a href="#bb0275" id="ref-id-b0275">[55]</a></dt><dd class="reference" id="h0455"><div class="contribution"><!-- react-text: 3837 -->T.J.<!-- /react-text --><!-- react-text: 3838 --> <!-- /react-text --><!-- react-text: 3839 -->Hsieh<!-- /react-text --><!-- react-text: 3840 -->, <!-- /react-text --><!-- react-text: 3841 -->H.F.<!-- /react-text --><!-- react-text: 3842 --> <!-- /react-text --><!-- react-text: 3843 -->Hsiao<!-- /react-text --><!-- react-text: 3844 -->, <!-- /react-text --><!-- react-text: 3845 -->W.C.<!-- /react-text --><!-- react-text: 3846 --> <!-- /react-text --><!-- react-text: 3847 -->Yeh<!-- /react-text --><strong class="title"><!-- react-text: 3849 -->Forecasting
 stock markets using wavelet transforms and recurrent neural networks: 
an integrated system based on artificial bee colony algorithm<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3851 -->Appl. Soft Comput.<!-- /react-text --><!-- react-text: 3852 -->, <!-- /react-text --><!-- react-text: 3853 -->11<!-- /react-text --><!-- react-text: 3854 --> (<!-- /react-text --><!-- react-text: 3855 -->2011<!-- /react-text --><!-- react-text: 3856 -->)<!-- /react-text --><!-- react-text: 3857 -->, pp. <!-- /react-text --><!-- react-text: 3858 -->2510<!-- /react-text --><!-- react-text: 3859 -->-<!-- /react-text --><!-- react-text: 3860 -->2525<!-- /react-text --></div></dd><dt class="label"><a href="#bb0280" id="ref-id-b0280">[56]</a></dt><dd class="reference" id="h0695"><div class="contribution"><!-- react-text: 3865 -->E.J.<!-- /react-text --><!-- react-text: 3866 --> <!-- /react-text --><!-- react-text: 3867 -->Humphrey<!-- /react-text --><!-- react-text: 3868 -->, <!-- /react-text --><!-- react-text: 3869 -->J.P.<!-- /react-text --><!-- react-text: 3870 --> <!-- /react-text --><!-- react-text: 3871 -->Bello<!-- /react-text --><!-- react-text: 3872 -->, <!-- /react-text --><!-- react-text: 3873 -->Y.<!-- /react-text --><!-- react-text: 3874 --> <!-- /react-text --><!-- react-text: 3875 -->LeCun<!-- /react-text --><strong class="title"><!-- react-text: 3877 -->Feature learning and deep architectures: new directions for music informatics<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3879 -->J. Intell. Inf. Syst.<!-- /react-text --><!-- react-text: 3880 -->, <!-- /react-text --><!-- react-text: 3881 -->41<!-- /react-text --><!-- react-text: 3882 --> (<!-- /react-text --><!-- react-text: 3883 -->3<!-- /react-text --><!-- react-text: 3884 -->)<!-- /react-text --><!-- react-text: 3885 --> (<!-- /react-text --><!-- react-text: 3886 -->2013<!-- /react-text --><!-- react-text: 3887 -->)<!-- /react-text --><!-- react-text: 3888 -->, pp. <!-- /react-text --><!-- react-text: 3889 -->461<!-- /react-text --><!-- react-text: 3890 -->-<!-- /react-text --><!-- react-text: 3891 -->481<!-- /react-text --></div></dd><dt class="label"><a href="#bb0285" id="ref-id-b0285">[57]</a></dt><dd class="reference" id="h0460"><div class="contribution"><!-- react-text: 3896 -->M.<!-- /react-text --><!-- react-text: 3897 --> <!-- /react-text --><!-- react-text: 3898 -->Hüsken<!-- /react-text --><!-- react-text: 3899 -->, <!-- /react-text --><!-- react-text: 3900 -->P.<!-- /react-text --><!-- react-text: 3901 --> <!-- /react-text --><!-- react-text: 3902 -->Stagge<!-- /react-text --><strong class="title"><!-- react-text: 3904 -->Recurrent neural networks for time series classification<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3906 -->Neurocomputing<!-- /react-text --><!-- react-text: 3907 -->, <!-- /react-text --><!-- react-text: 3908 -->50<!-- /react-text --><!-- react-text: 3909 --> (<!-- /react-text --><!-- react-text: 3910 -->2003<!-- /react-text --><!-- react-text: 3911 -->)<!-- /react-text --><!-- react-text: 3912 -->, pp. <!-- /react-text --><!-- react-text: 3913 -->223<!-- /react-text --><!-- react-text: 3914 -->-<!-- /react-text --><!-- react-text: 3915 -->235<!-- /react-text --></div></dd><dt class="label"><a href="#bb0290" id="ref-id-b0290">[58]</a></dt><dd class="reference" id="h0465"><div class="contribution"><!-- react-text: 3920 -->A.<!-- /react-text --><!-- react-text: 3921 --> <!-- /react-text --><!-- react-text: 3922 -->Hyvärinen<!-- /react-text --><!-- react-text: 3923 -->, <!-- /react-text --><!-- react-text: 3924 -->J.<!-- /react-text --><!-- react-text: 3925 --> <!-- /react-text --><!-- react-text: 3926 -->Hurri<!-- /react-text --><!-- react-text: 3927 -->, <!-- /react-text --><!-- react-text: 3928 -->J.<!-- /react-text --><!-- react-text: 3929 --> <!-- /react-text --><!-- react-text: 3930 -->Väyrynen<!-- /react-text --><strong class="title"><!-- react-text: 3932 -->Bubbles: a unifying framework for low-level statistical properties of natural image sequences<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3934 -->J. Opt. Soc. Am. A<!-- /react-text --><!-- react-text: 3935 -->, <!-- /react-text --><!-- react-text: 3936 -->20<!-- /react-text --><!-- react-text: 3937 --> (<!-- /react-text --><!-- react-text: 3938 -->2003<!-- /react-text --><!-- react-text: 3939 -->)<!-- /react-text --><!-- react-text: 3940 -->, pp. <!-- /react-text --><!-- react-text: 3941 -->1237<!-- /react-text --><!-- react-text: 3942 -->-<!-- /react-text --><!-- react-text: 3943 -->1252<!-- /react-text --></div></dd><dt class="label"><a href="#bb0295" id="ref-id-b0295">[59]</a></dt><dd class="reference" id="h0470"><div class="contribution"><!-- react-text: 3948 -->A.<!-- /react-text --><!-- react-text: 3949 --> <!-- /react-text --><!-- react-text: 3950 -->Hyvärinen<!-- /react-text --><!-- react-text: 3951 -->, <!-- /react-text --><!-- react-text: 3952 -->P.<!-- /react-text --><!-- react-text: 3953 --> <!-- /react-text --><!-- react-text: 3954 -->Ramkumar<!-- /react-text --><!-- react-text: 3955 -->, <!-- /react-text --><!-- react-text: 3956 -->L.<!-- /react-text --><!-- react-text: 3957 --> <!-- /react-text --><!-- react-text: 3958 -->Parkkonen<!-- /react-text --><!-- react-text: 3959 -->, <!-- /react-text --><!-- react-text: 3960 -->R.<!-- /react-text --><!-- react-text: 3961 --> <!-- /react-text --><!-- react-text: 3962 -->Hari<!-- /react-text --><strong class="title"><!-- react-text: 3964 -->Independent component analysis of short-time Fourier transforms for spontaneous EEG/MEG analysis<!-- /react-text --></strong></div><div class="host"><!-- react-text: 3966 -->NeuroImage<!-- /react-text --><!-- react-text: 3967 -->, <!-- /react-text --><!-- react-text: 3968 -->49<!-- /react-text --><!-- react-text: 3969 --> (<!-- /react-text --><!-- react-text: 3970 -->1<!-- /react-text --><!-- react-text: 3971 -->)<!-- /react-text --><!-- react-text: 3972 --> (<!-- /react-text --><!-- react-text: 3973 -->2010<!-- /react-text --><!-- react-text: 3974 -->)<!-- /react-text --><!-- react-text: 3975 -->, pp. <!-- /react-text --><!-- react-text: 3976 -->257<!-- /react-text --><!-- react-text: 3977 -->-<!-- /react-text --><!-- react-text: 3978 -->271<!-- /react-text --></div></dd><dt class="label"><a href="#bb0300" id="ref-id-b0300">[60]</a></dt><dd class="reference" id="h0475"><div class="contribution"><!-- react-text: 3983 -->A.<!-- /react-text --><!-- react-text: 3984 --> <!-- /react-text --><!-- react-text: 3985 -->Hyvèarinen<!-- /react-text --><!-- react-text: 3986 -->, <!-- /react-text --><!-- react-text: 3987 -->J.<!-- /react-text --><!-- react-text: 3988 --> <!-- /react-text --><!-- react-text: 3989 -->Hurri<!-- /react-text --><!-- react-text: 3990 -->, <!-- /react-text --><!-- react-text: 3991 -->P.O.<!-- /react-text --><!-- react-text: 3992 --> <!-- /react-text --><!-- react-text: 3993 -->Hoyer<!-- /react-text --></div><div class="host"><!-- react-text: 3995 -->Natural Image Statistics<!-- /react-text --><!-- react-text: 3996 -->, <!-- /react-text --><!-- react-text: 3997 -->vol. 39<!-- /react-text --><!-- react-text: 3998 -->, <!-- /react-text --><!-- react-text: 3999 -->Springer<!-- /react-text --><!-- react-text: 4000 --> (<!-- /react-text --><!-- react-text: 4001 -->2009<!-- /react-text --><!-- react-text: 4002 -->)<!-- /react-text --></div></dd><dt class="label"><a href="#bb0305" id="ref-id-b0305">[61]</a></dt><dd class="reference" id="h0480"><div class="contribution"><!-- react-text: 4007 -->N.<!-- /react-text --><!-- react-text: 4008 --> <!-- /react-text --><!-- react-text: 4009 -->Jaitly<!-- /react-text --><!-- react-text: 4010 -->, <!-- /react-text --><!-- react-text: 4011 -->G.<!-- /react-text --><!-- react-text: 4012 --> <!-- /react-text --><!-- react-text: 4013 -->Hinton<!-- /react-text --><strong class="title"><!-- react-text: 4015 -->Learning a better representation of speech soundwaves using restricted Boltzmann machines<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4017 -->2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)<!-- /react-text --><!-- react-text: 4018 -->, <!-- /react-text --><!-- react-text: 4019 -->IEEE<!-- /react-text --><!-- react-text: 4020 --> (<!-- /react-text --><!-- react-text: 4021 -->2011<!-- /react-text --><!-- react-text: 4022 -->)<!-- /react-text --><!-- react-text: 4023 -->, pp. <!-- /react-text --><!-- react-text: 4024 -->5884<!-- /react-text --><!-- react-text: 4025 -->-<!-- /react-text --><!-- react-text: 4026 -->5887<!-- /react-text --></div></dd><dt class="label"><a href="#bb0310" id="ref-id-b0310">[62]</a></dt><dd class="reference" id="h0485"><div class="contribution"><!-- react-text: 4031 -->S.J.<!-- /react-text --><!-- react-text: 4032 --> <!-- /react-text --><!-- react-text: 4033 -->Pan<!-- /react-text --><!-- react-text: 4034 -->, <!-- /react-text --><!-- react-text: 4035 -->Q.<!-- /react-text --><!-- react-text: 4036 --> <!-- /react-text --><!-- react-text: 4037 -->Yang<!-- /react-text --><strong class="title"><!-- react-text: 4039 -->A survey on transfer learning<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4041 -->IEEE Trans. Knowl. Data Eng.<!-- /react-text --><!-- react-text: 4042 -->, <!-- /react-text --><!-- react-text: 4043 -->22<!-- /react-text --><!-- react-text: 4044 --> (<!-- /react-text --><!-- react-text: 4045 -->2010<!-- /react-text --><!-- react-text: 4046 -->)<!-- /react-text --><!-- react-text: 4047 -->, p. <!-- /react-text --><!-- react-text: 4048 -->56<!-- /react-text --></div></dd><dt class="label"><a href="#bb0315" id="ref-id-b0315">[63]</a></dt><dd class="reference" id="h0090"><span>H.
 Kamyshanska, R. Memisevic, On autoencoder scoring, in: Proceedings of 
the 30th International Conference on Machine Learning (ICML-13), JMLR 
Workshop and Conference Proceedings, 2013, pp. 720–728.</span></dd><dt class="label"><a href="#bb0320" id="ref-id-b0320">[64]</a></dt><dd class="reference" id="h0095"><span>T.
 van Kasteren, A. Noulas, B. Kröse, Conditional random fields versus 
hidden markov models for activity recognition in temporal sensor data, 
in: Proceedings of the 14th Annual Conference of the Advanced School for
 Computing and Imaging (ASCI’08), The Netherlands, 2008.</span></dd><dt class="label"><a href="#bb0325" id="ref-id-b0325">[65]</a></dt><dd class="reference" id="h0490"><div class="contribution"><!-- react-text: 4061 -->K.<!-- /react-text --><!-- react-text: 4062 --> <!-- /react-text --><!-- react-text: 4063 -->Kavukcuoglu<!-- /react-text --><!-- react-text: 4064 -->, <!-- /react-text --><!-- react-text: 4065 -->M.<!-- /react-text --><!-- react-text: 4066 --> <!-- /react-text --><!-- react-text: 4067 -->Ranzato<!-- /react-text --><!-- react-text: 4068 -->, <!-- /react-text --><!-- react-text: 4069 -->R.<!-- /react-text --><!-- react-text: 4070 --> <!-- /react-text --><!-- react-text: 4071 -->Fergus<!-- /react-text --><!-- react-text: 4072 -->, <!-- /react-text --><!-- react-text: 4073 -->Y.<!-- /react-text --><!-- react-text: 4074 --> <!-- /react-text --><!-- react-text: 4075 -->Le-Cun<!-- /react-text --><strong class="title"><!-- react-text: 4077 -->Learning invariant features through topographic filter maps<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4079 -->IEEE Conference on Computer Vision and Pattern Recognition, 2009. CVPR 2009<!-- /react-text --><!-- react-text: 4080 -->, <!-- /react-text --><!-- react-text: 4081 -->IEEE<!-- /react-text --><!-- react-text: 4082 --> (<!-- /react-text --><!-- react-text: 4083 -->2009<!-- /react-text --><!-- react-text: 4084 -->)<!-- /react-text --><!-- react-text: 4085 -->, pp. <!-- /react-text --><!-- react-text: 4086 -->1605<!-- /react-text --><!-- react-text: 4087 -->-<!-- /react-text --><!-- react-text: 4088 -->1612<!-- /react-text --></div></dd><dt class="label"><a href="#bb0330" id="ref-id-b0330">[66]</a></dt><dd class="reference" id="h0100"><span>E.
 Keogh, S. Kasetty, On the need for time series data mining benchmarks: a
 survey and empirical demonstration, in: Proceedings of the Eighth ACM 
SIGKDD International Conference on Knowledge Discovery and Data Mining, 
2002, pp. 102–111.</span></dd><dt class="label"><a href="#bb0335" id="ref-id-b0335">[67]</a></dt><dd class="reference" id="h0495"><div class="contribution"><!-- react-text: 4097 -->S.S.<!-- /react-text --><!-- react-text: 4098 --> <!-- /react-text --><!-- react-text: 4099 -->Kim<!-- /react-text --><strong class="title"><!-- react-text: 4101 -->Time-delay recurrent neural network for temporal correlations and prediction<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4103 -->Neurocomputing<!-- /react-text --><!-- react-text: 4104 -->, <!-- /react-text --><!-- react-text: 4105 -->20<!-- /react-text --><!-- react-text: 4106 --> (<!-- /react-text --><!-- react-text: 4107 -->1998<!-- /react-text --><!-- react-text: 4108 -->)<!-- /react-text --><!-- react-text: 4109 -->, pp. <!-- /react-text --><!-- react-text: 4110 -->253<!-- /react-text --><!-- react-text: 4111 -->-<!-- /react-text --><!-- react-text: 4112 -->263<!-- /react-text --></div></dd><dt class="label"><a href="#bb0340" id="ref-id-b0340">[68]</a></dt><dd class="reference" id="h0500"><div class="contribution"><!-- react-text: 4117 -->J.D.<!-- /react-text --><!-- react-text: 4118 --> <!-- /react-text --><!-- react-text: 4119 -->Lafferty<!-- /react-text --><!-- react-text: 4120 -->, <!-- /react-text --><!-- react-text: 4121 -->A.<!-- /react-text --><!-- react-text: 4122 --> <!-- /react-text --><!-- react-text: 4123 -->McCallum<!-- /react-text --><!-- react-text: 4124 -->, <!-- /react-text --><!-- react-text: 4125 -->F.C.N.<!-- /react-text --><!-- react-text: 4126 --> <!-- /react-text --><!-- react-text: 4127 -->Pereira<!-- /react-text --><strong class="title"><!-- react-text: 4129 -->Conditional random fields: probabilistic models for segmenting and labeling sequence data<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4131 -->Proceedings of the 18th International Conference on Machine Learning<!-- /react-text --><!-- react-text: 4132 -->, <!-- /react-text --><!-- react-text: 4133 -->Morgan Kaufmann Publishers Inc.<!-- /react-text --><!-- react-text: 4134 -->, <!-- /react-text --><!-- react-text: 4135 -->San Francisco, CA, USA<!-- /react-text --><!-- react-text: 4136 --> (<!-- /react-text --><!-- react-text: 4137 -->2001<!-- /react-text --><!-- react-text: 4138 -->)<!-- /react-text --><!-- react-text: 4139 -->, pp. <!-- /react-text --><!-- react-text: 4140 -->282<!-- /react-text --><!-- react-text: 4141 -->-<!-- /react-text --><!-- react-text: 4142 -->289<!-- /react-text --></div></dd><dt class="label"><a href="#bb0345" id="ref-id-b0345">[69]</a></dt><dd class="reference" id="h0505"><div class="contribution"><!-- react-text: 4147 -->M.<!-- /react-text --><!-- react-text: 4148 --> <!-- /react-text --><!-- react-text: 4149 -->Längkvist<!-- /react-text --><!-- react-text: 4150 -->, <!-- /react-text --><!-- react-text: 4151 -->S.<!-- /react-text --><!-- react-text: 4152 --> <!-- /react-text --><!-- react-text: 4153 -->Coradeschi<!-- /react-text --><!-- react-text: 4154 -->, <!-- /react-text --><!-- react-text: 4155 -->A.<!-- /react-text --><!-- react-text: 4156 --> <!-- /react-text --><!-- react-text: 4157 -->Loutfi<!-- /react-text --><!-- react-text: 4158 -->, <!-- /react-text --><!-- react-text: 4159 -->J.B.B.<!-- /react-text --><!-- react-text: 4160 --> <!-- /react-text --><!-- react-text: 4161 -->Rayappan<!-- /react-text --><strong class="title"><!-- react-text: 4163 -->Fast classification of meat spoilage markers using nanostructured ZnO thin films and unsupervised feature learning<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4165 -->Sensors<!-- /react-text --><!-- react-text: 4166 -->, <!-- /react-text --><!-- react-text: 4167 -->13<!-- /react-text --><!-- react-text: 4168 --> (<!-- /react-text --><!-- react-text: 4169 -->2<!-- /react-text --><!-- react-text: 4170 -->)<!-- /react-text --><!-- react-text: 4171 --> (<!-- /react-text --><!-- react-text: 4172 -->2013<!-- /react-text --><!-- react-text: 4173 -->)<!-- /react-text --><!-- react-text: 4174 -->, pp. <!-- /react-text --><!-- react-text: 4175 -->1578<!-- /react-text --><!-- react-text: 4176 -->-<!-- /react-text --><!-- react-text: 4177 -->1592<!-- /react-text --><!-- react-text: 4178 -->, <!-- /react-text --><a href="https://doi.org/10.3390/s130201578" target="_blank">10.3390/s130201578</a></div></dd><dt class="label"><a href="#bb0350" id="ref-id-b0350">[70]</a></dt><dd class="reference" id="h0510"><div class="contribution"><!-- react-text: 4184 -->M.<!-- /react-text --><!-- react-text: 4185 --> <!-- /react-text --><!-- react-text: 4186 -->Längkvist<!-- /react-text --><!-- react-text: 4187 -->, <!-- /react-text --><!-- react-text: 4188 -->L.<!-- /react-text --><!-- react-text: 4189 --> <!-- /react-text --><!-- react-text: 4190 -->Karlsson<!-- /react-text --><!-- react-text: 4191 -->, <!-- /react-text --><!-- react-text: 4192 -->A.<!-- /react-text --><!-- react-text: 4193 --> <!-- /react-text --><!-- react-text: 4194 -->Loutfi<!-- /react-text --><strong class="title"><!-- react-text: 4196 -->Sleep stage classification using unsupervised feature learning<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4198 -->Adv. Artif. Neural Syst.<!-- /react-text --><!-- react-text: 4199 -->, <!-- /react-text --><!-- react-text: 4200 -->2012<!-- /react-text --><!-- react-text: 4201 --> (<!-- /react-text --><!-- react-text: 4202 -->2012<!-- /react-text --><!-- react-text: 4203 -->)<!-- /react-text --><!-- react-text: 4204 -->, <!-- /react-text --><a href="https://doi.org/10.1155/2012/107046" target="_blank">10.1155/2012/107046</a></div></dd><dt class="label"><a href="#bb0355" id="ref-id-b0355">[71]</a></dt><dd class="reference" id="h0105"><span>M.
 Längkvist, A. Loutfi, Unsupervised feature learning for electronic nose
 data applied to bacteria identification in blood, in: NIPS Workshop on 
Deep Learning and Unsupervised Feature Learning, 2011.</span></dd><dt class="label"><a href="#bb0360" id="ref-id-b0360">[72]</a></dt><dd class="reference" id="h0110"><span>M.
 Längkvist, A. Loutfi, Not all signals are created equal: dynamic 
objective auto-encoder for multivariate data, in: NIPS Workshop on Deep 
Learning and Unsupervised Feature Learning, 2012.</span></dd><dt class="label"><a href="#bb0365" id="ref-id-b0365">[73]</a></dt><dd class="reference" id="h0115"><span>Q.V.
 Le, W.Y. Zou, S.Y. Yeung, A.Y. Ng, Learning hierarchical invariant 
spatio-temporal features for action recognition with independent 
subspace analysis, in: Computer Vision and Pattern Recognition (CVPR), 
2011.</span></dd><dt class="label"><a href="#bb0370" id="ref-id-b0370">[74]</a></dt><dd class="reference" id="h0515"><div class="contribution"><!-- react-text: 4222 -->N.<!-- /react-text --><!-- react-text: 4223 --> <!-- /react-text --><!-- react-text: 4224 -->Le Roux<!-- /react-text --><!-- react-text: 4225 -->, <!-- /react-text --><!-- react-text: 4226 -->Y.<!-- /react-text --><!-- react-text: 4227 --> <!-- /react-text --><!-- react-text: 4228 -->Bengio<!-- /react-text --><strong class="title"><!-- react-text: 4230 -->Representational power of restricted Boltzmann machines and deep belief networks<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4232 -->Neural Comput.<!-- /react-text --><!-- react-text: 4233 -->, <!-- /react-text --><!-- react-text: 4234 -->20<!-- /react-text --><!-- react-text: 4235 --> (<!-- /react-text --><!-- react-text: 4236 -->2008<!-- /react-text --><!-- react-text: 4237 -->)<!-- /react-text --><!-- react-text: 4238 -->, pp. <!-- /react-text --><!-- react-text: 4239 -->1631<!-- /react-text --><!-- react-text: 4240 -->-<!-- /react-text --><!-- react-text: 4241 -->1649<!-- /react-text --></div></dd><dt class="label"><a href="#bb0375" id="ref-id-b0375">[75]</a></dt><dd class="reference" id="h0520"><div class="contribution"><!-- react-text: 4246 -->Y.<!-- /react-text --><!-- react-text: 4247 --> <!-- /react-text --><!-- react-text: 4248 -->LeCun<!-- /react-text --><!-- react-text: 4249 -->, <!-- /react-text --><!-- react-text: 4250 -->K.<!-- /react-text --><!-- react-text: 4251 --> <!-- /react-text --><!-- react-text: 4252 -->Kavukvuoglu<!-- /react-text --><!-- react-text: 4253 -->, <!-- /react-text --><!-- react-text: 4254 -->C.<!-- /react-text --><!-- react-text: 4255 --> <!-- /react-text --><!-- react-text: 4256 -->Farabet<!-- /react-text --><strong class="title"><!-- react-text: 4258 -->Convolutional networks and applications in vision<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4260 -->Proceedings International Symposium on Circuits and Systems (ISCASąŕ10)<!-- /react-text --><!-- react-text: 4261 -->, <!-- /react-text --><!-- react-text: 4262 -->IEEE<!-- /react-text --><!-- react-text: 4263 --> (<!-- /react-text --><!-- react-text: 4264 -->2010<!-- /react-text --><!-- react-text: 4265 -->)<!-- /react-text --></div></dd><dt class="label"><a href="#bb0380" id="ref-id-b0380">[76]</a></dt><dd class="reference" id="h0525"><div class="contribution"><!-- react-text: 4270 -->H.<!-- /react-text --><!-- react-text: 4271 --> <!-- /react-text --><!-- react-text: 4272 -->Lee<!-- /react-text --><!-- react-text: 4273 -->, <!-- /react-text --><!-- react-text: 4274 -->C.<!-- /react-text --><!-- react-text: 4275 --> <!-- /react-text --><!-- react-text: 4276 -->Ekanadham<!-- /react-text --><!-- react-text: 4277 -->, <!-- /react-text --><!-- react-text: 4278 -->A.Y.<!-- /react-text --><!-- react-text: 4279 --> <!-- /react-text --><!-- react-text: 4280 -->Ng<!-- /react-text --><strong class="title"><!-- react-text: 4282 -->Sparse deep belief net model for visual area V2<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4284 -->Adv. Neural Inf. Process. Syst.<!-- /react-text --><!-- react-text: 4285 -->, <!-- /react-text --><!-- react-text: 4286 -->20<!-- /react-text --><!-- react-text: 4287 --> (<!-- /react-text --><!-- react-text: 4288 -->2008<!-- /react-text --><!-- react-text: 4289 -->)<!-- /react-text --><!-- react-text: 4290 -->, pp. <!-- /react-text --><!-- react-text: 4291 -->873<!-- /react-text --><!-- react-text: 4292 -->-<!-- /react-text --><!-- react-text: 4293 -->880<!-- /react-text --></div></dd><dt class="label"><a href="#bb0385" id="ref-id-b0385">[77]</a></dt><dd class="reference" id="h0120"><span>H.
 Lee, R. Grosse, R. Ranganath, A.Y. Ng, Convolutional deep belief 
networks for scalable unsupervised learning of hierarchical 
representations, in: 26th International Conference on Machine Learning, 
2009.</span></dd><dt class="label"><a href="#bb0390" id="ref-id-b0390">[78]</a></dt><dd class="reference" id="h0530"><div class="contribution"><!-- react-text: 4302 -->H.<!-- /react-text --><!-- react-text: 4303 --> <!-- /react-text --><!-- react-text: 4304 -->Lee<!-- /react-text --><!-- react-text: 4305 -->, <!-- /react-text --><!-- react-text: 4306 -->Y.<!-- /react-text --><!-- react-text: 4307 --> <!-- /react-text --><!-- react-text: 4308 -->Largman<!-- /react-text --><!-- react-text: 4309 -->, <!-- /react-text --><!-- react-text: 4310 -->P.<!-- /react-text --><!-- react-text: 4311 --> <!-- /react-text --><!-- react-text: 4312 -->Pham<!-- /react-text --><!-- react-text: 4313 -->, <!-- /react-text --><!-- react-text: 4314 -->A.Y.<!-- /react-text --><!-- react-text: 4315 --> <!-- /react-text --><!-- react-text: 4316 -->Ng<!-- /react-text --><strong class="title"><!-- react-text: 4318 -->Unsupervised feature learning for audio classification using convolutional deep belief networks<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4320 -->Adv. Neural Inf. Process. Syst.<!-- /react-text --><!-- react-text: 4321 -->, <!-- /react-text --><!-- react-text: 4322 -->22<!-- /react-text --><!-- react-text: 4323 --> (<!-- /react-text --><!-- react-text: 4324 -->2009<!-- /react-text --><!-- react-text: 4325 -->)<!-- /react-text --><!-- react-text: 4326 -->, pp. <!-- /react-text --><!-- react-text: 4327 -->1096<!-- /react-text --><!-- react-text: 4328 -->-<!-- /react-text --><!-- react-text: 4329 -->1104<!-- /react-text --></div></dd><dt class="label"><a href="#bb0395" id="ref-id-b0395">[79]</a></dt><dd class="reference" id="h0535"><div class="contribution"><!-- react-text: 4334 -->Y.<!-- /react-text --><!-- react-text: 4335 --> <!-- /react-text --><!-- react-text: 4336 -->Li<!-- /react-text --><!-- react-text: 4337 -->, <!-- /react-text --><!-- react-text: 4338 -->W.<!-- /react-text --><!-- react-text: 4339 --> <!-- /react-text --><!-- react-text: 4340 -->Ma<!-- /react-text --><strong class="title"><!-- react-text: 4342 -->Applications of artificial neural networks in financial economics: a survey<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4344 -->Proceedings of the 2010 International Symposium on Computational Intelligence and Design<!-- /react-text --><!-- react-text: 4345 -->, <!-- /react-text --><!-- react-text: 4346 -->vol. 01<!-- /react-text --><!-- react-text: 4347 -->, <!-- /react-text --><!-- react-text: 4348 -->IEEE Computer Society<!-- /react-text --><!-- react-text: 4349 --> (<!-- /react-text --><!-- react-text: 4350 -->2010<!-- /react-text --><!-- react-text: 4351 -->)<!-- /react-text --><!-- react-text: 4352 -->, pp. <!-- /react-text --><!-- react-text: 4353 -->211<!-- /react-text --><!-- react-text: 4354 -->-<!-- /react-text --><!-- react-text: 4355 -->214<!-- /react-text --></div></dd><dt class="label"><a href="#bb0400" id="ref-id-b0400">[80]</a></dt><dd class="reference" id="h0540"><div class="contribution"><!-- react-text: 4360 -->X.<!-- /react-text --><!-- react-text: 4361 --> <!-- /react-text --><!-- react-text: 4362 -->Lin<!-- /react-text --><!-- react-text: 4363 -->, <!-- /react-text --><!-- react-text: 4364 -->Z.<!-- /react-text --><!-- react-text: 4365 --> <!-- /react-text --><!-- react-text: 4366 -->Yang<!-- /react-text --><!-- react-text: 4367 -->, <!-- /react-text --><!-- react-text: 4368 -->Y.<!-- /react-text --><!-- react-text: 4369 --> <!-- /react-text --><!-- react-text: 4370 -->Song<!-- /react-text --><strong class="title"><!-- react-text: 4372 -->Short-term stock price prediction based on echo state networks<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4374 -->Expert Syst. Appl.<!-- /react-text --><!-- react-text: 4375 -->, <!-- /react-text --><!-- react-text: 4376 -->36<!-- /react-text --><!-- react-text: 4377 --> (<!-- /react-text --><!-- react-text: 4378 -->2009<!-- /react-text --><!-- react-text: 4379 -->)<!-- /react-text --><!-- react-text: 4380 -->, pp. <!-- /react-text --><!-- react-text: 4381 -->7313<!-- /react-text --><!-- react-text: 4382 -->-<!-- /react-text --><!-- react-text: 4383 -->7317<!-- /react-text --></div></dd><dt class="label"><a href="#bb0405" id="ref-id-b0405">[81]</a></dt><dd class="reference" id="h0125"><span>D. Lowe, Object recognition from local scale-invariant features, in: ICCV, 1999.</span></dd><dt class="label"><a href="#bb0410" id="ref-id-b0410">[82]</a></dt><dd class="reference" id="h0545"><div class="contribution"><!-- react-text: 4392 -->D.<!-- /react-text --><!-- react-text: 4393 --> <!-- /react-text --><!-- react-text: 4394 -->Luenberger<!-- /react-text --><strong class="title"><!-- react-text: 4396 -->Introduction to Dynamic Systems: Theory, Models, and Applications<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4398 -->Wiley<!-- /react-text --><!-- react-text: 4399 --> (<!-- /react-text --><!-- react-text: 4400 -->1979<!-- /react-text --><!-- react-text: 4401 -->)<!-- /react-text --></div></dd><dt class="label"><a href="#bb0415" id="ref-id-b0415">[83]</a></dt><dd class="reference" id="h0550"><div class="contribution"><!-- react-text: 4406 -->H.<!-- /react-text --><!-- react-text: 4407 --> <!-- /react-text --><!-- react-text: 4408 -->Lütkepohl<!-- /react-text --><strong class="title"><!-- react-text: 4410 -->New Introduction to Multiple Time Series Analysis<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4412 -->Springer-Verlag<!-- /react-text --><!-- react-text: 4413 --> (<!-- /react-text --><!-- react-text: 4414 -->2005<!-- /react-text --><!-- react-text: 4415 -->)<!-- /react-text --></div></dd><dt class="label"><a href="#bb0420" id="ref-id-b0420">[84]</a></dt><dd class="reference" id="h0555"><div class="contribution"><!-- react-text: 4420 -->B.<!-- /react-text --><!-- react-text: 4421 --> <!-- /react-text --><!-- react-text: 4422 -->Malkiel<!-- /react-text --><strong class="title"><!-- react-text: 4424 -->The efficient market hypothesis and its critics<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4426 -->J. Econ. Perspect.<!-- /react-text --><!-- react-text: 4427 -->, <!-- /react-text --><!-- react-text: 4428 -->17<!-- /react-text --><!-- react-text: 4429 --> (<!-- /react-text --><!-- react-text: 4430 -->2003<!-- /react-text --><!-- react-text: 4431 -->)<!-- /react-text --></div><div class="host"><a href="http://dx.doi.org/10.2307/3216840" target="_blank">http://dx.doi.org/10.2307/3216840</a></div></dd><dt class="label"><a href="#bb0425" id="ref-id-b0425">[85]</a></dt><dd class="reference" id="h0130"><span>K.
 Markov, T. Matsui, Music genre classification using self-taught 
learning via sparse coding, in: 2012 IEEE International Conference on 
Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 1929–1932.</span></dd><dt class="label"><a href="#bb0430" id="ref-id-b0430">[86]</a></dt><dd class="reference" id="h0565"><div class="contribution"><!-- react-text: 4442 -->J.<!-- /react-text --><!-- react-text: 4443 --> <!-- /react-text --><!-- react-text: 4444 -->Martens<!-- /react-text --><!-- react-text: 4445 -->, <!-- /react-text --><!-- react-text: 4446 -->I.<!-- /react-text --><!-- react-text: 4447 --> <!-- /react-text --><!-- react-text: 4448 -->Sutskever<!-- /react-text --><strong class="title"><!-- react-text: 4450 -->Training deep and recurrent neural networks with hessian-free optimization<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4452 -->Neural Networks: Tricks of the Trade<!-- /react-text --><!-- react-text: 4453 -->, <!-- /react-text --><!-- react-text: 4454 -->Lecture Notes in Computer Science<!-- /react-text --><!-- react-text: 4455 -->, <!-- /react-text --><!-- react-text: 4456 -->vol. 7700<!-- /react-text --><!-- react-text: 4457 -->, <!-- /react-text --><!-- react-text: 4458 -->Springer<!-- /react-text --><!-- react-text: 4459 -->, <!-- /react-text --><!-- react-text: 4460 -->Berlin, Heidelberg<!-- /react-text --><!-- react-text: 4461 --> (<!-- /react-text --><!-- react-text: 4462 -->2012<!-- /react-text --><!-- react-text: 4463 -->)<!-- /react-text --></div></dd><dt class="label"><a href="#bb0435" id="ref-id-b0435">[87]</a></dt><dd class="reference" id="h0135"><span>J.
 Masci, U. Meier, D. Cireşan, J. Schmidhuber, Stacked convolutional 
auto-encoders for hierarchical feature extraction, in: Proceedings of 
the 21th International Conference on Artificial Neural Networks, vol. 
Part I, 2011, pp. 52–59.</span></dd><dt class="label"><a href="#bb0440" id="ref-id-b0440">[88]</a></dt><dd class="reference" id="h0570"><div class="contribution"><!-- react-text: 4472 -->R.<!-- /react-text --><!-- react-text: 4473 --> <!-- /react-text --><!-- react-text: 4474 -->Memisevic<!-- /react-text --><!-- react-text: 4475 -->, <!-- /react-text --><!-- react-text: 4476 -->G.<!-- /react-text --><!-- react-text: 4477 --> <!-- /react-text --><!-- react-text: 4478 -->Hinton<!-- /react-text --><strong class="title"><!-- react-text: 4480 -->Unsupervised learning of image transformations<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4482 -->IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<!-- /react-text --><!-- react-text: 4483 --> (<!-- /react-text --><!-- react-text: 4484 -->2007<!-- /react-text --><!-- react-text: 4485 -->)<!-- /react-text --><!-- react-text: 4486 -->, pp. <!-- /react-text --><!-- react-text: 4487 -->1<!-- /react-text --><!-- react-text: 4488 -->-<!-- /react-text --><!-- react-text: 4489 -->8<!-- /react-text --></div></dd><dt class="label"><a href="#bb0445" id="ref-id-b0445">[89]</a></dt><dd class="reference" id="h0575"><div class="contribution"><!-- react-text: 4494 -->R.<!-- /react-text --><!-- react-text: 4495 --> <!-- /react-text --><!-- react-text: 4496 -->Memisevic<!-- /react-text --><!-- react-text: 4497 -->, <!-- /react-text --><!-- react-text: 4498 -->G.E.<!-- /react-text --><!-- react-text: 4499 --> <!-- /react-text --><!-- react-text: 4500 -->Hinton<!-- /react-text --><strong class="title"><!-- react-text: 4502 -->Learning to represent spatial transformations with factored higher-order Boltzmann machines<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4504 -->Neural Comput.<!-- /react-text --><!-- react-text: 4505 -->, <!-- /react-text --><!-- react-text: 4506 -->22<!-- /react-text --><!-- react-text: 4507 --> (<!-- /react-text --><!-- react-text: 4508 -->2010<!-- /react-text --><!-- react-text: 4509 -->)<!-- /react-text --><!-- react-text: 4510 -->, pp. <!-- /react-text --><!-- react-text: 4511 -->1473<!-- /react-text --><!-- react-text: 4512 -->-<!-- /react-text --><!-- react-text: 4513 -->1492<!-- /react-text --></div></dd><dt class="label"><a href="#bb0450" id="ref-id-b0450">[90]</a></dt><dd class="reference" id="h0580"><div class="contribution"><!-- react-text: 4518 -->P.<!-- /react-text --><!-- react-text: 4519 --> <!-- /react-text --><!-- react-text: 4520 -->Mirowski<!-- /react-text --><!-- react-text: 4521 -->, <!-- /react-text --><!-- react-text: 4522 -->Y.<!-- /react-text --><!-- react-text: 4523 --> <!-- /react-text --><!-- react-text: 4524 -->LeCun<!-- /react-text --><strong class="title"><!-- react-text: 4526 -->Dynamic factor graphs for time series modeling<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4528 -->Mach. Learn. Knowl. Discovery Databases<!-- /react-text --><!-- react-text: 4529 --> (<!-- /react-text --><!-- react-text: 4530 -->2009<!-- /react-text --><!-- react-text: 4531 -->)<!-- /react-text --><!-- react-text: 4532 -->, pp. <!-- /react-text --><!-- react-text: 4533 -->128<!-- /react-text --><!-- react-text: 4534 -->-<!-- /react-text --><!-- react-text: 4535 -->143<!-- /react-text --></div></dd><dt class="label"><a href="#bb0455" id="ref-id-b0455">[91]</a></dt><dd class="reference" id="h0140"><span>P.
 Mirowski, D. Madhavan, Y. LeCun, Time-delay neural networks and 
independent component analysis for eeg-based prediction of epileptic 
seizures propagation, in: Association for the Advancement of Artificial 
Intelligence Conference, 2007.</span></dd><dt class="label"><a href="#bb0460" id="ref-id-b0460">[92]</a></dt><dd class="reference" id="h0585"><div class="contribution"><!-- react-text: 4544 -->P.W.<!-- /react-text --><!-- react-text: 4545 --> <!-- /react-text --><!-- react-text: 4546 -->Mirowski<!-- /react-text --><!-- react-text: 4547 -->, <!-- /react-text --><!-- react-text: 4548 -->Y.<!-- /react-text --><!-- react-text: 4549 --> <!-- /react-text --><!-- react-text: 4550 -->LeCun<!-- /react-text --><!-- react-text: 4551 -->, <!-- /react-text --><!-- react-text: 4552 -->D.<!-- /react-text --><!-- react-text: 4553 --> <!-- /react-text --><!-- react-text: 4554 -->Madhavan<!-- /react-text --><!-- react-text: 4555 -->, <!-- /react-text --><!-- react-text: 4556 -->R.<!-- /react-text --><!-- react-text: 4557 --> <!-- /react-text --><!-- react-text: 4558 -->Kuzniecky<!-- /react-text --><strong class="title"><!-- react-text: 4560 -->Comparing SVM and convolutional networks for epileptic seizure prediction from intracranial EEG<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4562 -->IEEE Workshop on Machine Learning for Signal Processing, 2008. MLSP 2008<!-- /react-text --><!-- react-text: 4563 -->, <!-- /react-text --><!-- react-text: 4564 -->IEEE<!-- /react-text --><!-- react-text: 4565 --> (<!-- /react-text --><!-- react-text: 4566 -->2008<!-- /react-text --><!-- react-text: 4567 -->)<!-- /react-text --><!-- react-text: 4568 -->, pp. <!-- /react-text --><!-- react-text: 4569 -->244<!-- /react-text --><!-- react-text: 4570 -->-<!-- /react-text --><!-- react-text: 4571 -->249<!-- /react-text --></div></dd><dt class="label"><a href="#bb0465" id="ref-id-b0465">[93]</a></dt><dd class="reference" id="h0590"><div class="contribution"><!-- react-text: 4576 -->A.<!-- /react-text --><!-- react-text: 4577 --> <!-- /react-text --><!-- react-text: 4578 -->Mohamed<!-- /react-text --><!-- react-text: 4579 -->, <!-- /react-text --><!-- react-text: 4580 -->G.E.<!-- /react-text --><!-- react-text: 4581 --> <!-- /react-text --><!-- react-text: 4582 -->Dahl<!-- /react-text --><!-- react-text: 4583 -->, <!-- /react-text --><!-- react-text: 4584 -->G.<!-- /react-text --><!-- react-text: 4585 --> <!-- /react-text --><!-- react-text: 4586 -->Hinton<!-- /react-text --><strong class="title"><!-- react-text: 4588 -->Acoustic modeling using deep belief networks<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4590 -->IEEE Trans. Audio Speech Lang. Process. Arch.<!-- /react-text --><!-- react-text: 4591 -->, <!-- /react-text --><!-- react-text: 4592 -->20<!-- /react-text --><!-- react-text: 4593 --> (<!-- /react-text --><!-- react-text: 4594 -->1<!-- /react-text --><!-- react-text: 4595 -->)<!-- /react-text --><!-- react-text: 4596 --> (<!-- /react-text --><!-- react-text: 4597 -->2012<!-- /react-text --><!-- react-text: 4598 -->)<!-- /react-text --><!-- react-text: 4599 -->, pp. <!-- /react-text --><!-- react-text: 4600 -->14<!-- /react-text --><!-- react-text: 4601 -->-<!-- /react-text --><!-- react-text: 4602 -->22<!-- /react-text --></div></dd><dt class="label"><a href="#bb0470" id="ref-id-b0470">[94]</a></dt><dd class="reference" id="h0145"><span>A.
 Mohamed, G. Hinton, Phone recognition using restricted Boltzmann 
machines, in: 2010 IEEE International Conference on Acoustics Speech and
 Signal Processing (ICASSP), 2010, pp. 4354–4357.</span></dd><dt class="label"><a href="#bb0475" id="ref-id-b0475">[95]</a></dt><dd class="reference" id="h0150"><span>J. Nam, Learning feature representations for music classification (Ph.D. thesis), Stanford University, 2012.</span></dd><dt class="label"><a href="#bb0480" id="ref-id-b0480">[96]</a></dt><dd class="reference" id="h0155"><span>J.
 Nam, J. Herrera, M. Slaney, J.O. Smith, Learning sparse feature 
representations for music annotation and retrieval, in: The 
International Society for Music Information Retrieval (ISMIR), 2012, pp.
 565–570.</span></dd><dt class="label"><a href="#bb0485" id="ref-id-b0485">[97]</a></dt><dd class="reference" id="h0595"><div class="contribution"><!-- react-text: 4619 -->A.<!-- /react-text --><!-- react-text: 4620 --> <!-- /react-text --><!-- react-text: 4621 -->Nanopoulos<!-- /react-text --><!-- react-text: 4622 -->, <!-- /react-text --><!-- react-text: 4623 -->R.<!-- /react-text --><!-- react-text: 4624 --> <!-- /react-text --><!-- react-text: 4625 -->Alcock<!-- /react-text --><!-- react-text: 4626 -->, <!-- /react-text --><!-- react-text: 4627 -->Y.<!-- /react-text --><!-- react-text: 4628 --> <!-- /react-text --><!-- react-text: 4629 -->Manolopoulos<!-- /react-text --><strong class="title"><!-- react-text: 4631 -->Feature-based classification of time-series data<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4633 -->Int. J. Comput. Res.<!-- /react-text --><!-- react-text: 4634 -->, <!-- /react-text --><!-- react-text: 4635 -->10<!-- /react-text --><!-- react-text: 4636 --> (<!-- /react-text --><!-- react-text: 4637 -->2001<!-- /react-text --><!-- react-text: 4638 -->)<!-- /react-text --><!-- react-text: 4639 -->, pp. <!-- /react-text --><!-- react-text: 4640 -->49<!-- /react-text --><!-- react-text: 4641 -->-<!-- /react-text --><!-- react-text: 4642 -->61<!-- /react-text --></div></dd><dt class="label"><a href="#bb0490" id="ref-id-b0490">[98]</a></dt><dd class="reference" id="h0160"><span>J.
 Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, A.Y. Ng, Multimodal deep 
learning, in: Proceedings of the 28th International Conference on 
Machine Learning, 2011.</span></dd><dt class="label"><a href="#bb0495" id="ref-id-b0495">[99]</a></dt><dd class="reference" id="h0600"><div class="contribution"><!-- react-text: 4651 -->G.R.<!-- /react-text --><!-- react-text: 4652 --> <!-- /react-text --><!-- react-text: 4653 -->Osuna<!-- /react-text --><!-- react-text: 4654 -->, <!-- /react-text --><!-- react-text: 4655 -->T.H.<!-- /react-text --><!-- react-text: 4656 --> <!-- /react-text --><!-- react-text: 4657 -->Nagle<!-- /react-text --><!-- react-text: 4658 -->, <!-- /react-text --><!-- react-text: 4659 -->B.<!-- /react-text --><!-- react-text: 4660 --> <!-- /react-text --><!-- react-text: 4661 -->Kermani<!-- /react-text --><!-- react-text: 4662 -->, <!-- /react-text --><!-- react-text: 4663 -->S.S.<!-- /react-text --><!-- react-text: 4664 --> <!-- /react-text --><!-- react-text: 4665 -->Schiffman<!-- /react-text --><strong class="title"><!-- react-text: 4667 -->Signal Conditioning and Preprocessing<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4669 -->HandBook of Machine Olfaction, Electronic Nose Technology<!-- /react-text --><!-- react-text: 4670 -->, <!-- /react-text --><!-- react-text: 4671 -->Wiley-Vch Verlag GmbH &amp; Co. KGaA<!-- /react-text --><!-- react-text: 4672 --> (<!-- /react-text --><!-- react-text: 4673 -->2003<!-- /react-text --><!-- react-text: 4674 -->)<!-- /react-text --></div><div class="comment">pp. 105–132</div></dd><dt class="label"><a href="#bb0500" id="ref-id-b0500">[100]</a></dt><dd class="reference" id="h0165"><span>W.
 Pan, L. Torresani, Unsupervised hierarchical modeling of locomotion 
styles, in: Proceedings of the 26th Annual International Conference on 
Machine Learning, 2009, pp. 785–792.</span></dd><dt class="label"><a href="#bb0505" id="ref-id-b0505">[101]</a></dt><dd class="reference" id="h0170"><span>E.
 Parris, M. Carey, Language independent gender identification, in: 1996 
IEEE International Conference on Acoustics, Speech, and Signal 
Processing, 1996. ICASSP-96. Conference Proceedings, vol. 2, 1996, pp. 
685–688.</span></dd><dt class="label"><a href="#bb0510" id="ref-id-b0510">[102]</a></dt><dd class="reference" id="h0175"><span>R.
 Pascanu, T. Mikolov, Y. Bengio, Understanding the exploding gradient 
problem, Computing Research Repository (CoRR) abs/1211.5063, 2012.</span></dd><dt class="label"><a href="#bb0515" id="ref-id-b0515">[103]</a></dt><dd class="reference" id="h0605"><div class="contribution"><!-- react-text: 4692 -->L.<!-- /react-text --><!-- react-text: 4693 --> <!-- /react-text --><!-- react-text: 4694 -->Rabiner<!-- /react-text --><!-- react-text: 4695 -->, <!-- /react-text --><!-- react-text: 4696 -->B.<!-- /react-text --><!-- react-text: 4697 --> <!-- /react-text --><!-- react-text: 4698 -->Juang<!-- /react-text --><strong class="title"><!-- react-text: 4700 -->An introduction to hidden Markov models<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4702 -->IEEE ASSP Mag.<!-- /react-text --><!-- react-text: 4703 -->, <!-- /react-text --><!-- react-text: 4704 -->3<!-- /react-text --><!-- react-text: 4705 --> (<!-- /react-text --><!-- react-text: 4706 -->1<!-- /react-text --><!-- react-text: 4707 -->)<!-- /react-text --><!-- react-text: 4708 --> (<!-- /react-text --><!-- react-text: 4709 -->1986<!-- /react-text --><!-- react-text: 4710 -->)<!-- /react-text --><!-- react-text: 4711 -->, pp. <!-- /react-text --><!-- react-text: 4712 -->4<!-- /react-text --><!-- react-text: 4713 -->-<!-- /react-text --><!-- react-text: 4714 -->16<!-- /react-text --></div></dd><dt class="label"><a href="#bb0520" id="ref-id-b0520">[104]</a></dt><dd class="reference" id="h0180"><span>R.
 Raina, A. Battle, H. Lee, B. Packer, A.Y. Ng, Self-taught learning: 
transfer learning from unlabeled data, in: Proceedings of the 24th 
International Conference on Machine Learning, 2007.</span></dd><dt class="label"><a href="#bb0525" id="ref-id-b0525">[105]</a></dt><dd class="reference" id="h0185"><span>M.
 Ranzato, G. Hinton, Modeling pixel means and covariances using 
factorized third-order Boltzmann machines, in: Proceedings of Computer 
Vision and Pattern Recognition Conference (CVPR 2010), 2010.</span></dd><dt class="label"><a href="#bb0530" id="ref-id-b0530">[106]</a></dt><dd class="reference" id="h0190"><span>M.
 Ranzato, A. Krizhevsky, G. Hinton, Factored 3-way restricted Boltzmann 
machines for modeling natural images, in: Proceedings of the 
International Conference on Artificial Intelligence and Statistics, 
2010.</span></dd><dt class="label"><a href="#bb0535" id="ref-id-b0535">[107]</a></dt><dd class="reference" id="h0610"><div class="contribution"><!-- react-text: 4731 -->M.<!-- /react-text --><!-- react-text: 4732 --> <!-- /react-text --><!-- react-text: 4733 -->Ranzato<!-- /react-text --><!-- react-text: 4734 -->, <!-- /react-text --><!-- react-text: 4735 -->C.<!-- /react-text --><!-- react-text: 4736 --> <!-- /react-text --><!-- react-text: 4737 -->Poultney<!-- /react-text --><!-- react-text: 4738 -->, <!-- /react-text --><!-- react-text: 4739 -->S.<!-- /react-text --><!-- react-text: 4740 --> <!-- /react-text --><!-- react-text: 4741 -->Chopra<!-- /react-text --><!-- react-text: 4742 -->, <!-- /react-text --><!-- react-text: 4743 -->Y.<!-- /react-text --><!-- react-text: 4744 --> <!-- /react-text --><!-- react-text: 4745 -->LeCun<!-- /react-text --><strong class="title"><!-- react-text: 4747 -->Efficient learning of sparse representations with an energy-based model<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4749 -->J.<!-- /react-text --><!-- react-text: 4750 --> <!-- /react-text --><!-- react-text: 4751 -->Platt<!-- /react-text --><!-- react-text: 4752 -->, <!-- /react-text --><em> et al.</em><!-- react-text: 4754 --> (Eds.)<!-- /react-text --><!-- react-text: 4755 -->, <!-- /react-text --><!-- react-text: 4756 -->Advances in Neural Information Processing Systems (NIPS 2006)<!-- /react-text --><!-- react-text: 4757 -->, <!-- /react-text --><!-- react-text: 4758 -->MIT Press<!-- /react-text --><!-- react-text: 4759 --> (<!-- /react-text --><!-- react-text: 4760 -->2006<!-- /react-text --><!-- react-text: 4761 -->)<!-- /react-text --></div></dd><dt class="label"><a href="#bb0540" id="ref-id-b0540">[108]</a></dt><dd class="reference" id="h0195"><span>A.
 Saxe, P. Koh, Z. Chen, M. Bhand, B. Suresh, A.Y. Ng, On random weights 
and unsupervised feature learning, in: Proceedings of the 28th 
International Conference on Machine Learning, 2011.</span></dd><dt class="label"><a href="#bb0545" id="ref-id-b0545">[109]</a></dt><dd class="reference" id="h0200"><span><!-- react-text: 4770 -->C. Schoerkhuber, A. Klapuri, Constant-<!-- /react-text --><em>q</em><!-- react-text: 4772 --> transform toolbox for music processing, in: Seventh Sound and Music Computing Conference, 2010.<!-- /react-text --></span></dd><dt class="label"><a href="#bb0550" id="ref-id-b0550">[110]</a></dt><dd class="reference" id="h0615"><div class="contribution"><!-- react-text: 4777 -->E.<!-- /react-text --><!-- react-text: 4778 --> <!-- /react-text --><!-- react-text: 4779 -->Smith<!-- /react-text --><!-- react-text: 4780 -->, <!-- /react-text --><!-- react-text: 4781 -->M.S.<!-- /react-text --><!-- react-text: 4782 --> <!-- /react-text --><!-- react-text: 4783 -->Lewicki<!-- /react-text --><strong class="title"><!-- react-text: 4785 -->Learning efficient auditory codes using spikes predicts cochlear filters<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4787 -->Advances in Neural Information Processing Systems<!-- /react-text --><!-- react-text: 4788 -->, <!-- /react-text --><!-- react-text: 4789 -->MIT Press<!-- /react-text --><!-- react-text: 4790 --> (<!-- /react-text --><!-- react-text: 4791 -->2005<!-- /react-text --><!-- react-text: 4792 -->)<!-- /react-text --></div></dd><dt class="label"><a href="#bb0555" id="ref-id-b0555">[111]</a></dt><dd class="reference" id="h0205"><span>D.
 Stavens, S. Thrun, Unsupervised learning of invariant features using 
video, in: 2010 IEEE Conference on Computer Vision and Pattern 
Recognition (CVPR), 2010, pp. 1649–1656.</span></dd><dt class="label"><a href="#bb0560" id="ref-id-b0560">[112]</a></dt><dd class="reference" id="h0210"><span>M.
 Sugiyama, H. Sawai, A. Waibel, Review of tdnn (time delay neural 
network) architectures for speech recognition, in: IEEE International 
Sympoisum on Circuits and Systems, vol. 1, 1991, pp. 582–585.</span></dd><dt class="label"><a href="#bb0565" id="ref-id-b0565">[113]</a></dt><dd class="reference" id="h0215"><span>I. Sutskever, Training recurrent neural networks (Ph.D. thesis), University of Toronto, 2012.</span></dd><dt class="label"><a href="#bb0570" id="ref-id-b0570">[114]</a></dt><dd class="reference" id="h0220"><span>I.
 Sutskever, G. Hinton, Learning multilevel distributed representations 
for high-dimensional sequences, Technical Report, University of Toronto,
 2006.</span></dd><dt class="label"><a href="#bb0575" id="ref-id-b0575">[115]</a></dt><dd class="reference" id="h0620"><div class="contribution"><!-- react-text: 4813 -->I.<!-- /react-text --><!-- react-text: 4814 --> <!-- /react-text --><!-- react-text: 4815 -->Sutskever<!-- /react-text --><!-- react-text: 4816 -->, <!-- /react-text --><!-- react-text: 4817 -->G.E.<!-- /react-text --><!-- react-text: 4818 --> <!-- /react-text --><!-- react-text: 4819 -->Hinton<!-- /react-text --><!-- react-text: 4820 -->, <!-- /react-text --><!-- react-text: 4821 -->G.W.<!-- /react-text --><!-- react-text: 4822 --> <!-- /react-text --><!-- react-text: 4823 -->Taylor<!-- /react-text --><strong class="title"><!-- react-text: 4825 -->The recurrent temporal restricted Boltzmann machine<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4827 -->Adv. Neural Inf. Process. Syst.<!-- /react-text --><!-- react-text: 4828 --> (<!-- /react-text --><!-- react-text: 4829 -->2008<!-- /react-text --><!-- react-text: 4830 -->)<!-- /react-text --><!-- react-text: 4831 -->, pp. <!-- /react-text --><!-- react-text: 4832 -->1601<!-- /react-text --><!-- react-text: 4833 -->-<!-- /react-text --><!-- react-text: 4834 -->1608<!-- /react-text --></div></dd><dt class="label"><a href="#bb0580" id="ref-id-b0580">[116]</a></dt><dd class="reference" id="h0225"><span>G.
 Taylor, R. Fergus, Y. LeCun, C. Bregler, Convolutional learning of 
spatio-temporal features, in: Proceedings European Conference on 
Computer Vision (ECCV’10), 2010.</span></dd><dt class="label"><a href="#bb0585" id="ref-id-b0585">[117]</a></dt><dd class="reference" id="h0230"><span>G.
 Taylor, G. Hinton, Factored conditional restricted Boltzmann machines 
for modeling motion style, in: Proceedings of the 26th International 
Conference on Machine Learning (ICML), 2009.</span></dd><dt class="label"><a href="#bb0590" id="ref-id-b0590">[118]</a></dt><dd class="reference" id="h0235"><span>G.
 Taylor, G.E. Hinton, S. Roweis, Modeling human motion using binary 
latent variables, in: Advances in Neural Information Processing Systems,
 2007.</span></dd><dt class="label"><a href="#bb0595" id="ref-id-b0595">[119]</a></dt><dd class="reference" id="h0240"><span>G.W.
 Taylor, Composable, distributed-state models for high-dimensional time 
series (Ph.D. thesis), Department of Computer Science, University of 
Toronto, 2009.</span></dd><dt class="label"><a href="#bb0600" id="ref-id-b0600">[120]</a></dt><dd class="reference" id="h0625"><div class="contribution"><!-- react-text: 4855 -->M.<!-- /react-text --><!-- react-text: 4856 --> <!-- /react-text --><!-- react-text: 4857 -->Trincavelli<!-- /react-text --><!-- react-text: 4858 -->, <!-- /react-text --><!-- react-text: 4859 -->S.<!-- /react-text --><!-- react-text: 4860 --> <!-- /react-text --><!-- react-text: 4861 -->Coradeschi<!-- /react-text --><!-- react-text: 4862 -->, <!-- /react-text --><!-- react-text: 4863 -->A.<!-- /react-text --><!-- react-text: 4864 --> <!-- /react-text --><!-- react-text: 4865 -->Loutfi<!-- /react-text --><!-- react-text: 4866 -->, <!-- /react-text --><!-- react-text: 4867 -->B.<!-- /react-text --><!-- react-text: 4868 --> <!-- /react-text --><!-- react-text: 4869 -->Söderquist<!-- /react-text --><!-- react-text: 4870 -->, <!-- /react-text --><!-- react-text: 4871 -->P.<!-- /react-text --><!-- react-text: 4872 --> <!-- /react-text --><!-- react-text: 4873 -->Thunberg<!-- /react-text --><strong class="title"><!-- react-text: 4875 -->Direct identification of bacteria in blood culture samples using an electronic nose<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4877 -->IEEE Trans. Biomed. Eng.<!-- /react-text --><!-- react-text: 4878 -->, <!-- /react-text --><!-- react-text: 4879 -->57<!-- /react-text --><!-- react-text: 4880 --> (<!-- /react-text --><!-- react-text: 4881 -->2010<!-- /react-text --><!-- react-text: 4882 -->)<!-- /react-text --><!-- react-text: 4883 -->, pp. <!-- /react-text --><!-- react-text: 4884 -->2884<!-- /react-text --><!-- react-text: 4885 -->-<!-- /react-text --><!-- react-text: 4886 -->2890<!-- /react-text --></div></dd><dt class="label"><a href="#bb0605" id="ref-id-b0605">[121]</a></dt><dd class="reference" id="h0630"><div class="contribution"><!-- react-text: 4891 -->C.F.<!-- /react-text --><!-- react-text: 4892 --> <!-- /react-text --><!-- react-text: 4893 -->Tsai<!-- /react-text --><!-- react-text: 4894 -->, <!-- /react-text --><!-- react-text: 4895 -->Y.C.<!-- /react-text --><!-- react-text: 4896 --> <!-- /react-text --><!-- react-text: 4897 -->Hsiao<!-- /react-text --><strong class="title"><!-- react-text: 4899 -->Combining multiple feature selection methods for stock prediction: union, intersection, and multi-intersection approaches<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4901 -->Decis. Support Syst.<!-- /react-text --><!-- react-text: 4902 -->, <!-- /react-text --><!-- react-text: 4903 -->50<!-- /react-text --><!-- react-text: 4904 --> (<!-- /react-text --><!-- react-text: 4905 -->2010<!-- /react-text --><!-- react-text: 4906 -->)<!-- /react-text --><!-- react-text: 4907 -->, pp. <!-- /react-text --><!-- react-text: 4908 -->258<!-- /react-text --><!-- react-text: 4909 -->-<!-- /react-text --><!-- react-text: 4910 -->269<!-- /react-text --></div></dd><dt class="label"><a href="#bb0610" id="ref-id-b0610">[122]</a></dt><dd class="reference" id="h0245"><span>C.
 Tucker, Self-organizing maps for time series analysis of 
electromyographic data, in: International Joint Conference on Neural 
Networks, 1999. IJCNN ’99, 1999, pp. 3577–3580.</span></dd><dt class="label"><a href="#bb0615" id="ref-id-b0615">[123]</a></dt><dd class="reference" id="h0635"><div class="contribution"><!-- react-text: 4919 -->S.<!-- /react-text --><!-- react-text: 4920 --> <!-- /react-text --><!-- react-text: 4921 -->Vembu<!-- /react-text --><!-- react-text: 4922 -->, <!-- /react-text --><!-- react-text: 4923 -->A.<!-- /react-text --><!-- react-text: 4924 --> <!-- /react-text --><!-- react-text: 4925 -->Vergara<!-- /react-text --><!-- react-text: 4926 -->, <!-- /react-text --><!-- react-text: 4927 -->M.K.<!-- /react-text --><!-- react-text: 4928 --> <!-- /react-text --><!-- react-text: 4929 -->Muezzinoglu<!-- /react-text --><!-- react-text: 4930 -->, <!-- /react-text --><!-- react-text: 4931 -->R.<!-- /react-text --><!-- react-text: 4932 --> <!-- /react-text --><!-- react-text: 4933 -->Huerta<!-- /react-text --><strong class="title"><!-- react-text: 4935 -->On time series features and kernels for machine olfaction<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4937 -->Sens. Actuators B: Chem.<!-- /react-text --><!-- react-text: 4938 -->, <!-- /react-text --><!-- react-text: 4939 -->174<!-- /react-text --><!-- react-text: 4940 --> (<!-- /react-text --><!-- react-text: 4941 -->2012<!-- /react-text --><!-- react-text: 4942 -->)<!-- /react-text --><!-- react-text: 4943 -->, pp. <!-- /react-text --><!-- react-text: 4944 -->535<!-- /react-text --><!-- react-text: 4945 -->-<!-- /react-text --><!-- react-text: 4946 -->546<!-- /react-text --></div></dd><dt class="label"><a href="#bb0620" id="ref-id-b0620">[124]</a></dt><dd class="reference" id="h0640"><div class="contribution"><!-- react-text: 4951 -->S.D.<!-- /react-text --><!-- react-text: 4952 --> <!-- /react-text --><!-- react-text: 4953 -->Vito<!-- /react-text --><!-- react-text: 4954 -->, <!-- /react-text --><!-- react-text: 4955 -->A.<!-- /react-text --><!-- react-text: 4956 --> <!-- /react-text --><!-- react-text: 4957 -->Castaldo<!-- /react-text --><!-- react-text: 4958 -->, <!-- /react-text --><!-- react-text: 4959 -->F.<!-- /react-text --><!-- react-text: 4960 --> <!-- /react-text --><!-- react-text: 4961 -->Loffredo<!-- /react-text --><!-- react-text: 4962 -->, <!-- /react-text --><!-- react-text: 4963 -->E.<!-- /react-text --><!-- react-text: 4964 --> <!-- /react-text --><!-- react-text: 4965 -->Massera<!-- /react-text --><!-- react-text: 4966 -->, <!-- /react-text --><!-- react-text: 4967 -->T.<!-- /react-text --><!-- react-text: 4968 --> <!-- /react-text --><!-- react-text: 4969 -->Polichetti<!-- /react-text --><!-- react-text: 4970 -->, <!-- /react-text --><!-- react-text: 4971 -->I.<!-- /react-text --><!-- react-text: 4972 --> <!-- /react-text --><!-- react-text: 4973 -->Nasti<!-- /react-text --><!-- react-text: 4974 -->, <!-- /react-text --><!-- react-text: 4975 -->P.<!-- /react-text --><!-- react-text: 4976 --> <!-- /react-text --><!-- react-text: 4977 -->Vacca<!-- /react-text --><!-- react-text: 4978 -->, <!-- /react-text --><!-- react-text: 4979 -->L.<!-- /react-text --><!-- react-text: 4980 --> <!-- /react-text --><!-- react-text: 4981 -->Quercia<!-- /react-text --><!-- react-text: 4982 -->, <!-- /react-text --><!-- react-text: 4983 -->G.D.<!-- /react-text --><!-- react-text: 4984 --> <!-- /react-text --><!-- react-text: 4985 -->Francia<!-- /react-text --><strong class="title"><!-- react-text: 4987 -->Gas concentration estimation in ternary mixtures with room temperature operating sensor array using tapped delay architectures<!-- /react-text --></strong></div><div class="host"><!-- react-text: 4989 -->Sens. Actuators B: Chem.<!-- /react-text --><!-- react-text: 4990 -->, <!-- /react-text --><!-- react-text: 4991 -->124<!-- /react-text --><!-- react-text: 4992 --> (<!-- /react-text --><!-- react-text: 4993 -->2007<!-- /react-text --><!-- react-text: 4994 -->)<!-- /react-text --><!-- react-text: 4995 -->, pp. <!-- /react-text --><!-- react-text: 4996 -->309<!-- /react-text --><!-- react-text: 4997 -->-<!-- /react-text --><!-- react-text: 4998 -->316<!-- /react-text --></div></dd><dt class="label"><a href="#bb0625" id="ref-id-b0625">[125]</a></dt><dd class="reference" id="h0645"><div class="contribution"><!-- react-text: 5003 -->A.<!-- /react-text --><!-- react-text: 5004 --> <!-- /react-text --><!-- react-text: 5005 -->Waibel<!-- /react-text --><!-- react-text: 5006 -->, <!-- /react-text --><!-- react-text: 5007 -->T.<!-- /react-text --><!-- react-text: 5008 --> <!-- /react-text --><!-- react-text: 5009 -->Hanazawa<!-- /react-text --><!-- react-text: 5010 -->, <!-- /react-text --><!-- react-text: 5011 -->G.<!-- /react-text --><!-- react-text: 5012 --> <!-- /react-text --><!-- react-text: 5013 -->Hinton<!-- /react-text --><!-- react-text: 5014 -->, <!-- /react-text --><!-- react-text: 5015 -->K.<!-- /react-text --><!-- react-text: 5016 --> <!-- /react-text --><!-- react-text: 5017 -->Shikano<!-- /react-text --><!-- react-text: 5018 -->, <!-- /react-text --><!-- react-text: 5019 -->K.<!-- /react-text --><!-- react-text: 5020 --> <!-- /react-text --><!-- react-text: 5021 -->Lang<!-- /react-text --><strong class="title"><!-- react-text: 5023 -->Phoneme recognition using time-delay neural networks<!-- /react-text --></strong></div><div class="host"><!-- react-text: 5025 -->IEEE Trans. Acoust. Speech Signal Process.<!-- /react-text --><!-- react-text: 5026 -->, <!-- /react-text --><!-- react-text: 5027 -->37<!-- /react-text --><!-- react-text: 5028 --> (<!-- /react-text --><!-- react-text: 5029 -->1989<!-- /react-text --><!-- react-text: 5030 -->)<!-- /react-text --><!-- react-text: 5031 -->, pp. <!-- /react-text --><!-- react-text: 5032 -->328<!-- /react-text --><!-- react-text: 5033 -->-<!-- /react-text --><!-- react-text: 5034 -->339<!-- /react-text --></div></dd><dt class="label"><a href="#bb0630" id="ref-id-b0630">[126]</a></dt><dd class="reference" id="h0650"><div class="contribution"><!-- react-text: 5039 -->D.<!-- /react-text --><!-- react-text: 5040 --> <!-- /react-text --><!-- react-text: 5041 -->Wang<!-- /react-text --><!-- react-text: 5042 -->, <!-- /react-text --><!-- react-text: 5043 -->Y.<!-- /react-text --><!-- react-text: 5044 --> <!-- /react-text --><!-- react-text: 5045 -->Shang<!-- /react-text --><strong class="title"><!-- react-text: 5047 -->Modeling physiological data with deep belief networks<!-- /react-text --></strong></div><div class="host"><!-- react-text: 5049 -->Int. J. Inf. Educ. Technol.<!-- /react-text --><!-- react-text: 5050 -->, <!-- /react-text --><!-- react-text: 5051 -->3<!-- /react-text --><!-- react-text: 5052 --> (<!-- /react-text --><!-- react-text: 5053 -->2013<!-- /react-text --><!-- react-text: 5054 -->)<!-- /react-text --></div></dd><dt class="label"><a href="#bb0635" id="ref-id-b0635">[127]</a></dt><dd class="reference" id="h0250"><span>J.M.
 Wang, D.J. Fleet, A. Hertzmann, Multi-factor Gaussian process models 
for style-content separation, in: International Conference of Machine 
Learning (ICML), 2007, pp. 975–982.</span></dd><dt class="label"><a href="#bb0640" id="ref-id-b0640">[128]</a></dt><dd class="reference" id="h0655"><div class="contribution"><!-- react-text: 5063 -->L.<!-- /react-text --><!-- react-text: 5064 --> <!-- /react-text --><!-- react-text: 5065 -->Wiskott<!-- /react-text --><!-- react-text: 5066 -->, <!-- /react-text --><!-- react-text: 5067 -->T.J.<!-- /react-text --><!-- react-text: 5068 --> <!-- /react-text --><!-- react-text: 5069 -->Sejnowski<!-- /react-text --><strong class="title"><!-- react-text: 5071 -->Slow feature analysis: unsupervised learning of invariances<!-- /react-text --></strong></div><div class="host"><!-- react-text: 5073 -->Neural Comput.<!-- /react-text --><!-- react-text: 5074 -->, <!-- /react-text --><!-- react-text: 5075 -->14<!-- /react-text --><!-- react-text: 5076 --> (<!-- /react-text --><!-- react-text: 5077 -->2002<!-- /react-text --><!-- react-text: 5078 -->)<!-- /react-text --><!-- react-text: 5079 -->, pp. <!-- /react-text --><!-- react-text: 5080 -->715<!-- /react-text --><!-- react-text: 5081 -->-<!-- /react-text --><!-- react-text: 5082 -->770<!-- /react-text --></div></dd><dt class="label"><a href="#bb0645" id="ref-id-b0645">[129]</a></dt><dd class="reference" id="h0660"><div class="contribution"><!-- react-text: 5087 -->D.<!-- /react-text --><!-- react-text: 5088 --> <!-- /react-text --><!-- react-text: 5089 -->Wulsin<!-- /react-text --><!-- react-text: 5090 -->, <!-- /react-text --><!-- react-text: 5091 -->J.<!-- /react-text --><!-- react-text: 5092 --> <!-- /react-text --><!-- react-text: 5093 -->Gupta<!-- /react-text --><!-- react-text: 5094 -->, <!-- /react-text --><!-- react-text: 5095 -->R.<!-- /react-text --><!-- react-text: 5096 --> <!-- /react-text --><!-- react-text: 5097 -->Mani<!-- /react-text --><!-- react-text: 5098 -->, <!-- /react-text --><!-- react-text: 5099 -->J.<!-- /react-text --><!-- react-text: 5100 --> <!-- /react-text --><!-- react-text: 5101 -->Blanco<!-- /react-text --><!-- react-text: 5102 -->, <!-- /react-text --><!-- react-text: 5103 -->B.<!-- /react-text --><!-- react-text: 5104 --> <!-- /react-text --><!-- react-text: 5105 -->Litt<!-- /react-text --><strong class="title"><!-- react-text: 5107 -->Modeling electroencephalography waveforms with semi-supervised deep belief nets: faster classification and anomaly measurement<!-- /react-text --></strong></div><div class="host"><!-- react-text: 5109 -->J. Neural Eng.<!-- /react-text --><!-- react-text: 5110 -->, <!-- /react-text --><!-- react-text: 5111 -->8<!-- /react-text --><!-- react-text: 5112 --> (<!-- /react-text --><!-- react-text: 5113 -->2011<!-- /react-text --><!-- react-text: 5114 -->)<!-- /react-text --><!-- react-text: 5115 -->, pp. <!-- /react-text --><!-- react-text: 5116 -->1741<!-- /react-text --><!-- react-text: 5117 -->-<!-- /react-text --><!-- react-text: 5118 -->2552<!-- /react-text --></div></dd><dt class="label"><a href="#bb0650" id="ref-id-b0650">[130]</a></dt><dd class="reference" id="h0665"><div class="contribution"><!-- react-text: 5123 -->A.<!-- /react-text --><!-- react-text: 5124 --> <!-- /react-text --><!-- react-text: 5125 -->Yamazaki<!-- /react-text --><!-- react-text: 5126 -->, <!-- /react-text --><!-- react-text: 5127 -->T.<!-- /react-text --><!-- react-text: 5128 --> <!-- /react-text --><!-- react-text: 5129 -->Ludermir<!-- /react-text --><!-- react-text: 5130 -->, <!-- /react-text --><!-- react-text: 5131 -->M.C.P.<!-- /react-text --><!-- react-text: 5132 --> <!-- /react-text --><!-- react-text: 5133 -->De Souto<!-- /react-text --><strong class="title"><!-- react-text: 5135 -->Classification of vintages of wine by artificial nose using time delay neural networks<!-- /react-text --></strong></div><div class="host"><!-- react-text: 5137 -->Electron. Lett.<!-- /react-text --><!-- react-text: 5138 -->, <!-- /react-text --><!-- react-text: 5139 -->37<!-- /react-text --><!-- react-text: 5140 --> (<!-- /react-text --><!-- react-text: 5141 -->2001<!-- /react-text --><!-- react-text: 5142 -->)<!-- /react-text --><!-- react-text: 5143 -->, pp. <!-- /react-text --><!-- react-text: 5144 -->1466<!-- /react-text --><!-- react-text: 5145 -->-<!-- /react-text --><!-- react-text: 5146 -->1467<!-- /react-text --></div></dd><dt class="label"><a href="#bb0655" id="ref-id-b0655">[131]</a></dt><dd class="reference" id="h0670"><div class="contribution"><!-- react-text: 5151 -->Q.<!-- /react-text --><!-- react-text: 5152 --> <!-- /react-text --><!-- react-text: 5153 -->Yang<!-- /react-text --><!-- react-text: 5154 -->, <!-- /react-text --><!-- react-text: 5155 -->X.<!-- /react-text --><!-- react-text: 5156 --> <!-- /react-text --><!-- react-text: 5157 -->Wu<!-- /react-text --><strong class="title"><!-- react-text: 5159 -->10 Challenging problems in data mining research<!-- /react-text --></strong></div><div class="host"><!-- react-text: 5161 -->Int. J. Inf. Technol. Decis. Making<!-- /react-text --><!-- react-text: 5162 -->, <!-- /react-text --><!-- react-text: 5163 -->05<!-- /react-text --><!-- react-text: 5164 --> (<!-- /react-text --><!-- react-text: 5165 -->2006<!-- /react-text --><!-- react-text: 5166 -->)<!-- /react-text --><!-- react-text: 5167 -->, pp. <!-- /react-text --><!-- react-text: 5168 -->597<!-- /react-text --><!-- react-text: 5169 -->-<!-- /react-text --><!-- react-text: 5170 -->604<!-- /react-text --></div></dd><dt class="label"><a href="#bb0660" id="ref-id-b0660">[132]</a></dt><dd class="reference" id="h0675"><div class="contribution"><!-- react-text: 5175 -->S.<!-- /react-text --><!-- react-text: 5176 --> <!-- /react-text --><!-- react-text: 5177 -->Zampolli<!-- /react-text --><!-- react-text: 5178 -->, <!-- /react-text --><!-- react-text: 5179 -->I.<!-- /react-text --><!-- react-text: 5180 --> <!-- /react-text --><!-- react-text: 5181 -->Elmi<!-- /react-text --><!-- react-text: 5182 -->, <!-- /react-text --><!-- react-text: 5183 -->F.<!-- /react-text --><!-- react-text: 5184 --> <!-- /react-text --><!-- react-text: 5185 -->Ahmed<!-- /react-text --><!-- react-text: 5186 -->, <!-- /react-text --><!-- react-text: 5187 -->M.<!-- /react-text --><!-- react-text: 5188 --> <!-- /react-text --><!-- react-text: 5189 -->Passini<!-- /react-text --><!-- react-text: 5190 -->, <!-- /react-text --><!-- react-text: 5191 -->G.<!-- /react-text --><!-- react-text: 5192 --> <!-- /react-text --><!-- react-text: 5193 -->Cardinali<!-- /react-text --><!-- react-text: 5194 -->, <!-- /react-text --><!-- react-text: 5195 -->S.<!-- /react-text --><!-- react-text: 5196 --> <!-- /react-text --><!-- react-text: 5197 -->Nicoletti<!-- /react-text --><!-- react-text: 5198 -->, <!-- /react-text --><!-- react-text: 5199 -->L.<!-- /react-text --><!-- react-text: 5200 --> <!-- /react-text --><!-- react-text: 5201 -->Dori<!-- /react-text --><strong class="title"><!-- react-text: 5203 -->An electronic nose based on solid state sensor arrays for low-cost indoor air quality monitoring applications<!-- /react-text --></strong></div><div class="host"><!-- react-text: 5205 -->Sens. Actuators B: Chem.<!-- /react-text --><!-- react-text: 5206 -->, <!-- /react-text --><!-- react-text: 5207 -->101<!-- /react-text --><!-- react-text: 5208 --> (<!-- /react-text --><!-- react-text: 5209 -->2004<!-- /react-text --><!-- react-text: 5210 -->)<!-- /react-text --><!-- react-text: 5211 -->, pp. <!-- /react-text --><!-- react-text: 5212 -->39<!-- /react-text --><!-- react-text: 5213 -->-<!-- /react-text --><!-- react-text: 5214 -->46<!-- /react-text --></div></dd><dt class="label"><a href="#bb0665" id="ref-id-b0665">[133]</a></dt><dd class="reference" id="h0680"><div class="contribution"><!-- react-text: 5219 -->H.<!-- /react-text --><!-- react-text: 5220 --> <!-- /react-text --><!-- react-text: 5221 -->Zhang<!-- /react-text --><!-- react-text: 5222 -->, <!-- /react-text --><!-- react-text: 5223 -->M.O.<!-- /react-text --><!-- react-text: 5224 --> <!-- /react-text --><!-- react-text: 5225 -->Balaban<!-- /react-text --><!-- react-text: 5226 -->, <!-- /react-text --><!-- react-text: 5227 -->J.C.<!-- /react-text --><!-- react-text: 5228 --> <!-- /react-text --><!-- react-text: 5229 -->Principe<!-- /react-text --><strong class="title"><!-- react-text: 5231 -->Improving pattern recognition of electronic nose data with time-delay neural networks<!-- /react-text --></strong></div><div class="host"><!-- react-text: 5233 -->Sens. Actuators B: Chem.<!-- /react-text --><!-- react-text: 5234 -->, <!-- /react-text --><!-- react-text: 5235 -->96<!-- /react-text --><!-- react-text: 5236 --> (<!-- /react-text --><!-- react-text: 5237 -->2003<!-- /react-text --><!-- react-text: 5238 -->)<!-- /react-text --><!-- react-text: 5239 -->, pp. <!-- /react-text --><!-- react-text: 5240 -->385<!-- /react-text --><!-- react-text: 5241 -->-<!-- /react-text --><!-- react-text: 5242 -->389<!-- /react-text --></div></dd><dt class="label"><a href="#bb0670" id="ref-id-b0670">[134]</a></dt><dd class="reference" id="h0685"><div class="contribution"><!-- react-text: 5247 -->X.<!-- /react-text --><!-- react-text: 5248 --> <!-- /react-text --><!-- react-text: 5249 -->Zhu<!-- /react-text --><!-- react-text: 5250 -->, <!-- /react-text --><!-- react-text: 5251 -->H.<!-- /react-text --><!-- react-text: 5252 --> <!-- /react-text --><!-- react-text: 5253 -->Wang<!-- /react-text --><!-- react-text: 5254 -->, <!-- /react-text --><!-- react-text: 5255 -->L.<!-- /react-text --><!-- react-text: 5256 --> <!-- /react-text --><!-- react-text: 5257 -->Xu<!-- /react-text --><!-- react-text: 5258 -->, <!-- /react-text --><!-- react-text: 5259 -->H.<!-- /react-text --><!-- react-text: 5260 --> <!-- /react-text --><!-- react-text: 5261 -->Li<!-- /react-text --><strong class="title"><!-- react-text: 5263 -->Predicting stock index increments by neural networks: the role of trading volume under different horizons<!-- /react-text --></strong></div><div class="host"><!-- react-text: 5265 -->Expert Syst. Appl.<!-- /react-text --><!-- react-text: 5266 -->, <!-- /react-text --><!-- react-text: 5267 -->34<!-- /react-text --><!-- react-text: 5268 --> (<!-- /react-text --><!-- react-text: 5269 -->2008<!-- /react-text --><!-- react-text: 5270 -->)<!-- /react-text --><!-- react-text: 5271 -->, pp. <!-- /react-text --><!-- react-text: 5272 -->3043<!-- /react-text --><!-- react-text: 5273 -->-<!-- /react-text --><!-- react-text: 5274 -->3054<!-- /react-text --></div></dd><dt class="label"><a href="#bb0675" id="ref-id-b0675">[135]</a></dt><dd class="reference" id="h0255"><span>W.Y.
 Zou, A.Y. Ng, K. Yu, Unsupervised learning of visual invariance with 
temporal coherence, in: NIPS 2011 Workshop on Deep Learning and 
Unsupervised Feature Learning, 2011.</span></dd></dl></section></section></div><!-- react-empty: 149 --><!-- react-empty: 150 --><!-- react-empty: 151 --><div class="Footnotes"><dl class="footnote"><dt class="footnote-label"><sup><a href="#baep-article-footnote-id12">☆</a></sup></dt><dd><p id="np005">This paper has been recommended for acceptance by A. Petrosino.</p></dd></dl><dl class="footnote"><dt class="footnote-label"><sup><a href="#bfn1">1</a></sup></dt><dd><p id="np010"><a href="http://www.nada.kth.se/cvap/actions/" target="_blank">http://www.nada.kth.se/cvap/actions/</a></p></dd></dl><dl class="footnote"><dt class="footnote-label"><sup><a href="#bfn2">2</a></sup></dt><dd><p id="np015"><a href="http://www.ldc.upenn.edu/Catalog/" target="_blank">http://www.ldc.upenn.edu/Catalog/</a></p></dd></dl><dl class="footnote"><dt class="footnote-label"><sup><a href="#bfn3">3</a></sup></dt><dd><p id="np020"><a href="http://marsyas.info/download/data_sets" target="_blank">http://marsyas.info/download/data_sets</a></p></dd></dl><dl class="footnote"><dt class="footnote-label"><sup><a href="#bfn4">4</a></sup></dt><dd><p id="np025"><a href="http://mocap.cs.cmu.edu/" target="_blank">http://mocap.cs.cmu.edu/</a></p></dd></dl></div><div class="Copyright"><span class="copyright-line">Copyright © 2014 Elsevier B.V. All rights reserved.</span></div></article><div class="hide-m hide-t-s cl-t-3-9 cl-l-3-12 pad-right"><aside class="RelatedContent" role="complementary" aria-label="Related content"><!-- react-empty: 163 --><section class="SidePanel"><header class="side-panel-header" id="recommended-articles-header"><a class="arrow-btn head rotated" href="#" aria-controls="recommended-articles" aria-expanded="true"><span class="Icon ArrowDown" style="display: inline-block; height: 16px; width: 16px; margin-right: 8px; transition: transform 0.3s ease 0s, -webkit-transform 0.3s ease 0s; transform: scaleY(-1);"><svg fill="currentColor" tabindex="-1" focusable="false" height="16" width="16" viewBox="0 0 16 16" style="width: 100%; height: 100%;"><path d="M16 4.452l-1.26-1.26L8 9.932l-6.74-6.74L0 4.452l8 8 8-8z"></path></svg></span><h2><span class="section-title">Recommended articles</span></h2></a></header><div id="recommended-articles" aria-hidden="false"><ul><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S0925231213007388" data-hack="#"><h3 class="article-title ellipsis" id="recommended-articles-article0-title"><span><!-- react-text: 194 -->Time series forecasting using a deep belief network with restricted Boltzmann machines<!-- /react-text --></span></h3></a><div class="article-source ellipsis"><div class="source">Neurocomputing, Volume 137, 2014, pp. 47-56</div></div></div><div class="buttons"><a href="https://www.sciencedirect.com/science/article/pii/S0925231213007388/pdfft?md5=3a94c36f41bc3de091386e202703890d&amp;pid=1-s2.0-S0925231213007388-main.pdf" class="icon-link" rel="nofollow" aria-describedby="recommended-articles-article0-title" target="_blank" data-hack="#"><i class="icon-pdf"></i><span class="primary size-m">Download PDF</span></a><a class="icon-link arrow-btn" href="#" aria-describedby="recommended-articles-article0-title" aria-controls="recommended-articles-article0" aria-expanded="false"><span class="Icon ArrowDown" style="display: inline-block; height: 12px; width: 12px; margin-right: 8px; transition: transform 0.3s ease 0s, -webkit-transform 0.3s ease 0s;"><svg fill="currentColor" tabindex="-1" focusable="false" height="16" width="16" viewBox="0 0 16 16" style="width: 100%; height: 100%;"><path d="M16 4.452l-1.26-1.26L8 9.932l-6.74-6.74L0 4.452l8 8 8-8z"></path></svg></span><span class="view-more">View details</span></a></div><div id="recommended-articles-article0" aria-hidden="true"></div></li><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S0893608014002135" data-hack="#"><h3 class="article-title ellipsis" id="recommended-articles-article1-title"><span><!-- react-text: 212 -->Deep learning in neural networks: An overview<!-- /react-text --></span></h3></a><div class="article-source ellipsis"><div class="source">Neural Networks, Volume 61, 2015, pp. 85-117</div></div></div><div class="buttons"><a href="https://www.sciencedirect.com/science/article/pii/S0893608014002135/pdfft?md5=faa1dd773ca0a8371a71c3221dc8086d&amp;pid=1-s2.0-S0893608014002135-main.pdf" class="icon-link" rel="nofollow" aria-describedby="recommended-articles-article1-title" target="_blank" data-hack="#"><i class="icon-pdf"></i><span class="primary size-m">Download PDF</span></a><a class="icon-link arrow-btn" href="#" aria-describedby="recommended-articles-article1-title" aria-controls="recommended-articles-article1" aria-expanded="false"><span class="Icon ArrowDown" style="display: inline-block; height: 12px; width: 12px; margin-right: 8px; transition: transform 0.3s ease 0s, -webkit-transform 0.3s ease 0s;"><svg fill="currentColor" tabindex="-1" focusable="false" height="16" width="16" viewBox="0 0 16 16" style="width: 100%; height: 100%;"><path d="M16 4.452l-1.26-1.26L8 9.932l-6.74-6.74L0 4.452l8 8 8-8z"></path></svg></span><span class="view-more">View details</span></a></div><div id="recommended-articles-article1" aria-hidden="true"></div></li><li class="SidePanelItem"><div class="sub-heading"><a href="https://www.sciencedirect.com/science/article/pii/S0925231213004888" data-hack="#"><h3 class="article-title ellipsis" id="recommended-articles-article2-title"><span><!-- react-text: 230 -->Active deep learning method for semi-supervised sentiment classification<!-- /react-text --></span></h3></a><div class="article-source ellipsis"><div class="source">Neurocomputing, Volume 120, 2013, pp. 536-546</div></div></div><div class="buttons"><a href="https://www.sciencedirect.com/science/article/pii/S0925231213004888/pdfft?md5=35d2f6507f48ccb891088ef8c33b7143&amp;pid=1-s2.0-S0925231213004888-main.pdf" class="icon-link" rel="nofollow" aria-describedby="recommended-articles-article2-title" target="_blank" data-hack="#"><i class="icon-pdf"></i><span class="primary size-m">Download PDF</span></a><a class="icon-link arrow-btn" href="#" aria-describedby="recommended-articles-article2-title" aria-controls="recommended-articles-article2" aria-expanded="false"><span class="Icon ArrowDown" style="display: inline-block; height: 12px; width: 12px; margin-right: 8px; transition: transform 0.3s ease 0s, -webkit-transform 0.3s ease 0s;"><svg fill="currentColor" tabindex="-1" focusable="false" height="16" width="16" viewBox="0 0 16 16" style="width: 100%; height: 100%;"><path d="M16 4.452l-1.26-1.26L8 9.932l-6.74-6.74L0 4.452l8 8 8-8z"></path></svg></span><span class="view-more">View details</span></a></div><div id="recommended-articles-article2" aria-hidden="true"></div></li></ul><a href="https://www.sciencedirect.com/science/recommended-articles/S0167865514000221" class="view-more-link" target="_self" aria-describedby="recommended-articles-header"><!-- react-text: 244 -->View more articles<!-- /react-text --><span class="Icon ArrowDown" style="display: inline-block; height: 8px; width: 8px;"><svg fill="currentColor" tabindex="-1" focusable="false" height="16" width="16" viewBox="0 0 16 16" style="width: 100%; height: 100%;"><path d="M16 4.452l-1.26-1.26L8 9.932l-6.74-6.74L0 4.452l8 8 8-8z"></path></svg></span></a></div></section><section class="SidePanel"><header class="side-panel-header" id="citing-articles-header"><a class="arrow-btn head" href="#" aria-controls="citing-articles" aria-expanded="false"><span class="Icon ArrowDown" style="display: inline-block; height: 16px; width: 16px; margin-right: 8px; transition: transform 0.3s ease 0s, -webkit-transform 0.3s ease 0s;"><svg fill="currentColor" tabindex="-1" focusable="false" height="16" width="16" viewBox="0 0 16 16" style="width: 100%; height: 100%;"><path d="M16 4.452l-1.26-1.26L8 9.932l-6.74-6.74L0 4.452l8 8 8-8z"></path></svg></span><h2><span class="section-title">Citing articles (95)</span></h2></a></header><div aria-hidden="true" id="citing-articles"></div></section><!-- react-empty: 174 --></aside></div></div><div></div></div></section></div></div></div></div></div>
      <footer>
        <div class="ccs-component"><div class="Footer" role="contentinfo"><div class="els-footer"><div class="els-footer-body"><a class="els-footer-elsevier" href="https://www.elsevier.com/" rel="nofollow" target="_blank"><span class="ccs-icon svg-wordmark-elsevier" aria-label="Elsevier"><span class="svg-text">Elsevier</span></span></a><div class="els-footer-content"><div class="els-footer-links"><a class="link-inherit" href="https://www.elsevier.com/solutions/sciencedirect" rel="nofollow" target="_blank">About ScienceDirect</a><wbr><a class="link-inherit" href="https://www.sciencedirect.com/science/activateaccess" rel="nofollow" target="_blank">Remote access</a><wbr><a class="link-inherit" href="https://www.sciencedirect.com/science?_ob=ShoppingCartURL&amp;_method=display&amp;md5=3ff44acb300f01481824c54a2973d019" rel="nofollow" target="_blank">Shopping cart</a><wbr><a class="link-inherit" href="https://service.elsevier.com/app/contact/supporthub/sciencedirect/" rel="nofollow" target="_blank">Contact and support</a><wbr><a class="link-inherit" href="https://www.elsevier.com/legal/elsevier-website-terms-and-conditions" rel="nofollow" target="_blank">Terms and conditions</a><wbr><a class="link-inherit" href="https://www.elsevier.com/legal/privacy-policy" rel="nofollow" target="_blank">Privacy policy</a></div><div class="els-footer-cookie">Cookies are used by this site. For more information, visit the <a class="link-blue" href="https://www.elsevier.com/solutions/sciencedirect/support/cookies" rel="nofollow" target="_blank">cookies page</a>.</div><div class="els-footer-copyright">Copyright © <span class="copyright-year">2017</span> Elsevier B.V. or its licensors or contributors. ScienceDirect ® is a registered trademark of Elsevier B.V.</div></div></div><div class="els-footer-relx-group"><a class="els-footer-relx" href="https://www.relx.com/" rel="nofollow" target="_blank"><span class="ccs-icon svg-wordmark-relx-group" aria-label="RELX Group"><span class="svg-text">RELX Group</span></span></a></div></div><script>if(typeof fbq === 'undefined') {
    !function(f,b,e,v,n,t,s){if(f.fbq)return;
    n=f.fbq=function(){n.callMethod?n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;
    n.push=n;
    n.loaded=!0;n.version='2.0';n.queue=[];t=b.createElement(e);
    t.async=!0;t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window,document,'script','https://connect.facebook.net/en_US/fbevents.js');
    fbq('set', 'autoConfig', 'false', '1086177924859222') // disable auto tracking of EVERYTHING
    fbq('init', '1086177924859222'); // Insert your pixel ID here.
 }
 fbq('track', 'PageView');</script><noscript><img height="1" width="1" style="display:none;" src="https://www.facebook.com/tr?id=1086177924859222&amp;ev=PageView&amp;noscript=1"/></noscript><script type="text/javascript">/* <![CDATA[ */
                        var google_conversion_id = 1017027555;
                        var google_custom_params = window.google_tag_params;
                        var google_remarketing_only = true;
                        /* ]]> */</script><script type="text/javascript" src="conversion.js"></script><noscript><div style="display:inline;"><img height="1" width="1" style="border_style:none;" alt="" src="//googleads.g.doubleclick.net/pagead/viewthroughconversion/1017027555/?guid=ON&amp;script=0"/></div></noscript></div></div>
      </footer>
      <script src="client.js"></script>
      <script src="satelliteLib-b7cfe8df39a4e5eec5536bba80e13f4b6fa0dd7c.js"></script><script src="satellite-565e008964746d4385002642.js"></script>
      <script src="arp.js" async=""></script>
      <script>
        window.pageData = {"content":[{"id":"sd:article:pii:s0167865514000221","entitlementType":"subscription","format":"MIME-XHTML","type":"SD:ARTICLE:SCOPE-FULL","detail":"SD:ARTICLE:subtype:sco"}],"page":{"businessUnit":"ELS:RP:ST","language":"en","name":"product:journal:article","noTracking":"false","productName":"SD","type":"CP-CA","environment":"prod","loadTimestamp":1510592422957,"loadTime":""},"visitor":{"accessType":"ae:ANON_SHIBBOLETH","accountId":"ae:10338","accountName":"ae:NESLI University of York - UK","loginStatus":"anonymous","userId":"ae:7103064","ipAddress":"86.138.139.221"}};
      </script>
      <!-- begin usabilla live embed code -->
      <script type="text/javascript">
        window.lightningjs||function(c){function g(b,d){d&&(d+=(/\?/.test(d)?"&":"?")+"lv=1");c[b]||function(){var i=window,h=document,j=b,g=h.location.protocol,l="load",k=0;(function(){function b(){a.P(l);a.w=1;c[j]("_load")}c[j]=function(){function m(){m.id=e;return c[j].apply(m,arguments)}var b,e=++k;b=this&&this!=i?this.id||0:0;(a.s=a.s||[]).push([e,b,arguments]);m.then=function(b,c,h){var d=a.fh[e]=a.fh[e]||[],j=a.eh[e]=a.eh[e]||[],f=a.ph[e]=a.ph[e]||[];b&&d.push(b);c&&j.push(c);h&&f.push(h);return m};return m};var a=c[j]._={};a.fh={};a.eh={};a.ph={};a.l=d?d.replace(/^\/\//,(g=="https:"?g:"http:")+"//"):d;a.p={0:+new Date};a.P=function(b){a.p[b]=new Date-a.p[0]};a.w&&b();i.addEventListener?i.addEventListener(l,b,!1):i.attachEvent("on"+l,b);var q=function(){function b(){return["<head></head><",c,' onload="var d=',n,";d.getElementsByTagName('head')[0].",d,"(d.",g,"('script')).",i,"='",a.l,"'\"></",c,">"].join("")}var c="body",e=h[c];if(!e)return setTimeout(q,100);a.P(1);var d="appendChild",g="createElement",i="src",k=h[g]("div"),l=k[d](h[g]("div")),f=h[g]("iframe"),n="document",p;k.style.display="none";e.insertBefore(k,e.firstChild).id=o+"-"+j;f.frameBorder="0";f.id=o+"-frame-"+j;/MSIE[ ]+6/.test(navigator.userAgent)&&(f[i]="javascript:false");f.allowTransparency="true";l[d](f);try{f.contentWindow[n].open()}catch(s){a.domain=h.domain,p="javascript:var d="+n+".open();d.domain='"+h.domain+"';",f[i]=p+"void(0);"}try{var r=f.contentWindow[n];r.write(b());r.close()}catch(t){f[i]=p+'d.write("'+b().replace(/"/g,String.fromCharCode(92)+'"')+'");d.close();'}a.P(2)};a.l&&setTimeout(q,0)})()}();c[b].lv="1";return c[b]}var o="lightningjs",k=window[o]=g(o);k.require=g;k.modules=c}({});
        window.usabilla_live = lightningjs.require("usabilla_live", "https://w.usabilla.com/eb1c14a91932.js");
        var customData = {};

        if(window.pageData && pageData.content && pageData.content[0]) {
          customData.entitlementType = pageData.content[0].entitlementType;
        }
        if(window.pageData && pageData.visitor) {
          customData.accessType = pageData.visitor.accessType;
          customData.accountId = pageData.visitor.accountId;
          customData.loginStatus = pageData.visitor.loginStatus;
        }
        usabilla_live("data", {"custom": customData });
      </script>
      <!-- end usabilla live embed code -->
      <script type="text/x-mathjax-config;executed=true">
        MathJax.Hub.Config({
          displayAlign: 'left',
          "fast-preview": {
            disabled: true
          },
          CommonHTML: { linebreaks: { automatic: true } },
          PreviewHTML: { linebreaks: { automatic: true } },
          'HTML-CSS': { linebreaks: { automatic: true } },
          SVG: {
            scale: 90,
            linebreaks: { automatic: true }
          }
        });
      </script>
      <script async="" src="MathJax.js"></script>
      <script async="" src="gpt.js"></script>
    
  <div class="adsbox">&nbsp;</div><div class="ReactModalPortal"><!-- react-empty: 1 --></div><div class="ReactModalPortal"><!-- react-empty: 1 --></div><div class="ReactModalPortal"><!-- react-empty: 1 --></div><div class="ReactModalPortal"><!-- react-empty: 1 --></div></body></html>