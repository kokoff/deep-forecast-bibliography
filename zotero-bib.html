<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><title>Bibliography</title><link rel="stylesheet" type="text/css" href="site/style.css"><style type="text/css" id="zoterostylesheet" >
.bibshowhide {display:none;}
.bib-venue-short, .bib-venue {display:none;}.bib-venue-short::before, .bib-venue::before, .blink a::before  {content: attr(data-before);}.blink {margin:0;margin-right:15px;padding:0;display:none;}</style></head><body><base target="_blank"><div class="bibliography"><script type="text/javascript" src="site/jquery.min.js"></script><script type="text/javascript">jQuery(document).ready(function () {jQuery('.blink a').click(showThis);
        jQuery(".bib-venue-short").each(function(){$(this).attr('data-before', $(this).html()); $(this).html("")});
        jQuery(".blink a").each(function(){$(this).attr('data-before', $(this).html()); $(this).html("")});
        });
function dwnD(data) {
    filename = "article.ris"
    var pom = document.createElement('a');
    var isSafari = navigator.vendor && navigator.vendor.indexOf('Apple') > -1 && navigator.userAgent && !navigator.userAgent.match('CriOS');
    var mime = (isSafari?"text/plain":"application/x-research-info-systems");
    pom.href = window.URL.createObjectURL(new Blob([atob(data)], {type: mime+";charset=utf-8"}));
    pom.download = filename;
    document.body.appendChild(pom);
    pom.click();
    setTimeout(function(){document.body.removeChild(pom);}, 100);
    return(void(0));}
function showThis(e) {
    elem = e.target;
    if (elem.parentNode) {
    var elems = elem.parentNode.parentNode.getElementsByTagName('*');
    for (i in elems) {
            if((' ' + elems[i].className + ' ').indexOf(' ' + 'bibshowhide' + ' ') > -1)
            { if (elems[i].parentNode != elem.parentNode)
        elems[i].style.display = 'none';
            }}
    elems = elem.parentNode.getElementsByTagName('*');
    for (i in elems) {
            if((' ' + elems[i].className + ' ').indexOf(' ' + 'bibshowhide' + ' ') > -1)
        { elems[i].style.display = 'block';
          hideagain = elems[i];
              e.stopPropagation();
          turnoff = function(e){
          if (! jQuery.contains(this, e.target))
              this.style.display = 'none';
          else
              jQuery(document).one("click",turnoff_b); // rebind itself
          }
          turnoff_b = turnoff.bind(elems[i])
          jQuery(document).one("click",turnoff_b);
              return(void(0));
            }}}
    return(void(0));}
function changeCSS() {
    if (!document.styleSheets) return;
    var theRules = new Array();
    //ss = document.styleSheets[document.styleSheets.length-1];
    var ss = document.getElementById('zoterostylesheet');
    if (ss) {
    ss = ss.sheet
    if (ss.cssRules)
            theRules = ss.cssRules
    else if (ss.rules)
            theRules = ss.rules
    else return;
    theRules[theRules.length-1].style.display = 'inline';
    }
}
changeCSS();
</script><h1 class="title">Bibliography</h1>
<div id="bib-preamble" style="visibility:hidden;"><ul id="bib-cat-collection" class="bib-cat"><li class='link'><a style='white-space: nowrap;' href='#' onclick='searchF(["XG9M3S5V"],"bibliography",1);return false;'>bibliography<span class='cat_count'> (77)</span></a></li>
</ul><ul id="bib-cat-year" class="bib-cat"><li class='link'><a style='white-space: nowrap;' href='#' onclick='searchF(["year__2015","year__2014","year__2017","year__2016","year__2013","year__2012"],"2012&ndash;2017",1);return false;'>2012&ndash;2017<span class='cat_count'> (23)</span></a></li>
<li class='link'><a style='white-space: nowrap;' href='#' onclick='searchF(["year__2006","year__2008","year__2011","year__2010","year__2007"],"2006&ndash;2011",1);return false;'>2006&ndash;2011<span class='cat_count'> (15)</span></a></li>
<li class='link'><a style='white-space: nowrap;' href='#' onclick='searchF(["year__2002","year__2003","year__2001","year__2004","year__2005"],"2000&ndash;2005",1);return false;'>2000&ndash;2005<span class='cat_count'> (6)</span></a></li>
<li class='link'><a style='white-space: nowrap;' href='#' onclick='searchF(["year__1987","year__1993","year__1992","year__1995","year__1994","year__1997","year__1996","year__1998","year__1975","year__1960","year__1988","year__1989","year__1985","year__1943","year__1979","year__1982","year__1945","year__1958"],"&ndash;1999",1);return false;'>&ndash;1999<span class='cat_count'> (27)</span></a></li>
</ul><ul id="bib-cat-keyword" class="bib-cat"><li class='link'><a style='white-space: nowrap;' href='#' onclick='searchF(["neural networks"],"neural networks",1);return false;'>neural networks<span class='cat_count'> (1)</span></a></li>
<li class='link'><a style='white-space: nowrap;' href='#' onclick='searchF(["time series"],"time series",1);return false;'>time series<span class='cat_count'> (1)</span></a></li>
<li class='link'><a style='white-space: nowrap;' href='#' onclick='searchF(["macoeconomic"],"macoeconomic",1);return false;'>macoeconomic<span class='cat_count'> (1)</span></a></li>
</ul><form id="pubSearchBox" name="pubSearchBox" style="visibility:hidden;"><input id="pubSearchInputBox" type="text" name="keyword" placeholder="keywords">&nbsp;<input id="pubSearchButton" type="button" value="Search" onClick="searchF()"></form><h2 id="searchTermSectionTitle" class="collectiontitle"></h2><script type="text/javascript">
  function getURLParameter(name) {
    return decodeURIComponent((new RegExp('[?|&]' + name + '=' + '([^&;]+?)(&|#|;|$)').exec(location.search)||[,""])[1].replace(/\+/g, '%20'))||null;
  }
  jQuery( document ).ready(function() {
    jQuery('#pubSearchBox,#bib-preamble').css("visibility","visible");
    var kw = getURLParameter("keyword");
    if (kw) {
        jQuery('#pubSearchInputBox').val(kw);
        searchF([kw]);
    }
  });
  jQuery.expr[":"].icontains = jQuery.expr.createPseudo(function(arg) {
    return function( elem ) {
        return jQuery(elem).text().toUpperCase().indexOf(arg.toUpperCase()) >= 0;
    };});
function searchF(searchTerms, shown, disjunctive) {
  var i=document.pubSearchBox.keyword.value;
  searchTerms = searchTerms || (i!=""&&i.split(" "));
  shown = shown || searchTerms;
  jQuery(".bib-item").css("display", "none");
  var q = ".bib-item";
  if (disjunctive)
  { for (x in searchTerms) {jQuery(".bib-item:icontains('"+searchTerms[x]+"')").css("display", "block");}
  }
  else
  { jQuery.each(searchTerms, function(i,x) {q = q + ":icontains('"+x+"')";});
    jQuery(q).css("display", "block");}
  jQuery("#searchTermSectionTitle").html(searchTerms.length>0?"<a href='#' onclick='searchF([]);'>&#x2715;</a> "+shown:"");
  jQuery(".collectiontitle").parent(".full-bib-section,.short-bib-section").css("display", "block");
  jQuery(".collectiontitle").parent(".full-bib-section,.short-bib-section").each(function(){
    var y = jQuery(this).find(".bib-item:visible");
    if (y.length==0) {jQuery(this).css("display","none");}
  });
}
  jQuery(function() {    // <== Doc ready
  // stackoverflow q 3971524
    var inputVal = jQuery("#pubSearchInputBox").val(),
        timer,
        checkForChange = function() {
            var self = this; // or just use .bind(this)
            if (timer) { clearTimeout(timer); }
            // check for change of the text field after each key up
            timer = setTimeout(function() {
                if(self.value != inputVal) {
                    searchF();
                    inputVal = self.value
                }
            }, 250);
        };
    jQuery("#pubSearchInputBox").bind('keyup paste cut', checkForChange);
});</script></div><div class="short-bib-section"><a id='XG9M3S5V' style='{display: block; position: relative; top: -150px; visibility: hidden;}'></a><h2 class="collectiontitle">bibliography</h2>
<ol><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">forecast: Forecasting functions for time series and linear models</span> <span class="containertitle"></span> (2018) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. Hyndman <i>et al.</i>, <span class="doctitle"><a class="doctitle" href="http://pkg.robjhyndman.com/forecast"><i>forecast: Forecasting functions for time series and linear models</i>.</a></span> 2018.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2018 type__book ""</span></div><div class="blinkitems"><div><div class="blink"><a   href="http://pkg.robjhyndman.com/forecast" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@book{hyndman2018forecast:,
	title = {forecast: {Forecasting} functions for time series and linear models},
	url = {http://pkg.robjhyndman.com/forecast},
	author = {Hyndman, Rob and Bergmeir, Christoph and Caceres, Gabriel and Chhay, Leanne and O'Hara-Wild, Mitchell and Petropoulos, Fotios and Razbash, Slava and Wang, Earo and Yasmeen, Farah},
	year = {2018},
	note = {00000}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQk9PSw0KVEkgIC0gZm9yZWNhc3Q6IEZvcmVjYXN0aW5nIGZ1bmN0aW9ucyBmb3IgdGltZSBzZXJpZXMgYW5kIGxpbmVhciBtb2RlbHMNCkFVICAtIEh5bmRtYW4sIFJvYg0KQVUgIC0gQmVyZ21laXIsIENocmlzdG9waA0KQVUgIC0gQ2FjZXJlcywgR2FicmllbA0KQVUgIC0gQ2hoYXksIExlYW5uZQ0KQVUgIC0gTydIYXJhLVdpbGQsIE1pdGNoZWxsDQpBVSAgLSBQZXRyb3BvdWxvcywgRm90aW9zDQpBVSAgLSBSYXpiYXNoLCBTbGF2YQ0KQVUgIC0gV2FuZywgRWFybw0KQVUgIC0gWWFzbWVlbiwgRmFyYWgNCkRBICAtIDIwMTgvLy8NClBZICAtIDIwMTgNClVSICAtIGh0dHA6Ly9wa2cucm9iamh5bmRtYW4uY29tL2ZvcmVjYXN0DQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. Hyndman <i>et al.</i>, <i>forecast: Forecasting functions for time series and linear models</i>. 2018.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=forecast%3A%20Forecasting%20functions%20for%20time%20series%20and%20linear%20models&amp;rft.aufirst=Rob&amp;rft.aulast=Hyndman&amp;rft.au=Rob%20Hyndman&amp;rft.au=Christoph%20Bergmeir&amp;rft.au=Gabriel%20Caceres&amp;rft.au=Leanne%20Chhay&amp;rft.au=Mitchell%20O&apos;Hara-Wild&amp;rft.au=Fotios%20Petropoulos&amp;rft.au=Slava%20Razbash&amp;rft.au=Earo%20Wang&amp;rft.au=Farah%20Yasmeen&amp;rft.date=2018"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Population Based Training of Neural Networks</span> <span class="containertitle">arXiv:1711.09846 [cs]</span> (November 2017) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">M. Jaderberg <i>et al.</i>, “<span class="doctitle"><a class="doctitle" href="http://arxiv.org/abs/1711.09846">Population Based Training of Neural Networks</a></span>,” <i>arXiv:1711.09846 [cs]</i>, Nov. 2017.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2017 type__journalArticle ""</span></div><div class="bib-venue">arXiv:1711.09846 [cs]</div><div class="blinkitems"><div><div class="blink"><a   href="http://arxiv.org/abs/1711.09846" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{jaderberg2017population,
	title = {Population {Based} {Training} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1711.09846},
	journal = {arXiv:1711.09846 [cs]},
	author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
	month = nov,
	year = {2017}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gUG9wdWxhdGlvbiBCYXNlZCBUcmFpbmluZyBvZiBOZXVyYWwgTmV0d29ya3MNCkFVICAtIEphZGVyYmVyZywgTWF4DQpBVSAgLSBEYWxpYmFyZCwgVmFsZW50aW4NCkFVICAtIE9zaW5kZXJvLCBTaW1vbg0KQVUgIC0gQ3phcm5lY2tpLCBXb2pjaWVjaCBNLg0KQVUgIC0gRG9uYWh1ZSwgSmVmZg0KQVUgIC0gUmF6YXZpLCBBbGkNCkFVICAtIFZpbnlhbHMsIE9yaW9sDQpBVSAgLSBHcmVlbiwgVGltDQpBVSAgLSBEdW5uaW5nLCBJYWluDQpBVSAgLSBTaW1vbnlhbiwgS2FyZW4NCkFVICAtIEZlcm5hbmRvLCBDaHJpc2FudGhhDQpBVSAgLSBLYXZ1a2N1b2dsdSwgS29yYXkNClQyICAtIGFyWGl2OjE3MTEuMDk4NDYgW2NzXQ0KREEgIC0gMjAxNy8xMS8vDQpQWSAgLSAyMDE3DQpVUiAgLSBodHRwOi8vYXJ4aXYub3JnL2Ficy8xNzExLjA5ODQ2DQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">M. Jaderberg <i>et al.</i>, “Population Based Training of Neural Networks,” <i>arXiv:1711.09846 [cs]</i>, Nov. 2017.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Population%20Based%20Training%20of%20Neural%20Networks&amp;rft.jtitle=arXiv%3A1711.09846%20%5Bcs%5D&amp;rft.aufirst=Max&amp;rft.aulast=Jaderberg&amp;rft.au=Max%20Jaderberg&amp;rft.au=Valentin%20Dalibard&amp;rft.au=Simon%20Osindero&amp;rft.au=Wojciech%20M.%20Czarnecki&amp;rft.au=Jeff%20Donahue&amp;rft.au=Ali%20Razavi&amp;rft.au=Oriol%20Vinyals&amp;rft.au=Tim%20Green&amp;rft.au=Iain%20Dunning&amp;rft.au=Karen%20Simonyan&amp;rft.au=Chrisantha%20Fernando&amp;rft.au=Koray%20Kavukcuoglu&amp;rft.date=2017-11"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Macroeconomic Indicator Forecasting with Deep Neural Networks</span> <span class="containertitle"></span> (September 2017) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">A. Smalter Hall and T. R. Cook, “<span class="doctitle"><a class="doctitle" href="https://papers.ssrn.com/abstract=3046657">Macroeconomic Indicator Forecasting with Deep Neural Networks</a></span>,” Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 3046657, Sep. 2017.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2017 type__report ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Economic policymaking relies upon accurate forecasts of economic conditions. Current methods for unconditional forecasting are dominated by inherently linear models that exhibit model dependence and have high data demands. We explore deep neural networks as an opportunity to improve upon forecast accuracy with limited data and while remaining agnostic as to functional form. We focus on predicting civilian unemployment using models based on four different neural network architectures. Each of these models outperforms benchmark models at short time horizons. One model‚ based on an Encoder Decoder architecture outperforms benchmark models at every forecast horizon (up to four quarters).</div></div></div><div class="blink"><a   href="https://papers.ssrn.com/abstract=3046657" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@techreport{smalter_hall_macroeconomic_2017,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Macroeconomic {Indicator} {Forecasting} with {Deep} {Neural} {Networks}},
	url = {https://papers.ssrn.com/abstract=3046657},
	language = {en},
	number = {ID 3046657},
	institution = {Social Science Research Network},
	author = {Smalter Hall, Aaron and Cook, Thomas R.},
	month = sep,
	year = {2017}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gUlBSVA0KVEkgIC0gTWFjcm9lY29ub21pYyBJbmRpY2F0b3IgRm9yZWNhc3Rpbmcgd2l0aCBEZWVwIE5ldXJhbCBOZXR3b3Jrcw0KQVUgIC0gU21hbHRlciBIYWxsLCBBYXJvbg0KQVUgIC0gQ29vaywgVGhvbWFzIFIuDQpBQiAgLSBFY29ub21pYyBwb2xpY3ltYWtpbmcgcmVsaWVzIHVwb24gYWNjdXJhdGUgZm9yZWNhc3RzIG9mIGVjb25vbWljIGNvbmRpdGlvbnMuIEN1cnJlbnQgbWV0aG9kcyBmb3IgdW5jb25kaXRpb25hbCBmb3JlY2FzdGluZyBhcmUgZG9taW5hdGVkIGJ5IGluaGVyZW50bHkgbGluZWFyIG1vZGVscyB0aGF0IGV4aGliaXQgbW9kZWwgZGVwZW5kZW5jZSBhbmQgaGF2ZSBoaWdoIGRhdGEgZGVtYW5kcy4gV2UgZXhwbG9yZSBkZWVwIG5ldXJhbCBuZXR3b3JrcyBhcyBhbiBvcHBvcnR1bml0eSB0byBpbXByb3ZlIHVwb24gZm9yZWNhc3QgYWNjdXJhY3kgd2l0aCBsaW1pdGVkIGRhdGEgYW5kIHdoaWxlIHJlbWFpbmluZyBhZ25vc3RpYyBhcyB0byBmdW5jdGlvbmFsIGZvcm0uIFdlIGZvY3VzIG9uIHByZWRpY3RpbmcgY2l2aWxpYW4gdW5lbXBsb3ltZW50IHVzaW5nIG1vZGVscyBiYXNlZCBvbiBmb3VyIGRpZmZlcmVudCBuZXVyYWwgbmV0d29yayBhcmNoaXRlY3R1cmVzLiBFYWNoIG9mIHRoZXNlIG1vZGVscyBvdXRwZXJmb3JtcyBiZW5jaG1hcmsgbW9kZWxzIGF0IHNob3J0IHRpbWUgaG9yaXpvbnMuIE9uZSBtb2RlbCwgYmFzZWQgb24gYW4gRW5jb2RlciBEZWNvZGVyIGFyY2hpdGVjdHVyZSBvdXRwZXJmb3JtcyBiZW5jaG1hcmsgbW9kZWxzIGF0IGV2ZXJ5IGZvcmVjYXN0IGhvcml6b24gKHVwIHRvIGZvdXIgcXVhcnRlcnMpLg0KQ1kgIC0gUm9jaGVzdGVyLCBOWQ0KREEgIC0gMjAxNy8wOS8vDQpQWSAgLSAyMDE3DQpMQSAgLSBlbg0KTTMgIC0gU1NSTiBTY2hvbGFybHkgUGFwZXINClBCICAtIFNvY2lhbCBTY2llbmNlIFJlc2VhcmNoIE5ldHdvcmsNClNOICAtIElEIDMwNDY2NTcNClVSICAtIGh0dHBzOi8vcGFwZXJzLnNzcm4uY29tL2Fic3RyYWN0PTMwNDY2NTcNCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">A. Smalter Hall and T. R. Cook, “Macroeconomic Indicator Forecasting with Deep Neural Networks,” Social Science Research Network, Rochester, NY, SSRN Scholarly Paper ID 3046657, Sep. 2017.</div>
  </div></div></div></div></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Neural Decomposition of Time-Series Data for Effective Generalization</span> <span class="containertitle">IEEE Transactions on Neural Networks and Learning Systems</span> (2017) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">L. B. Godfrey and M. S. Gashler, “<span class="doctitle"><a class="doctitle" href="http://arxiv.org/abs/1705.09137">Neural Decomposition of Time-Series Data for Effective Generalization</a></span>,” <i>IEEE Transactions on Neural Networks and Learning Systems</i>, pp. 1–13, 2017.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2017 type__journalArticle ""</span></div><div class="bib-venue">IEEE Transactions on Neural Networks and Learning Systems</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">We present a neural network technique for the analysis and extrapolation of time-series data called Neural Decomposition (ND). Units with a sinusoidal activation function are used to perform a Fourier-like decomposition of training samples into a sum of sinusoids‚ augmented by units with nonperiodic activation functions to capture linear trends and other nonperiodic components. We show how careful weight initialization can be combined with regularization to form a simple model that generalizes well. Our method generalizes effectively on the Mackey-Glass series‚ a dataset of unemployment rates as reported by the U.S. Department of Labor Statistics‚ a time-series of monthly international airline passengers‚ the monthly ozone concentration in downtown Los Angeles‚ and an unevenly sampled time-series of oxygen isotope measurements from a cave in north India. We find that ND outperforms popular time-series forecasting techniques including LSTM‚ echo state networks‚ ARIMA‚ SARIMA‚ SVR with a radial basis function‚ and Gashler and Ashmore’s model.</div></div></div><div class="blink"><a   href="http://arxiv.org/abs/1705.09137" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{godfrey2017neural,
	title = {Neural {Decomposition} of {Time}-{Series} {Data} for {Effective} {Generalization}},
	issn = {2162-237X, 2162-2388},
	url = {http://arxiv.org/abs/1705.09137},
	doi = {10.1109/TNNLS.2017.2709324},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Godfrey, Luke B. and Gashler, Michael S.},
	year = {2017},
	note = {00000},
	pages = {1--13}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gTmV1cmFsIERlY29tcG9zaXRpb24gb2YgVGltZS1TZXJpZXMgRGF0YSBmb3IgRWZmZWN0aXZlIEdlbmVyYWxpemF0aW9uDQpBVSAgLSBHb2RmcmV5LCBMdWtlIEIuDQpBVSAgLSBHYXNobGVyLCBNaWNoYWVsIFMuDQpUMiAgLSBJRUVFIFRyYW5zYWN0aW9ucyBvbiBOZXVyYWwgTmV0d29ya3MgYW5kIExlYXJuaW5nIFN5c3RlbXMNCkFCICAtIFdlIHByZXNlbnQgYSBuZXVyYWwgbmV0d29yayB0ZWNobmlxdWUgZm9yIHRoZSBhbmFseXNpcyBhbmQgZXh0cmFwb2xhdGlvbiBvZiB0aW1lLXNlcmllcyBkYXRhIGNhbGxlZCBOZXVyYWwgRGVjb21wb3NpdGlvbiAoTkQpLiBVbml0cyB3aXRoIGEgc2ludXNvaWRhbCBhY3RpdmF0aW9uIGZ1bmN0aW9uIGFyZSB1c2VkIHRvIHBlcmZvcm0gYSBGb3VyaWVyLWxpa2UgZGVjb21wb3NpdGlvbiBvZiB0cmFpbmluZyBzYW1wbGVzIGludG8gYSBzdW0gb2Ygc2ludXNvaWRzLCBhdWdtZW50ZWQgYnkgdW5pdHMgd2l0aCBub25wZXJpb2RpYyBhY3RpdmF0aW9uIGZ1bmN0aW9ucyB0byBjYXB0dXJlIGxpbmVhciB0cmVuZHMgYW5kIG90aGVyIG5vbnBlcmlvZGljIGNvbXBvbmVudHMuIFdlIHNob3cgaG93IGNhcmVmdWwgd2VpZ2h0IGluaXRpYWxpemF0aW9uIGNhbiBiZSBjb21iaW5lZCB3aXRoIHJlZ3VsYXJpemF0aW9uIHRvIGZvcm0gYSBzaW1wbGUgbW9kZWwgdGhhdCBnZW5lcmFsaXplcyB3ZWxsLiBPdXIgbWV0aG9kIGdlbmVyYWxpemVzIGVmZmVjdGl2ZWx5IG9uIHRoZSBNYWNrZXktR2xhc3Mgc2VyaWVzLCBhIGRhdGFzZXQgb2YgdW5lbXBsb3ltZW50IHJhdGVzIGFzIHJlcG9ydGVkIGJ5IHRoZSBVLlMuIERlcGFydG1lbnQgb2YgTGFib3IgU3RhdGlzdGljcywgYSB0aW1lLXNlcmllcyBvZiBtb250aGx5IGludGVybmF0aW9uYWwgYWlybGluZSBwYXNzZW5nZXJzLCB0aGUgbW9udGhseSBvem9uZSBjb25jZW50cmF0aW9uIGluIGRvd250b3duIExvcyBBbmdlbGVzLCBhbmQgYW4gdW5ldmVubHkgc2FtcGxlZCB0aW1lLXNlcmllcyBvZiBveHlnZW4gaXNvdG9wZSBtZWFzdXJlbWVudHMgZnJvbSBhIGNhdmUgaW4gbm9ydGggSW5kaWEuIFdlIGZpbmQgdGhhdCBORCBvdXRwZXJmb3JtcyBwb3B1bGFyIHRpbWUtc2VyaWVzIGZvcmVjYXN0aW5nIHRlY2huaXF1ZXMgaW5jbHVkaW5nIExTVE0sIGVjaG8gc3RhdGUgbmV0d29ya3MsIEFSSU1BLCBTQVJJTUEsIFNWUiB3aXRoIGEgcmFkaWFsIGJhc2lzIGZ1bmN0aW9uLCBhbmQgR2FzaGxlciBhbmQgQXNobW9yZSdzIG1vZGVsLg0KREEgIC0gMjAxNy8vLw0KUFkgIC0gMjAxNw0KRE8gIC0gMTAuMTEwOS9UTk5MUy4yMDE3LjI3MDkzMjQNClNQICAtIDENCkVQICAtIDEzDQpTTiAgLSAyMTYyLTIzN1gsIDIxNjItMjM4OA0KVVIgIC0gaHR0cDovL2FyeGl2Lm9yZy9hYnMvMTcwNS4wOTEzNw0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">L. B. Godfrey and M. S. Gashler, “Neural Decomposition of Time-Series Data for Effective Generalization,” <i>IEEE Transactions on Neural Networks and Learning Systems</i>, pp. 1–13, 2017.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FTNNLS.2017.2709324&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Neural%20Decomposition%20of%20Time-Series%20Data%20for%20Effective%20Generalization&amp;rft.jtitle=IEEE%20Transactions%20on%20Neural%20Networks%20and%20Learning%20Systems&amp;rft.aufirst=Luke%20B.&amp;rft.aulast=Godfrey&amp;rft.au=Luke%20B.%20Godfrey&amp;rft.au=Michael%20S.%20Gashler&amp;rft.date=2017&amp;rft.pages=1%E2%80%9313&amp;rft.issn=2162-237X%2C%202162-2388"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Particle Swarm Optimization for Hyper-parameter Selection in Deep Neural Networks</span> <span class="containertitle"></span> (2017) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">P. R. Lorenzo, J. Nalepa, M. Kawulok, L. S. Ramos, and J. R. Pastor, “<span class="doctitle"><a class="doctitle" href="http://doi.acm.org/10.1145/3071178.3071208">Particle Swarm Optimization for Hyper-parameter Selection in Deep Neural Networks</a></span>,” in <i>Proceedings of the Genetic and Evolutionary Computation Conference</i>, New York, NY, USA, 2017, pp. 481–488.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2017 venue_short__GECCO '17 type__conferencePaper ""</span></div><div class="bib-venue-short">GECCO '17</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Deep neural networks (DNNs) have achieved unprecedented success in a wide array of tasks. However‚ the performance of these systems depends directly on their hyper-parameters which often must be selected by an expert. Optimizing the hyper-parameters remains a substantial obstacle in designing DNNs in practice. In this work‚ we propose to select them using particle swarm optimization (PSO). Such biologically-inspired approaches have not been extensively exploited for this task. We demonstrate that PSO efficiently explores the solution space‚ allowing DNNs of a minimal topology to obtain competitive classification performance over the MNIST dataset. We showed that very small DNNs optimized by PSO retrieve promising classification accuracy for CIFAR-10. Also‚ PSO improves the performance of existing architectures. Extensive experimental study‚ backed-up with the statistical tests‚ revealed that PSO is an effective technique for automating hyper-parameter selection and efficiently exploits computational resources.</div></div></div><div class="blink"><a   href="http://doi.acm.org/10.1145/3071178.3071208" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@inproceedings{lorenzo2017particle,
	address = {New York, NY, USA},
	series = {{GECCO} '17},
	title = {Particle {Swarm} {Optimization} for {Hyper}-parameter {Selection} in {Deep} {Neural} {Networks}},
	isbn = {978-1-4503-4920-8},
	url = {http://doi.acm.org/10.1145/3071178.3071208},
	doi = {10.1145/3071178.3071208},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {ACM},
	author = {Lorenzo, Pablo Ribalta and Nalepa, Jakub and Kawulok, Michal and Ramos, Luciano Sanchez and Pastor, José Ranilla},
	year = {2017},
	note = {00000},
	pages = {481--488}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('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');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">P. R. Lorenzo, J. Nalepa, M. Kawulok, L. S. Ramos, and J. R. Pastor, “Particle Swarm Optimization for Hyper-parameter Selection in Deep Neural Networks,” in <i>Proceedings of the Genetic and Evolutionary Computation Conference</i>, New York, NY, USA, 2017, pp. 481–488.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1145%2F3071178.3071208&amp;rft_id=urn%3Aisbn%3A978-1-4503-4920-8&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Particle%20Swarm%20Optimization%20for%20Hyper-parameter%20Selection%20in%20Deep%20Neural%20Networks&amp;rft.btitle=Proceedings%20of%20the%20Genetic%20and%20Evolutionary%20Computation%20Conference&amp;rft.place=New%20York%2C%20NY%2C%20USA&amp;rft.publisher=ACM&amp;rft.series=GECCO%20&apos;17&amp;rft.aufirst=Pablo%20Ribalta&amp;rft.aulast=Lorenzo&amp;rft.au=Pablo%20Ribalta%20Lorenzo&amp;rft.au=Jakub%20Nalepa&amp;rft.au=Michal%20Kawulok&amp;rft.au=Luciano%20Sanchez%20Ramos&amp;rft.au=Jos%C3%A9%20Ranilla%20Pastor&amp;rft.date=2017&amp;rft.pages=481%E2%80%93488&amp;rft.isbn=978-1-4503-4920-8"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Hyper-parameter Selection in Deep Neural Networks Using Parallel Particle Swarm Optimization</span> <span class="containertitle"></span> (2017) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">P. R. Lorenzo, J. Nalepa, L. S. Ramos, and J. R. Pastor, “<span class="doctitle"><a class="doctitle" href="http://doi.acm.org/10.1145/3067695.3084211">Hyper-parameter Selection in Deep Neural Networks Using Parallel Particle Swarm Optimization</a></span>,” in <i>Proceedings of the Genetic and Evolutionary Computation Conference Companion</i>, New York, NY, USA, 2017, pp. 1864–1871.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2017 venue_short__GECCO '17 type__conferencePaper ""</span></div><div class="bib-venue-short">GECCO '17</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">The need of manual hyper-parameter selection can seriously hamper the model optimization of Deep Neural Networks (DNNs). Conventional automated approaches tackling this problem suffer from poor scalability or fail in certain scenarios. In this paper‚ we introduce a parallel method that applies Particle Swarm Optimization (PSO) for the hyper-parameter selection in DNNs. To estimate the best hyper-parameters‚ a population of particles is evolved‚ with their fitness calculated in parallel. The experimental results demonstrate very desirable scalability properties for different DNNs. We show that the parallel PSO can further optimize existent models designed by experts in an affordable amount of time.</div></div></div><div class="blink"><a   href="http://doi.acm.org/10.1145/3067695.3084211" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@inproceedings{lorenzo2017hyper-parameter,
	address = {New York, NY, USA},
	series = {{GECCO} '17},
	title = {Hyper-parameter {Selection} in {Deep} {Neural} {Networks} {Using} {Parallel} {Particle} {Swarm} {Optimization}},
	isbn = {978-1-4503-4939-0},
	url = {http://doi.acm.org/10.1145/3067695.3084211},
	doi = {10.1145/3067695.3084211},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference} {Companion}},
	publisher = {ACM},
	author = {Lorenzo, Pablo Ribalta and Nalepa, Jakub and Ramos, Luciano Sanchez and Pastor, José Ranilla},
	year = {2017},
	pages = {1864--1871}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ09ORg0KVEkgIC0gSHlwZXItcGFyYW1ldGVyIFNlbGVjdGlvbiBpbiBEZWVwIE5ldXJhbCBOZXR3b3JrcyBVc2luZyBQYXJhbGxlbCBQYXJ0aWNsZSBTd2FybSBPcHRpbWl6YXRpb24NCkFVICAtIExvcmVuem8sIFBhYmxvIFJpYmFsdGENCkFVICAtIE5hbGVwYSwgSmFrdWINCkFVICAtIFJhbW9zLCBMdWNpYW5vIFNhbmNoZXoNCkFVICAtIFBhc3RvciwgSm9zw6kgUmFuaWxsYQ0KVDMgIC0gR0VDQ08gJzE3DQpBQiAgLSBUaGUgbmVlZCBvZiBtYW51YWwgaHlwZXItcGFyYW1ldGVyIHNlbGVjdGlvbiBjYW4gc2VyaW91c2x5IGhhbXBlciB0aGUgbW9kZWwgb3B0aW1pemF0aW9uIG9mIERlZXAgTmV1cmFsIE5ldHdvcmtzIChETk5zKS4gQ29udmVudGlvbmFsIGF1dG9tYXRlZCBhcHByb2FjaGVzIHRhY2tsaW5nIHRoaXMgcHJvYmxlbSBzdWZmZXIgZnJvbSBwb29yIHNjYWxhYmlsaXR5IG9yIGZhaWwgaW4gY2VydGFpbiBzY2VuYXJpb3MuIEluIHRoaXMgcGFwZXIsIHdlIGludHJvZHVjZSBhIHBhcmFsbGVsIG1ldGhvZCB0aGF0IGFwcGxpZXMgUGFydGljbGUgU3dhcm0gT3B0aW1pemF0aW9uIChQU08pIGZvciB0aGUgaHlwZXItcGFyYW1ldGVyIHNlbGVjdGlvbiBpbiBETk5zLiBUbyBlc3RpbWF0ZSB0aGUgYmVzdCBoeXBlci1wYXJhbWV0ZXJzLCBhIHBvcHVsYXRpb24gb2YgcGFydGljbGVzIGlzIGV2b2x2ZWQsIHdpdGggdGhlaXIgZml0bmVzcyBjYWxjdWxhdGVkIGluIHBhcmFsbGVsLiBUaGUgZXhwZXJpbWVudGFsIHJlc3VsdHMgZGVtb25zdHJhdGUgdmVyeSBkZXNpcmFibGUgc2NhbGFiaWxpdHkgcHJvcGVydGllcyBmb3IgZGlmZmVyZW50IEROTnMuIFdlIHNob3cgdGhhdCB0aGUgcGFyYWxsZWwgUFNPIGNhbiBmdXJ0aGVyIG9wdGltaXplIGV4aXN0ZW50IG1vZGVscyBkZXNpZ25lZCBieSBleHBlcnRzIGluIGFuIGFmZm9yZGFibGUgYW1vdW50IG9mIHRpbWUuDQpDMSAgLSBOZXcgWW9yaywgTlksIFVTQQ0KQzMgIC0gUHJvY2VlZGluZ3Mgb2YgdGhlIEdlbmV0aWMgYW5kIEV2b2x1dGlvbmFyeSBDb21wdXRhdGlvbiBDb25mZXJlbmNlIENvbXBhbmlvbg0KREEgIC0gMjAxNy8vLw0KUFkgIC0gMjAxNw0KRE8gIC0gMTAuMTE0NS8zMDY3Njk1LjMwODQyMTENClNQICAtIDE4NjQNCkVQICAtIDE4NzENClBCICAtIEFDTQ0KU04gIC0gOTc4LTEtNDUwMy00OTM5LTANClVSICAtIGh0dHA6Ly9kb2kuYWNtLm9yZy8xMC4xMTQ1LzMwNjc2OTUuMzA4NDIxMQ0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">P. R. Lorenzo, J. Nalepa, L. S. Ramos, and J. R. Pastor, “Hyper-parameter Selection in Deep Neural Networks Using Parallel Particle Swarm Optimization,” in <i>Proceedings of the Genetic and Evolutionary Computation Conference Companion</i>, New York, NY, USA, 2017, pp. 1864–1871.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1145%2F3067695.3084211&amp;rft_id=urn%3Aisbn%3A978-1-4503-4939-0&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Hyper-parameter%20Selection%20in%20Deep%20Neural%20Networks%20Using%20Parallel%20Particle%20Swarm%20Optimization&amp;rft.btitle=Proceedings%20of%20the%20Genetic%20and%20Evolutionary%20Computation%20Conference%20Companion&amp;rft.place=New%20York%2C%20NY%2C%20USA&amp;rft.publisher=ACM&amp;rft.series=GECCO%20&apos;17&amp;rft.aufirst=Pablo%20Ribalta&amp;rft.aulast=Lorenzo&amp;rft.au=Pablo%20Ribalta%20Lorenzo&amp;rft.au=Jakub%20Nalepa&amp;rft.au=Luciano%20Sanchez%20Ramos&amp;rft.au=Jos%C3%A9%20Ranilla%20Pastor&amp;rft.date=2017&amp;rft.pages=1864%E2%80%931871&amp;rft.isbn=978-1-4503-4939-0"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Evolving Deep Neural Networks</span> <span class="containertitle">arXiv:1703.00548 [cs]</span> (March 2017) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. Miikkulainen <i>et al.</i>, “<span class="doctitle"><a class="doctitle" href="http://arxiv.org/abs/1703.00548">Evolving Deep Neural Networks</a></span>,” <i>arXiv:1703.00548 [cs]</i>, Mar. 2017.</div>
  </div><div class="bib-extra">00058</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2017 type__journalArticle ""</span></div><div class="bib-venue">arXiv:1703.00548 [cs]</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">The success of deep learning depends on finding an architecture to fit the task. As deep learning has scaled up to more challenging tasks‚ the architectures have become difficult to design by hand. This paper proposes an automated method‚ CoDeepNEAT‚ for optimizing deep learning architectures through evolution. By extending existing neuroevolution methods to topology‚ components‚ and hyperparameters‚ this method achieves results comparable to best human designs in standard benchmarks in object recognition and language modeling. It also supports building a real-world application of automated image captioning on a magazine website. Given the anticipated increases in available computing power‚ evolution of deep networks is promising approach to constructing deep learning applications in the future.</div></div></div><div class="blink"><a   href="http://arxiv.org/abs/1703.00548" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{miikkulainen2017evolving,
	title = {Evolving {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1703.00548},
	journal = {arXiv:1703.00548 [cs]},
	author = {Miikkulainen, Risto and Liang, Jason and Meyerson, Elliot and Rawal, Aditya and Fink, Dan and Francon, Olivier and Raju, Bala and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel and Hodjat, Babak},
	month = mar,
	year = {2017},
	note = {00058}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gRXZvbHZpbmcgRGVlcCBOZXVyYWwgTmV0d29ya3MNCkFVICAtIE1paWtrdWxhaW5lbiwgUmlzdG8NCkFVICAtIExpYW5nLCBKYXNvbg0KQVUgIC0gTWV5ZXJzb24sIEVsbGlvdA0KQVUgIC0gUmF3YWwsIEFkaXR5YQ0KQVUgIC0gRmluaywgRGFuDQpBVSAgLSBGcmFuY29uLCBPbGl2aWVyDQpBVSAgLSBSYWp1LCBCYWxhDQpBVSAgLSBTaGFocnphZCwgSG9ybW96DQpBVSAgLSBOYXZydXp5YW4sIEFyc2hhaw0KQVUgIC0gRHVmZnksIE5pZ2VsDQpBVSAgLSBIb2RqYXQsIEJhYmFrDQpUMiAgLSBhclhpdjoxNzAzLjAwNTQ4IFtjc10NCkFCICAtIFRoZSBzdWNjZXNzIG9mIGRlZXAgbGVhcm5pbmcgZGVwZW5kcyBvbiBmaW5kaW5nIGFuIGFyY2hpdGVjdHVyZSB0byBmaXQgdGhlIHRhc2suIEFzIGRlZXAgbGVhcm5pbmcgaGFzIHNjYWxlZCB1cCB0byBtb3JlIGNoYWxsZW5naW5nIHRhc2tzLCB0aGUgYXJjaGl0ZWN0dXJlcyBoYXZlIGJlY29tZSBkaWZmaWN1bHQgdG8gZGVzaWduIGJ5IGhhbmQuIFRoaXMgcGFwZXIgcHJvcG9zZXMgYW4gYXV0b21hdGVkIG1ldGhvZCwgQ29EZWVwTkVBVCwgZm9yIG9wdGltaXppbmcgZGVlcCBsZWFybmluZyBhcmNoaXRlY3R1cmVzIHRocm91Z2ggZXZvbHV0aW9uLiBCeSBleHRlbmRpbmcgZXhpc3RpbmcgbmV1cm9ldm9sdXRpb24gbWV0aG9kcyB0byB0b3BvbG9neSwgY29tcG9uZW50cywgYW5kIGh5cGVycGFyYW1ldGVycywgdGhpcyBtZXRob2QgYWNoaWV2ZXMgcmVzdWx0cyBjb21wYXJhYmxlIHRvIGJlc3QgaHVtYW4gZGVzaWducyBpbiBzdGFuZGFyZCBiZW5jaG1hcmtzIGluIG9iamVjdCByZWNvZ25pdGlvbiBhbmQgbGFuZ3VhZ2UgbW9kZWxpbmcuIEl0IGFsc28gc3VwcG9ydHMgYnVpbGRpbmcgYSByZWFsLXdvcmxkIGFwcGxpY2F0aW9uIG9mIGF1dG9tYXRlZCBpbWFnZSBjYXB0aW9uaW5nIG9uIGEgbWFnYXppbmUgd2Vic2l0ZS4gR2l2ZW4gdGhlIGFudGljaXBhdGVkIGluY3JlYXNlcyBpbiBhdmFpbGFibGUgY29tcHV0aW5nIHBvd2VyLCBldm9sdXRpb24gb2YgZGVlcCBuZXR3b3JrcyBpcyBwcm9taXNpbmcgYXBwcm9hY2ggdG8gY29uc3RydWN0aW5nIGRlZXAgbGVhcm5pbmcgYXBwbGljYXRpb25zIGluIHRoZSBmdXR1cmUuDQpEQSAgLSAyMDE3LzAzLy8NClBZICAtIDIwMTcNClVSICAtIGh0dHA6Ly9hcnhpdi5vcmcvYWJzLzE3MDMuMDA1NDgNCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. Miikkulainen <i>et al.</i>, “Evolving Deep Neural Networks,” <i>arXiv:1703.00548 [cs]</i>, Mar. 2017.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Evolving%20Deep%20Neural%20Networks&amp;rft.jtitle=arXiv%3A1703.00548%20%5Bcs%5D&amp;rft.aufirst=Risto&amp;rft.aulast=Miikkulainen&amp;rft.au=Risto%20Miikkulainen&amp;rft.au=Jason%20Liang&amp;rft.au=Elliot%20Meyerson&amp;rft.au=Aditya%20Rawal&amp;rft.au=Dan%20Fink&amp;rft.au=Olivier%20Francon&amp;rft.au=Bala%20Raju&amp;rft.au=Hormoz%20Shahrzad&amp;rft.au=Arshak%20Navruzyan&amp;rft.au=Nigel%20Duffy&amp;rft.au=Babak%20Hodjat&amp;rft.date=2017-03"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Deep Learning</span> <span class="containertitle"></span> (2016) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">I. Goodfellow, Y. Bengio, and A. Courville, <span class="doctitle"><a class="doctitle" href="http://www.deeplearningbook.org"><i>Deep Learning</i>.</a></span> MIT Press, 2016.</div>
  </div><div class="bib-extra">02804</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2016 type__book ""</span></div><div class="blinkitems"><div><div class="blink"><a   href="http://www.deeplearningbook.org" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@book{goodfellow2016deep,
	title = {Deep {Learning}},
	url = {http://www.deeplearningbook.org},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	note = {02804}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQk9PSw0KVEkgIC0gRGVlcCBMZWFybmluZw0KQVUgIC0gR29vZGZlbGxvdywgSWFuDQpBVSAgLSBCZW5naW8sIFlvc2h1YQ0KQVUgIC0gQ291cnZpbGxlLCBBYXJvbg0KREEgIC0gMjAxNi8vLw0KUFkgIC0gMjAxNg0KUEIgIC0gTUlUIFByZXNzDQpVUiAgLSBodHRwOi8vd3d3LmRlZXBsZWFybmluZ2Jvb2sub3JnDQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">I. Goodfellow, Y. Bengio, and A. Courville, <i>Deep Learning</i>. MIT Press, 2016.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Deep%20Learning&amp;rft.publisher=MIT%20Press&amp;rft.aufirst=Ian&amp;rft.aulast=Goodfellow&amp;rft.au=Ian%20Goodfellow&amp;rft.au=Yoshua%20Bengio&amp;rft.au=Aaron%20Courville&amp;rft.date=2016"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Learning Ordinary Differential Equations for Macroeconomic Modelling</span> <span class="containertitle"></span> (December 2015) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">Z. Georgiev and D. Kazakov, “<span class="doctitle">Learning Ordinary Differential Equations for Macroeconomic Modelling</span>,” in <i>2015 IEEE Symposium Series on Computational Intelligence</i>, 2015, pp. 905–909.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2015 type__conferencePaper ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">This article describes an empirical approach to the macroeconomic modelling of the Euro zone. Data for the period 1971 – 2007 has been used to learn systems of ordinary differential equations (ODE) linking inflation‚ real interest and output growth. The equation discovery algorithm LAGRAMGE was used in conjunction with a grammar defining a potentially large range of possible parametric equations. The coefficients of each equation are automatically fitted on the training data and the ones with the lowest error rates returned as a result. We have added a tool for out-of-sample error evaluation to the in-sample evaluation built in LAGRAMGE. The paper compares the performance of ODE models to previous work on the learning of ordinary equations for the same purpose.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@inproceedings{georgiev2015learning,
	title = {Learning {Ordinary} {Differential} {Equations} for {Macroeconomic} {Modelling}},
	doi = {10.1109/SSCI.2015.133},
	booktitle = {2015 {IEEE} {Symposium} {Series} on {Computational} {Intelligence}},
	author = {Georgiev, Z. and Kazakov, D.},
	month = dec,
	year = {2015},
	pages = {905--909}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ09ORg0KVEkgIC0gTGVhcm5pbmcgT3JkaW5hcnkgRGlmZmVyZW50aWFsIEVxdWF0aW9ucyBmb3IgTWFjcm9lY29ub21pYyBNb2RlbGxpbmcNCkFVICAtIEdlb3JnaWV2LCBaLg0KQVUgIC0gS2F6YWtvdiwgRC4NCkFCICAtIFRoaXMgYXJ0aWNsZSBkZXNjcmliZXMgYW4gZW1waXJpY2FsIGFwcHJvYWNoIHRvIHRoZSBtYWNyb2Vjb25vbWljIG1vZGVsbGluZyBvZiB0aGUgRXVybyB6b25lLiBEYXRhIGZvciB0aGUgcGVyaW9kIDE5NzEg4oCTIDIwMDcgaGFzIGJlZW4gdXNlZCB0byBsZWFybiBzeXN0ZW1zIG9mIG9yZGluYXJ5IGRpZmZlcmVudGlhbCBlcXVhdGlvbnMgKE9ERSkgbGlua2luZyBpbmZsYXRpb24sIHJlYWwgaW50ZXJlc3QgYW5kIG91dHB1dCBncm93dGguIFRoZSBlcXVhdGlvbiBkaXNjb3ZlcnkgYWxnb3JpdGhtIExBR1JBTUdFIHdhcyB1c2VkIGluIGNvbmp1bmN0aW9uIHdpdGggYSBncmFtbWFyIGRlZmluaW5nIGEgcG90ZW50aWFsbHkgbGFyZ2UgcmFuZ2Ugb2YgcG9zc2libGUgcGFyYW1ldHJpYyBlcXVhdGlvbnMuIFRoZSBjb2VmZmljaWVudHMgb2YgZWFjaCBlcXVhdGlvbiBhcmUgYXV0b21hdGljYWxseSBmaXR0ZWQgb24gdGhlIHRyYWluaW5nIGRhdGEgYW5kIHRoZSBvbmVzIHdpdGggdGhlIGxvd2VzdCBlcnJvciByYXRlcyByZXR1cm5lZCBhcyBhIHJlc3VsdC4gV2UgaGF2ZSBhZGRlZCBhIHRvb2wgZm9yIG91dC1vZi1zYW1wbGUgZXJyb3IgZXZhbHVhdGlvbiB0byB0aGUgaW4tc2FtcGxlIGV2YWx1YXRpb24gYnVpbHQgaW4gTEFHUkFNR0UuIFRoZSBwYXBlciBjb21wYXJlcyB0aGUgcGVyZm9ybWFuY2Ugb2YgT0RFIG1vZGVscyB0byBwcmV2aW91cyB3b3JrIG9uIHRoZSBsZWFybmluZyBvZiBvcmRpbmFyeSBlcXVhdGlvbnMgZm9yIHRoZSBzYW1lIHB1cnBvc2UuDQpDMyAgLSAyMDE1IElFRUUgU3ltcG9zaXVtIFNlcmllcyBvbiBDb21wdXRhdGlvbmFsIEludGVsbGlnZW5jZQ0KREEgIC0gMjAxNS8xMi8vDQpQWSAgLSAyMDE1DQpETyAgLSAxMC4xMTA5L1NTQ0kuMjAxNS4xMzMNClNQICAtIDkwNQ0KRVAgIC0gOTA5DQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">Z. Georgiev and D. Kazakov, “Learning Ordinary Differential Equations for Macroeconomic Modelling,” in <i>2015 IEEE Symposium Series on Computational Intelligence</i>, 2015, pp. 905–909.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FSSCI.2015.133&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Learning%20Ordinary%20Differential%20Equations%20for%20Macroeconomic%20Modelling&amp;rft.btitle=2015%20IEEE%20Symposium%20Series%20on%20Computational%20Intelligence&amp;rft.aufirst=Z.&amp;rft.aulast=Georgiev&amp;rft.au=Z.%20Georgiev&amp;rft.au=D.%20Kazakov&amp;rft.date=2015-12&amp;rft.pages=905%E2%80%93909"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Guide to NumPy: 2nd Edition</span> <span class="containertitle"></span> (September 2015) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">T. E. Oliphant PhD, <span class="doctitle"><i>Guide to NumPy: 2nd Edition</i></span>, 2 edition. Austin, Tex.: CreateSpace Independent Publishing Platform, 2015.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2015 type__book ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">This is the second edition of Travis Oliphant’s A Guide to NumPy originally published electronically in 2006. It is designed to be a reference that can be used by practitioners who are familiar with Python but want to learn more about NumPy and related tools. In this updated edition‚ new perspectives are shared as well as descriptions of new distributed processing tools in the ecosystem‚ and how Numba can be used to compile code using NumPy arrays. Travis Oliphant is the co-founder and CEO of Continuum Analytics. Continuum Analytics develops Anaconda‚ the leading modern open source analytics platform powered by Python. Travis‚ who is a passionate advocate of open source technology‚ has a Ph.D. from Mayo Clinic and B.S. and M.S. degrees in Mathematics and Electrical Engineering from Brigham Young University. Since 1997‚ he has worked extensively with Python for computational and data science. He was the primary creator of the NumPy package and founding contributor to the SciPy package. He was also a co-founder and past board member of NumFOCUS‚ a non-profit for reproducible and accessible science that supports the PyData stack. He also served on the board of the Python Software Foundation.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@book{oliphant_phd_guide_2015,
	address = {Austin, Tex.},
	edition = {2 edition},
	title = {Guide to {NumPy}: 2nd {Edition}},
	isbn = {978-1-5173-0007-4},
	language = {English},
	publisher = {CreateSpace Independent Publishing Platform},
	author = {Oliphant PhD, Travis E.},
	month = sep,
	year = {2015}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQk9PSw0KVEkgIC0gR3VpZGUgdG8gTnVtUHk6IDJuZCBFZGl0aW9uDQpBVSAgLSBPbGlwaGFudCBQaEQsIFRyYXZpcyBFLg0KQUIgIC0gVGhpcyBpcyB0aGUgc2Vjb25kIGVkaXRpb24gb2YgVHJhdmlzIE9saXBoYW50J3MgQSBHdWlkZSB0byBOdW1QeSBvcmlnaW5hbGx5IHB1Ymxpc2hlZCBlbGVjdHJvbmljYWxseSBpbiAyMDA2LiBJdCBpcyBkZXNpZ25lZCB0byBiZSBhIHJlZmVyZW5jZSB0aGF0IGNhbiBiZSB1c2VkIGJ5IHByYWN0aXRpb25lcnMgd2hvIGFyZSBmYW1pbGlhciB3aXRoIFB5dGhvbiBidXQgd2FudCB0byBsZWFybiBtb3JlIGFib3V0IE51bVB5IGFuZCByZWxhdGVkIHRvb2xzLiBJbiB0aGlzIHVwZGF0ZWQgZWRpdGlvbiwgbmV3IHBlcnNwZWN0aXZlcyBhcmUgc2hhcmVkIGFzIHdlbGwgYXMgZGVzY3JpcHRpb25zIG9mIG5ldyBkaXN0cmlidXRlZCBwcm9jZXNzaW5nIHRvb2xzIGluIHRoZSBlY29zeXN0ZW0sIGFuZCBob3cgTnVtYmEgY2FuIGJlIHVzZWQgdG8gY29tcGlsZSBjb2RlIHVzaW5nIE51bVB5IGFycmF5cy4gVHJhdmlzIE9saXBoYW50IGlzIHRoZSBjby1mb3VuZGVyIGFuZCBDRU8gb2YgQ29udGludXVtIEFuYWx5dGljcy4gQ29udGludXVtIEFuYWx5dGljcyBkZXZlbG9wcyBBbmFjb25kYSwgdGhlIGxlYWRpbmcgbW9kZXJuIG9wZW4gc291cmNlIGFuYWx5dGljcyBwbGF0Zm9ybSBwb3dlcmVkIGJ5IFB5dGhvbi4gVHJhdmlzLCB3aG8gaXMgYSBwYXNzaW9uYXRlIGFkdm9jYXRlIG9mIG9wZW4gc291cmNlIHRlY2hub2xvZ3ksIGhhcyBhIFBoLkQuIGZyb20gTWF5byBDbGluaWMgYW5kIEIuUy4gYW5kIE0uUy4gZGVncmVlcyBpbiBNYXRoZW1hdGljcyBhbmQgRWxlY3RyaWNhbCBFbmdpbmVlcmluZyBmcm9tIEJyaWdoYW0gWW91bmcgVW5pdmVyc2l0eS4gU2luY2UgMTk5NywgaGUgaGFzIHdvcmtlZCBleHRlbnNpdmVseSB3aXRoIFB5dGhvbiBmb3IgY29tcHV0YXRpb25hbCBhbmQgZGF0YSBzY2llbmNlLiBIZSB3YXMgdGhlIHByaW1hcnkgY3JlYXRvciBvZiB0aGUgTnVtUHkgcGFja2FnZSBhbmQgZm91bmRpbmcgY29udHJpYnV0b3IgdG8gdGhlIFNjaVB5IHBhY2thZ2UuIEhlIHdhcyBhbHNvIGEgY28tZm91bmRlciBhbmQgcGFzdCBib2FyZCBtZW1iZXIgb2YgTnVtRk9DVVMsIGEgbm9uLXByb2ZpdCBmb3IgcmVwcm9kdWNpYmxlIGFuZCBhY2Nlc3NpYmxlIHNjaWVuY2UgdGhhdCBzdXBwb3J0cyB0aGUgUHlEYXRhIHN0YWNrLiBIZSBhbHNvIHNlcnZlZCBvbiB0aGUgYm9hcmQgb2YgdGhlIFB5dGhvbiBTb2Z0d2FyZSBGb3VuZGF0aW9uLg0KQ1kgIC0gQXVzdGluLCBUZXguDQpEQSAgLSAyMDE1LzA5Ly8NClBZICAtIDIwMTUNCkVUICAtIDIgZWRpdGlvbg0KTEEgIC0gRW5nbGlzaA0KUEIgIC0gQ3JlYXRlU3BhY2UgSW5kZXBlbmRlbnQgUHVibGlzaGluZyBQbGF0Zm9ybQ0KU04gIC0gOTc4LTEtNTE3My0wMDA3LTQNCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">T. E. Oliphant PhD, <i>Guide to NumPy: 2nd Edition</i>, 2 edition. Austin, Tex.: CreateSpace Independent Publishing Platform, 2015.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-1-5173-0007-4&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Guide%20to%20NumPy%3A%202nd%20Edition&amp;rft.place=Austin%2C%20Tex.&amp;rft.publisher=CreateSpace%20Independent%20Publishing%20Platform&amp;rft.edition=2%20edition&amp;rft.aufirst=Travis%20E.&amp;rft.aulast=Oliphant%20PhD&amp;rft.au=Travis%20E.%20Oliphant%20PhD&amp;rft.date=2015-09&amp;rft.isbn=978-1-5173-0007-4"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</span> <span class="containertitle"></span> (2015) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">Martín Abadi <i>et al.</i>, <span class="doctitle"><a class="doctitle" href="https://www.tensorflow.org/"><i>TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</i>.</a></span> 2015.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2015 type__book ""</span></div><div class="blinkitems"><div><div class="blink"><a   href="https://www.tensorflow.org/" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@book{martin_abadi_tensorflow:_2015,
	title = {{TensorFlow}: {Large}-{Scale} {Machine} {Learning} on {Heterogeneous} {Systems}},
	url = {https://www.tensorflow.org/},
	author = {{Martín Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and {Greg S. Corrado} and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Mané} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Viégas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
	year = {2015}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQk9PSw0KVEkgIC0gVGVuc29yRmxvdzogTGFyZ2UtU2NhbGUgTWFjaGluZSBMZWFybmluZyBvbiBIZXRlcm9nZW5lb3VzIFN5c3RlbXMNCkFVICAtIE1hcnTDrW4gQWJhZGkNCkFVICAtIEFzaGlzaCBBZ2Fyd2FsDQpBVSAgLSBQYXVsIEJhcmhhbQ0KQVUgIC0gRXVnZW5lIEJyZXZkbw0KQVUgIC0gWmhpZmVuZyBDaGVuDQpBVSAgLSBDcmFpZyBDaXRybw0KQVUgIC0gR3JlZyBTLiBDb3JyYWRvDQpBVSAgLSBBbmR5IERhdmlzDQpBVSAgLSBKZWZmcmV5IERlYW4NCkFVICAtIE1hdHRoaWV1IERldmluDQpBVSAgLSBTYW5qYXkgR2hlbWF3YXQNCkFVICAtIElhbiBHb29kZmVsbG93DQpBVSAgLSBBbmRyZXcgSGFycA0KQVUgIC0gR2VvZmZyZXkgSXJ2aW5nDQpBVSAgLSBNaWNoYWVsIElzYXJkDQpBVSAgLSBKaWEsIFlhbmdxaW5nDQpBVSAgLSBSYWZhbCBKb3plZm93aWN6DQpBVSAgLSBMdWthc3ogS2Fpc2VyDQpBVSAgLSBNYW5qdW5hdGggS3VkbHVyDQpBVSAgLSBKb3NoIExldmVuYmVyZw0KQVUgIC0gRGFuZGVsaW9uIE1hbsOpDQpBVSAgLSBSYWphdCBNb25nYQ0KQVUgIC0gU2hlcnJ5IE1vb3JlDQpBVSAgLSBEZXJlayBNdXJyYXkNCkFVICAtIENocmlzIE9sYWgNCkFVICAtIE1pa2UgU2NodXN0ZXINCkFVICAtIEpvbmF0aG9uIFNobGVucw0KQVUgIC0gQmVub2l0IFN0ZWluZXINCkFVICAtIElseWEgU3V0c2tldmVyDQpBVSAgLSBLdW5hbCBUYWx3YXINCkFVICAtIFBhdWwgVHVja2VyDQpBVSAgLSBWaW5jZW50IFZhbmhvdWNrZQ0KQVUgIC0gVmlqYXkgVmFzdWRldmFuDQpBVSAgLSBGZXJuYW5kYSBWacOpZ2FzDQpBVSAgLSBPcmlvbCBWaW55YWxzDQpBVSAgLSBQZXRlIFdhcmRlbg0KQVUgIC0gTWFydGluIFdhdHRlbmJlcmcNCkFVICAtIE1hcnRpbiBXaWNrZQ0KQVUgIC0gWXVhbiBZdQ0KQVUgIC0gWGlhb3FpYW5nIFpoZW5nDQpEQSAgLSAyMDE1Ly8vDQpQWSAgLSAyMDE1DQpVUiAgLSBodHRwczovL3d3dy50ZW5zb3JmbG93Lm9yZy8NCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">Martín Abadi <i>et al.</i>, <i>TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</i>. 2015.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=TensorFlow%3A%20Large-Scale%20Machine%20Learning%20on%20Heterogeneous%20Systems&amp;rft.aulast=Mart%C3%ADn%20Abadi&amp;rft.au=Mart%C3%ADn%20Abadi&amp;rft.au=Ashish%20Agarwal&amp;rft.au=Paul%20Barham&amp;rft.au=Eugene%20Brevdo&amp;rft.au=Zhifeng%20Chen&amp;rft.au=Craig%20Citro&amp;rft.au=Greg%20S.%20Corrado&amp;rft.au=Andy%20Davis&amp;rft.au=Jeffrey%20Dean&amp;rft.au=Matthieu%20Devin&amp;rft.au=Sanjay%20Ghemawat&amp;rft.au=Ian%20Goodfellow&amp;rft.au=Andrew%20Harp&amp;rft.au=Geoffrey%20Irving&amp;rft.au=Michael%20Isard&amp;rft.au=Yangqing%20Jia&amp;rft.au=Rafal%20Jozefowicz&amp;rft.au=Lukasz%20Kaiser&amp;rft.au=Manjunath%20Kudlur&amp;rft.au=Josh%20Levenberg&amp;rft.au=Dandelion%20Man%C3%A9&amp;rft.au=Rajat%20Monga&amp;rft.au=Sherry%20Moore&amp;rft.au=Derek%20Murray&amp;rft.au=Chris%20Olah&amp;rft.au=Mike%20Schuster&amp;rft.au=Jonathon%20Shlens&amp;rft.au=Benoit%20Steiner&amp;rft.au=Ilya%20Sutskever&amp;rft.au=Kunal%20Talwar&amp;rft.au=Paul%20Tucker&amp;rft.au=Vincent%20Vanhoucke&amp;rft.au=Vijay%20Vasudevan&amp;rft.au=Fernanda%20Vi%C3%A9gas&amp;rft.au=Oriol%20Vinyals&amp;rft.au=Pete%20Warden&amp;rft.au=Martin%20Wattenberg&amp;rft.au=Martin%20Wicke&amp;rft.au=Yuan%20Yu&amp;rft.au=Xiaoqiang%20Zheng&amp;rft.date=2015"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Keras</span> <span class="containertitle"></span> (2015) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">F. Chollet and others, <span class="doctitle"><a class="doctitle" href="https://keras.io"><i>Keras</i>.</a></span> 2015.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2015 type__book ""</span></div><div class="blinkitems"><div><div class="blink"><a   href="https://keras.io" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@book{chollet2015keras,
	title = {Keras},
	url = {https://keras.io},
	author = {Chollet, François and {others}},
	year = {2015},
	note = {00000}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQk9PSw0KVEkgIC0gS2VyYXMNCkFVICAtIENob2xsZXQsIEZyYW7Dp29pcw0KQVUgIC0gb3RoZXJzDQpEQSAgLSAyMDE1Ly8vDQpQWSAgLSAyMDE1DQpVUiAgLSBodHRwczovL2tlcmFzLmlvDQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">F. Chollet and others, <i>Keras</i>. 2015.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Keras&amp;rft.aufirst=Fran%C3%A7ois&amp;rft.aulast=Chollet&amp;rft.au=Fran%C3%A7ois%20Chollet&amp;rft.au=others&amp;rft.date=2015"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Time Series Analysis: Forecasting and Control</span> <span class="containertitle"></span> (May 2015) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">G. E. P. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, <span class="doctitle"><i>Time Series Analysis: Forecasting and Control</i>.</span> John Wiley &amp; Sons, 2015.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2015 type__book ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Praise for the Fourth Edition “The book follows faithfully the style of the original edition. The approach is heavily motivated by real-world time series‚ and by developing a complete approach to model building‚ estimation‚ forecasting and control." - Mathematical Reviews Bridging classical models and modern topics‚ the Fifth Edition of Time Series Analysis: Forecasting and Control maintains a balanced presentation of the tools for modeling and analyzing time series. Also describing the latest developments that have occurred in the field over the past decade through applications from areas such as business‚ finance‚ and engineering‚ the Fifth Edition continues to serve as one of the most influential and prominent works on the subject. Time Series Analysis: Forecasting and Control‚ Fifth Edition provides a clearly written exploration of the key methods for building‚ classifying‚ testing‚ and analyzing stochastic models for time series and describes their use in five important areas of application: forecasting; determining the transfer function of a system; modeling the effects of intervention events; developing multivariate dynamic models; and designing simple control schemes. Along with these classical uses‚ the new edition covers modern topics with new features that include: A redesigned chapter on multivariate time series analysis with an expanded treatment of Vector Autoregressive‚ or VAR models‚ along with a discussion of the analytical tools needed for modeling vector time series An expanded chapter on special topics covering unit root testing‚ time-varying volatility models such as ARCH and GARCH‚ nonlinear time series models‚ and long memory models Numerous examples drawn from finance‚ economics‚ engineering‚ and other related fields The use of the publicly available R software for graphical illustrations and numerical calculations along with scripts that demonstrate the use of R for model building and forecasting Updates to literature references throughout and new end-of-chapter exercises Streamlined chapter introductions and revisions that update and enhance the exposition Time Series Analysis: Forecasting and Control‚ Fifth Edition is a valuable real-world reference for researchers and practitioners in time series analysis‚ econometrics‚ finance‚ and related fields. The book is also an excellent textbook for beginning graduate-level courses in advanced statistics‚ mathematics‚ economics‚ finance‚ engineering‚ and physics.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@book{box2015time,
	title = {Time {Series} {Analysis}: {Forecasting} and {Control}},
	isbn = {978-1-118-67492-5},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Box, George E. P. and Jenkins, Gwilym M. and Reinsel, Gregory C. and Ljung, Greta M.},
	month = may,
	year = {2015}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('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');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">G. E. P. Box, G. M. Jenkins, G. C. Reinsel, and G. M. Ljung, <i>Time Series Analysis: Forecasting and Control</i>. John Wiley &amp; Sons, 2015.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-1-118-67492-5&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Time%20Series%20Analysis%3A%20Forecasting%20and%20Control&amp;rft.publisher=John%20Wiley%20%26%20Sons&amp;rft.aufirst=George%20E.%20P.&amp;rft.aulast=Box&amp;rft.au=George%20E.%20P.%20Box&amp;rft.au=Gwilym%20M.%20Jenkins&amp;rft.au=Gregory%20C.%20Reinsel&amp;rft.au=Greta%20M.%20Ljung&amp;rft.date=2015-05&amp;rft.isbn=978-1-118-67492-5"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</span> <span class="containertitle">arXiv:1502.01852 [cs]</span> (February 2015) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">K. He, X. Zhang, S. Ren, and J. Sun, “<span class="doctitle"><a class="doctitle" href="http://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a></span>,” <i>arXiv:1502.01852 [cs]</i>, Feb. 2015.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2015 type__journalArticle ""</span></div><div class="bib-venue">arXiv:1502.01852 [cs]</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work‚ we study rectifier neural networks for image classification from two aspects. First‚ we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second‚ we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets)‚ we achieve 4.94% top-5 test error on the ImageNet 2012 classification dataset. This is a 26% relative improvement over the ILSVRC 2014 winner (GoogLeNet‚ 6.66%). To our knowledge‚ our result is the first to surpass human-level performance (5.1%‚ Russakovsky et al.) on this visual recognition challenge.</div></div></div><div class="blink"><a   href="http://arxiv.org/abs/1502.01852" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{he2015delving,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	url = {http://arxiv.org/abs/1502.01852},
	journal = {arXiv:1502.01852 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = feb,
	year = {2015}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gRGVsdmluZyBEZWVwIGludG8gUmVjdGlmaWVyczogU3VycGFzc2luZyBIdW1hbi1MZXZlbCBQZXJmb3JtYW5jZSBvbiBJbWFnZU5ldCBDbGFzc2lmaWNhdGlvbg0KQVUgIC0gSGUsIEthaW1pbmcNCkFVICAtIFpoYW5nLCBYaWFuZ3l1DQpBVSAgLSBSZW4sIFNoYW9xaW5nDQpBVSAgLSBTdW4sIEppYW4NClQyICAtIGFyWGl2OjE1MDIuMDE4NTIgW2NzXQ0KQUIgIC0gUmVjdGlmaWVkIGFjdGl2YXRpb24gdW5pdHMgKHJlY3RpZmllcnMpIGFyZSBlc3NlbnRpYWwgZm9yIHN0YXRlLW9mLXRoZS1hcnQgbmV1cmFsIG5ldHdvcmtzLiBJbiB0aGlzIHdvcmssIHdlIHN0dWR5IHJlY3RpZmllciBuZXVyYWwgbmV0d29ya3MgZm9yIGltYWdlIGNsYXNzaWZpY2F0aW9uIGZyb20gdHdvIGFzcGVjdHMuIEZpcnN0LCB3ZSBwcm9wb3NlIGEgUGFyYW1ldHJpYyBSZWN0aWZpZWQgTGluZWFyIFVuaXQgKFBSZUxVKSB0aGF0IGdlbmVyYWxpemVzIHRoZSB0cmFkaXRpb25hbCByZWN0aWZpZWQgdW5pdC4gUFJlTFUgaW1wcm92ZXMgbW9kZWwgZml0dGluZyB3aXRoIG5lYXJseSB6ZXJvIGV4dHJhIGNvbXB1dGF0aW9uYWwgY29zdCBhbmQgbGl0dGxlIG92ZXJmaXR0aW5nIHJpc2suIFNlY29uZCwgd2UgZGVyaXZlIGEgcm9idXN0IGluaXRpYWxpemF0aW9uIG1ldGhvZCB0aGF0IHBhcnRpY3VsYXJseSBjb25zaWRlcnMgdGhlIHJlY3RpZmllciBub25saW5lYXJpdGllcy4gVGhpcyBtZXRob2QgZW5hYmxlcyB1cyB0byB0cmFpbiBleHRyZW1lbHkgZGVlcCByZWN0aWZpZWQgbW9kZWxzIGRpcmVjdGx5IGZyb20gc2NyYXRjaCBhbmQgdG8gaW52ZXN0aWdhdGUgZGVlcGVyIG9yIHdpZGVyIG5ldHdvcmsgYXJjaGl0ZWN0dXJlcy4gQmFzZWQgb24gb3VyIFBSZUxVIG5ldHdvcmtzIChQUmVMVS1uZXRzKSwgd2UgYWNoaWV2ZSA0Ljk0JSB0b3AtNSB0ZXN0IGVycm9yIG9uIHRoZSBJbWFnZU5ldCAyMDEyIGNsYXNzaWZpY2F0aW9uIGRhdGFzZXQuIFRoaXMgaXMgYSAyNiUgcmVsYXRpdmUgaW1wcm92ZW1lbnQgb3ZlciB0aGUgSUxTVlJDIDIwMTQgd2lubmVyIChHb29nTGVOZXQsIDYuNjYlKS4gVG8gb3VyIGtub3dsZWRnZSwgb3VyIHJlc3VsdCBpcyB0aGUgZmlyc3QgdG8gc3VycGFzcyBodW1hbi1sZXZlbCBwZXJmb3JtYW5jZSAoNS4xJSwgUnVzc2Frb3Zza3kgZXQgYWwuKSBvbiB0aGlzIHZpc3VhbCByZWNvZ25pdGlvbiBjaGFsbGVuZ2UuDQpEQSAgLSAyMDE1LzAyLy8NClBZICAtIDIwMTUNClVSICAtIGh0dHA6Ly9hcnhpdi5vcmcvYWJzLzE1MDIuMDE4NTINCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">K. He, X. Zhang, S. Ren, and J. Sun, “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification,” <i>arXiv:1502.01852 [cs]</i>, Feb. 2015.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Delving%20Deep%20into%20Rectifiers%3A%20Surpassing%20Human-Level%20Performance%20on%20ImageNet%20Classification&amp;rft.jtitle=arXiv%3A1502.01852%20%5Bcs%5D&amp;rft.aufirst=Kaiming&amp;rft.aulast=He&amp;rft.au=Kaiming%20He&amp;rft.au=Xiangyu%20Zhang&amp;rft.au=Shaoqing%20Ren&amp;rft.au=Jian%20Sun&amp;rft.date=2015-02"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</span> <span class="containertitle">arXiv:1502.03167 [cs]</span> (February 2015) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">S. Ioffe and C. Szegedy, “<span class="doctitle"><a class="doctitle" href="http://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></span>,” <i>arXiv:1502.03167 [cs]</i>, Feb. 2015.</div>
  </div><div class="bib-extra">04808</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2015 type__journalArticle ""</span></div><div class="bib-venue">arXiv:1502.03167 [cs]</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training‚ as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization‚ and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift‚ and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer‚ in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model‚ Batch Normalization achieves the same accuracy with 14 times fewer training steps‚ and beats the original model by a significant margin. Using an ensemble of batch-normalized networks‚ we improve upon the best published result on ImageNet classification: reaching 4.9% top-5 validation error (and 4.8% test error)‚ exceeding the accuracy of human raters.</div></div></div><div class="blink"><a   href="http://arxiv.org/abs/1502.03167" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{ioffe2015batch,
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	url = {http://arxiv.org/abs/1502.03167},
	journal = {arXiv:1502.03167 [cs]},
	author = {Ioffe, Sergey and Szegedy, Christian},
	month = feb,
	year = {2015},
	note = {04808}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gQmF0Y2ggTm9ybWFsaXphdGlvbjogQWNjZWxlcmF0aW5nIERlZXAgTmV0d29yayBUcmFpbmluZyBieSBSZWR1Y2luZyBJbnRlcm5hbCBDb3ZhcmlhdGUgU2hpZnQNCkFVICAtIElvZmZlLCBTZXJnZXkNCkFVICAtIFN6ZWdlZHksIENocmlzdGlhbg0KVDIgIC0gYXJYaXY6MTUwMi4wMzE2NyBbY3NdDQpBQiAgLSBUcmFpbmluZyBEZWVwIE5ldXJhbCBOZXR3b3JrcyBpcyBjb21wbGljYXRlZCBieSB0aGUgZmFjdCB0aGF0IHRoZSBkaXN0cmlidXRpb24gb2YgZWFjaCBsYXllcidzIGlucHV0cyBjaGFuZ2VzIGR1cmluZyB0cmFpbmluZywgYXMgdGhlIHBhcmFtZXRlcnMgb2YgdGhlIHByZXZpb3VzIGxheWVycyBjaGFuZ2UuIFRoaXMgc2xvd3MgZG93biB0aGUgdHJhaW5pbmcgYnkgcmVxdWlyaW5nIGxvd2VyIGxlYXJuaW5nIHJhdGVzIGFuZCBjYXJlZnVsIHBhcmFtZXRlciBpbml0aWFsaXphdGlvbiwgYW5kIG1ha2VzIGl0IG5vdG9yaW91c2x5IGhhcmQgdG8gdHJhaW4gbW9kZWxzIHdpdGggc2F0dXJhdGluZyBub25saW5lYXJpdGllcy4gV2UgcmVmZXIgdG8gdGhpcyBwaGVub21lbm9uIGFzIGludGVybmFsIGNvdmFyaWF0ZSBzaGlmdCwgYW5kIGFkZHJlc3MgdGhlIHByb2JsZW0gYnkgbm9ybWFsaXppbmcgbGF5ZXIgaW5wdXRzLiBPdXIgbWV0aG9kIGRyYXdzIGl0cyBzdHJlbmd0aCBmcm9tIG1ha2luZyBub3JtYWxpemF0aW9uIGEgcGFydCBvZiB0aGUgbW9kZWwgYXJjaGl0ZWN0dXJlIGFuZCBwZXJmb3JtaW5nIHRoZSBub3JtYWxpemF0aW9uIGZvciBlYWNoIHRyYWluaW5nIG1pbmktYmF0Y2guIEJhdGNoIE5vcm1hbGl6YXRpb24gYWxsb3dzIHVzIHRvIHVzZSBtdWNoIGhpZ2hlciBsZWFybmluZyByYXRlcyBhbmQgYmUgbGVzcyBjYXJlZnVsIGFib3V0IGluaXRpYWxpemF0aW9uLiBJdCBhbHNvIGFjdHMgYXMgYSByZWd1bGFyaXplciwgaW4gc29tZSBjYXNlcyBlbGltaW5hdGluZyB0aGUgbmVlZCBmb3IgRHJvcG91dC4gQXBwbGllZCB0byBhIHN0YXRlLW9mLXRoZS1hcnQgaW1hZ2UgY2xhc3NpZmljYXRpb24gbW9kZWwsIEJhdGNoIE5vcm1hbGl6YXRpb24gYWNoaWV2ZXMgdGhlIHNhbWUgYWNjdXJhY3kgd2l0aCAxNCB0aW1lcyBmZXdlciB0cmFpbmluZyBzdGVwcywgYW5kIGJlYXRzIHRoZSBvcmlnaW5hbCBtb2RlbCBieSBhIHNpZ25pZmljYW50IG1hcmdpbi4gVXNpbmcgYW4gZW5zZW1ibGUgb2YgYmF0Y2gtbm9ybWFsaXplZCBuZXR3b3Jrcywgd2UgaW1wcm92ZSB1cG9uIHRoZSBiZXN0IHB1Ymxpc2hlZCByZXN1bHQgb24gSW1hZ2VOZXQgY2xhc3NpZmljYXRpb246IHJlYWNoaW5nIDQuOSUgdG9wLTUgdmFsaWRhdGlvbiBlcnJvciAoYW5kIDQuOCUgdGVzdCBlcnJvciksIGV4Y2VlZGluZyB0aGUgYWNjdXJhY3kgb2YgaHVtYW4gcmF0ZXJzLg0KREEgIC0gMjAxNS8wMi8vDQpQWSAgLSAyMDE1DQpVUiAgLSBodHRwOi8vYXJ4aXYub3JnL2Ficy8xNTAyLjAzMTY3DQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,” <i>arXiv:1502.03167 [cs]</i>, Feb. 2015.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Batch%20Normalization%3A%20Accelerating%20Deep%20Network%20Training%20by%20Reducing%20Internal%20Covariate%20Shift&amp;rft.jtitle=arXiv%3A1502.03167%20%5Bcs%5D&amp;rft.aufirst=Sergey&amp;rft.aulast=Ioffe&amp;rft.au=Sergey%20Ioffe&amp;rft.au=Christian%20Szegedy&amp;rft.date=2015-02"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Adam: A Method for Stochastic Optimization</span> <span class="containertitle">arXiv:1412.6980 [cs]</span> (December 2014) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. P. Kingma and J. Ba, “<span class="doctitle"><a class="doctitle" href="http://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a></span>,” <i>arXiv:1412.6980 [cs]</i>, Dec. 2014.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2014 type__journalArticle ""</span></div><div class="bib-venue">arXiv:1412.6980 [cs]</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">We introduce Adam‚ an algorithm for first-order gradient-based optimization of stochastic objective functions‚ based on adaptive estimates of lower-order moments. The method is straightforward to implement‚ is computationally efficient‚ has little memory requirements‚ is invariant to diagonal rescaling of the gradients‚ and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms‚ on which Adam was inspired‚ are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally‚ we discuss AdaMax‚ a variant of Adam based on the infinity norm.</div></div></div><div class="blink"><a   href="http://arxiv.org/abs/1412.6980" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{kingma2014adam:,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	url = {http://arxiv.org/abs/1412.6980},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = dec,
	year = {2014}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gQWRhbTogQSBNZXRob2QgZm9yIFN0b2NoYXN0aWMgT3B0aW1pemF0aW9uDQpBVSAgLSBLaW5nbWEsIERpZWRlcmlrIFAuDQpBVSAgLSBCYSwgSmltbXkNClQyICAtIGFyWGl2OjE0MTIuNjk4MCBbY3NdDQpBQiAgLSBXZSBpbnRyb2R1Y2UgQWRhbSwgYW4gYWxnb3JpdGhtIGZvciBmaXJzdC1vcmRlciBncmFkaWVudC1iYXNlZCBvcHRpbWl6YXRpb24gb2Ygc3RvY2hhc3RpYyBvYmplY3RpdmUgZnVuY3Rpb25zLCBiYXNlZCBvbiBhZGFwdGl2ZSBlc3RpbWF0ZXMgb2YgbG93ZXItb3JkZXIgbW9tZW50cy4gVGhlIG1ldGhvZCBpcyBzdHJhaWdodGZvcndhcmQgdG8gaW1wbGVtZW50LCBpcyBjb21wdXRhdGlvbmFsbHkgZWZmaWNpZW50LCBoYXMgbGl0dGxlIG1lbW9yeSByZXF1aXJlbWVudHMsIGlzIGludmFyaWFudCB0byBkaWFnb25hbCByZXNjYWxpbmcgb2YgdGhlIGdyYWRpZW50cywgYW5kIGlzIHdlbGwgc3VpdGVkIGZvciBwcm9ibGVtcyB0aGF0IGFyZSBsYXJnZSBpbiB0ZXJtcyBvZiBkYXRhIGFuZC9vciBwYXJhbWV0ZXJzLiBUaGUgbWV0aG9kIGlzIGFsc28gYXBwcm9wcmlhdGUgZm9yIG5vbi1zdGF0aW9uYXJ5IG9iamVjdGl2ZXMgYW5kIHByb2JsZW1zIHdpdGggdmVyeSBub2lzeSBhbmQvb3Igc3BhcnNlIGdyYWRpZW50cy4gVGhlIGh5cGVyLXBhcmFtZXRlcnMgaGF2ZSBpbnR1aXRpdmUgaW50ZXJwcmV0YXRpb25zIGFuZCB0eXBpY2FsbHkgcmVxdWlyZSBsaXR0bGUgdHVuaW5nLiBTb21lIGNvbm5lY3Rpb25zIHRvIHJlbGF0ZWQgYWxnb3JpdGhtcywgb24gd2hpY2ggQWRhbSB3YXMgaW5zcGlyZWQsIGFyZSBkaXNjdXNzZWQuIFdlIGFsc28gYW5hbHl6ZSB0aGUgdGhlb3JldGljYWwgY29udmVyZ2VuY2UgcHJvcGVydGllcyBvZiB0aGUgYWxnb3JpdGhtIGFuZCBwcm92aWRlIGEgcmVncmV0IGJvdW5kIG9uIHRoZSBjb252ZXJnZW5jZSByYXRlIHRoYXQgaXMgY29tcGFyYWJsZSB0byB0aGUgYmVzdCBrbm93biByZXN1bHRzIHVuZGVyIHRoZSBvbmxpbmUgY29udmV4IG9wdGltaXphdGlvbiBmcmFtZXdvcmsuIEVtcGlyaWNhbCByZXN1bHRzIGRlbW9uc3RyYXRlIHRoYXQgQWRhbSB3b3JrcyB3ZWxsIGluIHByYWN0aWNlIGFuZCBjb21wYXJlcyBmYXZvcmFibHkgdG8gb3RoZXIgc3RvY2hhc3RpYyBvcHRpbWl6YXRpb24gbWV0aG9kcy4gRmluYWxseSwgd2UgZGlzY3VzcyBBZGFNYXgsIGEgdmFyaWFudCBvZiBBZGFtIGJhc2VkIG9uIHRoZSBpbmZpbml0eSBub3JtLg0KREEgIC0gMjAxNC8xMi8vDQpQWSAgLSAyMDE0DQpVUiAgLSBodHRwOi8vYXJ4aXYub3JnL2Ficy8xNDEyLjY5ODANCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,” <i>arXiv:1412.6980 [cs]</i>, Dec. 2014.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Adam%3A%20A%20Method%20for%20Stochastic%20Optimization&amp;rft.jtitle=arXiv%3A1412.6980%20%5Bcs%5D&amp;rft.aufirst=Diederik%20P.&amp;rft.aulast=Kingma&amp;rft.au=Diederik%20P.%20Kingma&amp;rft.au=Jimmy%20Ba&amp;rft.date=2014-12"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">seaborn: v0.5.0 (November 2014)</span> <span class="containertitle"></span> (November 2014) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">M. Waskom <i>et al.</i>, <span class="doctitle"><a class="doctitle" href="https://zenodo.org/record/12710#.Ww_VRHXwaV4"><i>seaborn: v0.5.0 (November 2014)</i>.</a></span> Zenodo, 2014.</div>
  </div><div class="bib-extra">DOI: 10.5281/zenodo.12710</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2014 type__book ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">This is a major release from 0.4. Highlights include new functions for plotting heatmaps‚ possibly while applying clustering algorithms to discover structured relationships. These functions are complemented by new custom colormap functions and a full set of IPython widgets that allow interactive selection of colormap parameters. The palette tutorial has been rewritten to cover these new tools and more generally provide guidance on how to use color in visualizations. There are also a number of smaller changes and bugfixes. Plotting functions Added the heatmap function for visualizing a matrix of data by color-encoding the values. See the docs for more information. Added the clustermap function for clustering and visualizing a matrix of data‚ with options to label individual rows and columns by colors. See the docs for more information. This work was lead by Olga Botvinnik. lmplot and pairplot get a new keyword argument‚ markers. This can be a single kind of marker or a list of different markers for each level of the hue variable. Using different markers for different hues should let plots be more comprehensible when reproduced to black-and-white (i.e. when printed). See the github pull request for examples. More generally‚ there is a new keyword argument in FacetGrid and PairGrid‚ hue_kws. This similarly lets plot aesthetics vary across the levels of the hue variable‚ but more flexibily. hue_kws should be a dictionary that maps the name of keyword arguments to lists of values that are as long as the number of levels of the hue variable. The argument subplot_kws has been added to FacetGrid. This allows for faceted plots with custom projections‚ including maps with Cartopy. Color palettes Added two new functions to create custom color palettes. For sequential palettes‚ you can use the light_palette function‚ which takes a seed color and creates a ramp from a very light‚ desaturated variant of it. For diverging palettes‚ you can use the diverging_palette function to create a balanced ramp between two endpoints to a light or dark midpoint. See the palette tutorial \textbackslashtextlesspalette_tutorial\textbackslashtextgreater for more information. Added the ability to specify the seed color for light_palette and dark_palette as a tuple of husl or hls space values or as a named xkcd color. The interpretation of the seed color is now provided by the new input parameter to these functions. Added several new interactive palette widgets: choose_colorbrewer_palette‚ choose_light_palette‚ choose_dark_palette‚ and choose_diverging_palette. For consistency‚ renamed the cubehelix widget to choose_cubehelix_palette (and fixed a bug where the cubehelix palette was reversed). These functions also now return either a color palette list or a matplotlib colormap when called‚ and that object will be live-updated as you play with the widget. This should make it easy to iterate over a plot until you find a good representation for the data. See the Github pull request or this notebook (download it to use the widgets) for more information. Overhauled the color palette tutorial to organize the discussion by class of color palette and provide more motivation behind the various choices one might make when choosing colors for their data. Bug fixes Fixed a bug in PairGrid that gave incorrect results (or a crash) when the input DataFrame has a non-default index. Fixed a bug in PairGrid where passing columns with a date-like datatype raised an exception. Fixed a bug where lmplot would show a legend when the hue variable was also used on either the rows or columns (making the legend redundant). Worked around a matplotlib bug that was forcing outliers in boxplot to appear as blue. kdeplot now accepts pandas Series for the data and data2 arguments. Using a non-default correlation method in corrplot now implies sig_stars=False as the permutation test used to significance values for the correlations uses a pearson metric. Removed pdf.fonttype from the style definitions‚ as the value used in version 0.4 resulted in very large PDF files.</div></div></div><div class="blink"><a   href="https://zenodo.org/record/12710#.Ww_VRHXwaV4" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@book{waskom2014seaborn:,
	title = {seaborn: v0.5.0 ({November} 2014)},
	url = {https://zenodo.org/record/12710#.Ww_VRHXwaV4},
	publisher = {Zenodo},
	author = {Waskom, Michael and Botvinnik, Olga and Hobson, Paul and Cole, John B. and Halchenko, Yaroslav and Hoyer, Stephan and Miles, Alistair and Augspurger, Tom and Yarkoni, Tal and Megies, Tobias and Coelho, Luis Pedro and Wehner, Daniel and {cynddl} and Ziegler, Erik and {diego0020} and Zaytsev, Yury V. and Hoppe, Travis and Seabold, Skipper and Cloud, Phillip and Koskinen, Miikka and Meyer, Kyle and Qalieh, Adel and Allan, Dan},
	month = nov,
	year = {2014},
	doi = {10.5281/zenodo.12710}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('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');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">M. Waskom <i>et al.</i>, <i>seaborn: v0.5.0 (November 2014)</i>. Zenodo, 2014.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=seaborn%3A%20v0.5.0%20(November%202014)&amp;rft.publisher=Zenodo&amp;rft.aufirst=Michael&amp;rft.aulast=Waskom&amp;rft.au=Michael%20Waskom&amp;rft.au=Olga%20Botvinnik&amp;rft.au=Paul%20Hobson&amp;rft.au=John%20B.%20Cole&amp;rft.au=Yaroslav%20Halchenko&amp;rft.au=Stephan%20Hoyer&amp;rft.au=Alistair%20Miles&amp;rft.au=Tom%20Augspurger&amp;rft.au=Tal%20Yarkoni&amp;rft.au=Tobias%20Megies&amp;rft.au=Luis%20Pedro%20Coelho&amp;rft.au=Daniel%20Wehner&amp;rft.au=cynddl&amp;rft.au=Erik%20Ziegler&amp;rft.au=diego0020&amp;rft.au=Yury%20V.%20Zaytsev&amp;rft.au=Travis%20Hoppe&amp;rft.au=Skipper%20Seabold&amp;rft.au=Phillip%20Cloud&amp;rft.au=Miikka%20Koskinen&amp;rft.au=Kyle%20Meyer&amp;rft.au=Adel%20Qalieh&amp;rft.au=Dan%20Allan&amp;rft.date=2014-11"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Once you SCOOP, no need to fork</span> <span class="containertitle"></span> (2014) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">Y. Hold-Geoffroy, O. Gagnon, and M. Parizeau, “<span class="doctitle">Once you SCOOP, no need to fork</span>,” in <i>Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment</i>, 2014, p. 60.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2014 type__conferencePaper ""</span></div><div class="blinkitems"><div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@inproceedings{hold-geoffroy2014once,
	title = {Once you {SCOOP}, no need to fork},
	booktitle = {Proceedings of the 2014 {Annual} {Conference} on {Extreme} {Science} and {Engineering} {Discovery} {Environment}},
	publisher = {ACM},
	author = {Hold-Geoffroy, Yannick and Gagnon, Olivier and Parizeau, Marc},
	year = {2014},
	pages = {60}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ09ORg0KVEkgIC0gT25jZSB5b3UgU0NPT1AsIG5vIG5lZWQgdG8gZm9yaw0KQVUgIC0gSG9sZC1HZW9mZnJveSwgWWFubmljaw0KQVUgIC0gR2Fnbm9uLCBPbGl2aWVyDQpBVSAgLSBQYXJpemVhdSwgTWFyYw0KQzMgIC0gUHJvY2VlZGluZ3Mgb2YgdGhlIDIwMTQgQW5udWFsIENvbmZlcmVuY2Ugb24gRXh0cmVtZSBTY2llbmNlIGFuZCBFbmdpbmVlcmluZyBEaXNjb3ZlcnkgRW52aXJvbm1lbnQNCkRBICAtIDIwMTQvLy8NClBZICAtIDIwMTQNClNQICAtIDYwDQpQQiAgLSBBQ00NCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">Y. Hold-Geoffroy, O. Gagnon, and M. Parizeau, “Once you SCOOP, no need to fork,” in <i>Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment</i>, 2014, p. 60.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Once%20you%20SCOOP%2C%20no%20need%20to%20fork&amp;rft.btitle=Proceedings%20of%20the%202014%20Annual%20Conference%20on%20Extreme%20Science%20and%20Engineering%20Discovery%20Environment&amp;rft.publisher=ACM&amp;rft.aufirst=Yannick&amp;rft.aulast=Hold-Geoffroy&amp;rft.au=Yannick%20Hold-Geoffroy&amp;rft.au=Olivier%20Gagnon&amp;rft.au=Marc%20Parizeau&amp;rft.date=2014&amp;rft.pages=60"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</span> <span class="containertitle"></span> (2014) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “<span class="doctitle">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</span>,” p. 30, 2014.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2014 type__journalArticle ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Deep neural nets with a large number of parameters are very powerful machine learning systems. However‚ overfitting is a serious problem in such networks. Large networks are also slow to use‚ making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training‚ dropout samples from an exponential number of different “thinned” networks. At test time‚ it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision‚ speech recognition‚ document classification and computational biology‚ obtaining state-of-the-art results on many benchmark data sets.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{srivastava2014dropout:,
	title = {Dropout: {A} {Simple} {Way} to {Prevent} {Neural} {Networks} from {Overfitting}},
	language = {en},
	author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {30}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gRHJvcG91dDogQSBTaW1wbGUgV2F5IHRvIFByZXZlbnQgTmV1cmFsIE5ldHdvcmtzIGZyb20gT3ZlcmZpdHRpbmcNCkFVICAtIFNyaXZhc3RhdmEsIE5pdGlzaA0KQVUgIC0gSGludG9uLCBHZW9mZnJleQ0KQVUgIC0gS3JpemhldnNreSwgQWxleA0KQVUgIC0gU3V0c2tldmVyLCBJbHlhDQpBVSAgLSBTYWxha2h1dGRpbm92LCBSdXNsYW4NCkFCICAtIERlZXAgbmV1cmFsIG5ldHMgd2l0aCBhIGxhcmdlIG51bWJlciBvZiBwYXJhbWV0ZXJzIGFyZSB2ZXJ5IHBvd2VyZnVsIG1hY2hpbmUgbGVhcm5pbmcgc3lzdGVtcy4gSG93ZXZlciwgb3ZlcmZpdHRpbmcgaXMgYSBzZXJpb3VzIHByb2JsZW0gaW4gc3VjaCBuZXR3b3Jrcy4gTGFyZ2UgbmV0d29ya3MgYXJlIGFsc28gc2xvdyB0byB1c2UsIG1ha2luZyBpdCBkaWZmaWN1bHQgdG8gZGVhbCB3aXRoIG92ZXJmaXR0aW5nIGJ5IGNvbWJpbmluZyB0aGUgcHJlZGljdGlvbnMgb2YgbWFueSBkaWZmZXJlbnQgbGFyZ2UgbmV1cmFsIG5ldHMgYXQgdGVzdCB0aW1lLiBEcm9wb3V0IGlzIGEgdGVjaG5pcXVlIGZvciBhZGRyZXNzaW5nIHRoaXMgcHJvYmxlbS4gVGhlIGtleSBpZGVhIGlzIHRvIHJhbmRvbWx5IGRyb3AgdW5pdHMgKGFsb25nIHdpdGggdGhlaXIgY29ubmVjdGlvbnMpIGZyb20gdGhlIG5ldXJhbCBuZXR3b3JrIGR1cmluZyB0cmFpbmluZy4gVGhpcyBwcmV2ZW50cyB1bml0cyBmcm9tIGNvLWFkYXB0aW5nIHRvbyBtdWNoLiBEdXJpbmcgdHJhaW5pbmcsIGRyb3BvdXQgc2FtcGxlcyBmcm9tIGFuIGV4cG9uZW50aWFsIG51bWJlciBvZiBkaWZmZXJlbnQg4oCcdGhpbm5lZOKAnSBuZXR3b3Jrcy4gQXQgdGVzdCB0aW1lLCBpdCBpcyBlYXN5IHRvIGFwcHJveGltYXRlIHRoZSBlZmZlY3Qgb2YgYXZlcmFnaW5nIHRoZSBwcmVkaWN0aW9ucyBvZiBhbGwgdGhlc2UgdGhpbm5lZCBuZXR3b3JrcyBieSBzaW1wbHkgdXNpbmcgYSBzaW5nbGUgdW50aGlubmVkIG5ldHdvcmsgdGhhdCBoYXMgc21hbGxlciB3ZWlnaHRzLiBUaGlzIHNpZ25pZmljYW50bHkgcmVkdWNlcyBvdmVyZml0dGluZyBhbmQgZ2l2ZXMgbWFqb3IgaW1wcm92ZW1lbnRzIG92ZXIgb3RoZXIgcmVndWxhcml6YXRpb24gbWV0aG9kcy4gV2Ugc2hvdyB0aGF0IGRyb3BvdXQgaW1wcm92ZXMgdGhlIHBlcmZvcm1hbmNlIG9mIG5ldXJhbCBuZXR3b3JrcyBvbiBzdXBlcnZpc2VkIGxlYXJuaW5nIHRhc2tzIGluIHZpc2lvbiwgc3BlZWNoIHJlY29nbml0aW9uLCBkb2N1bWVudCBjbGFzc2lmaWNhdGlvbiBhbmQgY29tcHV0YXRpb25hbCBiaW9sb2d5LCBvYnRhaW5pbmcgc3RhdGUtb2YtdGhlLWFydCByZXN1bHRzIG9uIG1hbnkgYmVuY2htYXJrIGRhdGEgc2V0cy4NCkRBICAtIDIwMTQvLy8NClBZICAtIDIwMTQNClNQICAtIDMwDQpMQSAgLSBlbg0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, “Dropout: A Simple Way to Prevent Neural Networks from Overfitting,” p. 30, 2014.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Dropout%3A%20A%20Simple%20Way%20to%20Prevent%20Neural%20Networks%20from%20Overfitting&amp;rft.aufirst=Nitish&amp;rft.aulast=Srivastava&amp;rft.au=Nitish%20Srivastava&amp;rft.au=Geoffrey%20Hinton&amp;rft.au=Alex%20Krizhevsky&amp;rft.au=Ilya%20Sutskever&amp;rft.au=Ruslan%20Salakhutdinov&amp;rft.date=2014&amp;rft.pages=30"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">API design for machine learning software: experiences from the scikit-learn project</span> <span class="containertitle"></span> (2013) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">L. Buitinck <i>et al.</i>, “<span class="doctitle">API design for machine learning software: experiences from the scikit-learn project</span>,” in <i>ECML PKDD Workshop: Languages for Data Mining and Machine Learning</i>, 2013, pp. 108–122.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2013 type__conferencePaper ""</span></div><div class="blinkitems"><div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@inproceedings{buitinck2013api,
	title = {{API} design for machine learning software: experiences from the scikit-learn project},
	booktitle = {{ECML} {PKDD} {Workshop}: {Languages} for {Data} {Mining} and {Machine} {Learning}},
	author = {Buitinck, Lars and Louppe, Gilles and Blondel, Mathieu and Pedregosa, Fabian and Mueller, Andreas and Grisel, Olivier and Niculae, Vlad and Prettenhofer, Peter and Gramfort, Alexandre and Grobler, Jaques and Layton, Robert and VanderPlas, Jake and Joly, Arnaud and Holt, Brian and Varoquaux, Gaël},
	year = {2013},
	pages = {108--122}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ09ORg0KVEkgIC0gQVBJIGRlc2lnbiBmb3IgbWFjaGluZSBsZWFybmluZyBzb2Z0d2FyZTogZXhwZXJpZW5jZXMgZnJvbSB0aGUgc2Npa2l0LWxlYXJuIHByb2plY3QNCkFVICAtIEJ1aXRpbmNrLCBMYXJzDQpBVSAgLSBMb3VwcGUsIEdpbGxlcw0KQVUgIC0gQmxvbmRlbCwgTWF0aGlldQ0KQVUgIC0gUGVkcmVnb3NhLCBGYWJpYW4NCkFVICAtIE11ZWxsZXIsIEFuZHJlYXMNCkFVICAtIEdyaXNlbCwgT2xpdmllcg0KQVUgIC0gTmljdWxhZSwgVmxhZA0KQVUgIC0gUHJldHRlbmhvZmVyLCBQZXRlcg0KQVUgIC0gR3JhbWZvcnQsIEFsZXhhbmRyZQ0KQVUgIC0gR3JvYmxlciwgSmFxdWVzDQpBVSAgLSBMYXl0b24sIFJvYmVydA0KQVUgIC0gVmFuZGVyUGxhcywgSmFrZQ0KQVUgIC0gSm9seSwgQXJuYXVkDQpBVSAgLSBIb2x0LCBCcmlhbg0KQVUgIC0gVmFyb3F1YXV4LCBHYcOrbA0KQzMgIC0gRUNNTCBQS0REIFdvcmtzaG9wOiBMYW5ndWFnZXMgZm9yIERhdGEgTWluaW5nIGFuZCBNYWNoaW5lIExlYXJuaW5nDQpEQSAgLSAyMDEzLy8vDQpQWSAgLSAyMDEzDQpTUCAgLSAxMDgNCkVQICAtIDEyMg0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">L. Buitinck <i>et al.</i>, “API design for machine learning software: experiences from the scikit-learn project,” in <i>ECML PKDD Workshop: Languages for Data Mining and Machine Learning</i>, 2013, pp. 108–122.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=API%20design%20for%20machine%20learning%20software%3A%20experiences%20from%20the%20scikit-learn%20project&amp;rft.btitle=ECML%20PKDD%20Workshop%3A%20Languages%20for%20Data%20Mining%20and%20Machine%20Learning&amp;rft.aufirst=Lars&amp;rft.aulast=Buitinck&amp;rft.au=Lars%20Buitinck&amp;rft.au=Gilles%20Louppe&amp;rft.au=Mathieu%20Blondel&amp;rft.au=Fabian%20Pedregosa&amp;rft.au=Andreas%20Mueller&amp;rft.au=Olivier%20Grisel&amp;rft.au=Vlad%20Niculae&amp;rft.au=Peter%20Prettenhofer&amp;rft.au=Alexandre%20Gramfort&amp;rft.au=Jaques%20Grobler&amp;rft.au=Robert%20Layton&amp;rft.au=Jake%20VanderPlas&amp;rft.au=Arnaud%20Joly&amp;rft.au=Brian%20Holt&amp;rft.au=Ga%C3%ABl%20Varoquaux&amp;rft.date=2013&amp;rft.pages=108%E2%80%93122"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms</span> <span class="containertitle"></span> (2013) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown, “<span class="doctitle"><a class="doctitle" href="http://doi.acm.org/10.1145/2487575.2487629">Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms</a></span>,” in <i>Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i>, New York, NY, USA, 2013, pp. 847–855.</div>
  </div><div class="bib-extra">00354</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2013 venue_short__KDD '13 type__conferencePaper ""</span></div><div class="bib-venue-short">KDD '13</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Many different machine learning algorithms exist; taking into account each algorithm’s hyperparameters‚ there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters‚ going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach‚ leveraging recent innovations in Bayesian optimization. Specifically‚ we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA’s standard distribution‚ spanning 2 ensemble methods‚ 10 meta-methods‚ 27 base classifiers‚ and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository‚ the KDD Cup 09‚ variants of the MNIST dataset and CIFAR-10‚ we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications‚ and hence to achieve improved performance.</div></div></div><div class="blink"><a   href="http://doi.acm.org/10.1145/2487575.2487629" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@inproceedings{thornton2013auto-weka:,
	address = {New York, NY, USA},
	series = {{KDD} '13},
	title = {Auto-{WEKA}: {Combined} {Selection} and {Hyperparameter} {Optimization} of {Classification} {Algorithms}},
	isbn = {978-1-4503-2174-7},
	url = {http://doi.acm.org/10.1145/2487575.2487629},
	doi = {10.1145/2487575.2487629},
	booktitle = {Proceedings of the 19th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Thornton, Chris and Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
	year = {2013},
	note = {00354},
	pages = {847--855}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('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');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">C. Thornton, F. Hutter, H. H. Hoos, and K. Leyton-Brown, “Auto-WEKA: Combined Selection and Hyperparameter Optimization of Classification Algorithms,” in <i>Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i>, New York, NY, USA, 2013, pp. 847–855.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1145%2F2487575.2487629&amp;rft_id=urn%3Aisbn%3A978-1-4503-2174-7&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Auto-WEKA%3A%20Combined%20Selection%20and%20Hyperparameter%20Optimization%20of%20Classification%20Algorithms&amp;rft.btitle=Proceedings%20of%20the%2019th%20ACM%20SIGKDD%20International%20Conference%20on%20Knowledge%20Discovery%20and%20Data%20Mining&amp;rft.place=New%20York%2C%20NY%2C%20USA&amp;rft.publisher=ACM&amp;rft.series=KDD%20&apos;13&amp;rft.aufirst=Chris&amp;rft.aulast=Thornton&amp;rft.au=Chris%20Thornton&amp;rft.au=Frank%20Hutter&amp;rft.au=Holger%20H.%20Hoos&amp;rft.au=Kevin%20Leyton-Brown&amp;rft.date=2013&amp;rft.pages=847%E2%80%93855&amp;rft.isbn=978-1-4503-2174-7"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">ADADELTA: An Adaptive Learning Rate Method</span> <span class="containertitle">arXiv:1212.5701 [cs]</span> (December 2012) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">M. D. Zeiler, “<span class="doctitle"><a class="doctitle" href="http://arxiv.org/abs/1212.5701">ADADELTA: An Adaptive Learning Rate Method</a></span>,” <i>arXiv:1212.5701 [cs]</i>, Dec. 2012.</div>
  </div><div class="bib-extra">01742</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2012 type__journalArticle ""</span></div><div class="bib-venue">arXiv:1212.5701 [cs]</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information‚ different model architecture choices‚ various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.</div></div></div><div class="blink"><a   href="http://arxiv.org/abs/1212.5701" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{zeiler2012adadelta:,
	title = {{ADADELTA}: {An} {Adaptive} {Learning} {Rate} {Method}},
	url = {http://arxiv.org/abs/1212.5701},
	journal = {arXiv:1212.5701 [cs]},
	author = {Zeiler, Matthew D.},
	month = dec,
	year = {2012},
	note = {01742}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gQURBREVMVEE6IEFuIEFkYXB0aXZlIExlYXJuaW5nIFJhdGUgTWV0aG9kDQpBVSAgLSBaZWlsZXIsIE1hdHRoZXcgRC4NClQyICAtIGFyWGl2OjEyMTIuNTcwMSBbY3NdDQpBQiAgLSBXZSBwcmVzZW50IGEgbm92ZWwgcGVyLWRpbWVuc2lvbiBsZWFybmluZyByYXRlIG1ldGhvZCBmb3IgZ3JhZGllbnQgZGVzY2VudCBjYWxsZWQgQURBREVMVEEuIFRoZSBtZXRob2QgZHluYW1pY2FsbHkgYWRhcHRzIG92ZXIgdGltZSB1c2luZyBvbmx5IGZpcnN0IG9yZGVyIGluZm9ybWF0aW9uIGFuZCBoYXMgbWluaW1hbCBjb21wdXRhdGlvbmFsIG92ZXJoZWFkIGJleW9uZCB2YW5pbGxhIHN0b2NoYXN0aWMgZ3JhZGllbnQgZGVzY2VudC4gVGhlIG1ldGhvZCByZXF1aXJlcyBubyBtYW51YWwgdHVuaW5nIG9mIGEgbGVhcm5pbmcgcmF0ZSBhbmQgYXBwZWFycyByb2J1c3QgdG8gbm9pc3kgZ3JhZGllbnQgaW5mb3JtYXRpb24sIGRpZmZlcmVudCBtb2RlbCBhcmNoaXRlY3R1cmUgY2hvaWNlcywgdmFyaW91cyBkYXRhIG1vZGFsaXRpZXMgYW5kIHNlbGVjdGlvbiBvZiBoeXBlcnBhcmFtZXRlcnMuIFdlIHNob3cgcHJvbWlzaW5nIHJlc3VsdHMgY29tcGFyZWQgdG8gb3RoZXIgbWV0aG9kcyBvbiB0aGUgTU5JU1QgZGlnaXQgY2xhc3NpZmljYXRpb24gdGFzayB1c2luZyBhIHNpbmdsZSBtYWNoaW5lIGFuZCBvbiBhIGxhcmdlIHNjYWxlIHZvaWNlIGRhdGFzZXQgaW4gYSBkaXN0cmlidXRlZCBjbHVzdGVyIGVudmlyb25tZW50Lg0KREEgIC0gMjAxMi8xMi8vDQpQWSAgLSAyMDEyDQpVUiAgLSBodHRwOi8vYXJ4aXYub3JnL2Ficy8xMjEyLjU3MDENCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">M. D. Zeiler, “ADADELTA: An Adaptive Learning Rate Method,” <i>arXiv:1212.5701 [cs]</i>, Dec. 2012.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=ADADELTA%3A%20An%20Adaptive%20Learning%20Rate%20Method&amp;rft.jtitle=arXiv%3A1212.5701%20%5Bcs%5D&amp;rft.aufirst=Matthew%20D.&amp;rft.aulast=Zeiler&amp;rft.au=Matthew%20D.%20Zeiler&amp;rft.date=2012-12"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">On the difficulty of training Recurrent Neural Networks</span> <span class="containertitle">arXiv:1211.5063 [cs]</span> (November 2012) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. Pascanu, T. Mikolov, and Y. Bengio, “<span class="doctitle"><a class="doctitle" href="http://arxiv.org/abs/1211.5063">On the difficulty of training Recurrent Neural Networks</a></span>,” <i>arXiv:1211.5063 [cs]</i>, Nov. 2012.</div>
  </div><div class="bib-extra">01016</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2012 type__journalArticle ""</span></div><div class="bib-venue">arXiv:1211.5063 [cs]</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Training Recurrent Neural Networks is more troublesome than feedforward ones because of the vanishing and exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to understand the fundamental issues underlying the exploding gradient problem by exploring it from an analytical‚ a geometric and a dynamical system perspective. Our analysis is used to justify the simple yet effective solution of norm clipping the exploded gradient. In the experimental section‚ the comparison between this heuristic solution and standard SGD provides empirical evidence towards our hypothesis as well as it shows that such a heuristic is required to reach state of the art results on a character prediction task and a polyphonic music prediction one.</div></div></div><div class="blink"><a   href="http://arxiv.org/abs/1211.5063" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{pascanu2012difficulty,
	title = {On the difficulty of training {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1211.5063},
	language = {en},
	journal = {arXiv:1211.5063 [cs]},
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	month = nov,
	year = {2012},
	note = {01016}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gT24gdGhlIGRpZmZpY3VsdHkgb2YgdHJhaW5pbmcgUmVjdXJyZW50IE5ldXJhbCBOZXR3b3Jrcw0KQVUgIC0gUGFzY2FudSwgUmF6dmFuDQpBVSAgLSBNaWtvbG92LCBUb21hcw0KQVUgIC0gQmVuZ2lvLCBZb3NodWENClQyICAtIGFyWGl2OjEyMTEuNTA2MyBbY3NdDQpBQiAgLSBUcmFpbmluZyBSZWN1cnJlbnQgTmV1cmFsIE5ldHdvcmtzIGlzIG1vcmUgdHJvdWJsZXNvbWUgdGhhbiBmZWVkZm9yd2FyZCBvbmVzIGJlY2F1c2Ugb2YgdGhlIHZhbmlzaGluZyBhbmQgZXhwbG9kaW5nIGdyYWRpZW50IHByb2JsZW1zIGRldGFpbGVkIGluIEJlbmdpbyBldCBhbC4gKDE5OTQpLiBJbiB0aGlzIHBhcGVyIHdlIGF0dGVtcHQgdG8gdW5kZXJzdGFuZCB0aGUgZnVuZGFtZW50YWwgaXNzdWVzIHVuZGVybHlpbmcgdGhlIGV4cGxvZGluZyBncmFkaWVudCBwcm9ibGVtIGJ5IGV4cGxvcmluZyBpdCBmcm9tIGFuIGFuYWx5dGljYWwsIGEgZ2VvbWV0cmljIGFuZCBhIGR5bmFtaWNhbCBzeXN0ZW0gcGVyc3BlY3RpdmUuIE91ciBhbmFseXNpcyBpcyB1c2VkIHRvIGp1c3RpZnkgdGhlIHNpbXBsZSB5ZXQgZWZmZWN0aXZlIHNvbHV0aW9uIG9mIG5vcm0gY2xpcHBpbmcgdGhlIGV4cGxvZGVkIGdyYWRpZW50LiBJbiB0aGUgZXhwZXJpbWVudGFsIHNlY3Rpb24sIHRoZSBjb21wYXJpc29uIGJldHdlZW4gdGhpcyBoZXVyaXN0aWMgc29sdXRpb24gYW5kIHN0YW5kYXJkIFNHRCBwcm92aWRlcyBlbXBpcmljYWwgZXZpZGVuY2UgdG93YXJkcyBvdXIgaHlwb3RoZXNpcyBhcyB3ZWxsIGFzIGl0IHNob3dzIHRoYXQgc3VjaCBhIGhldXJpc3RpYyBpcyByZXF1aXJlZCB0byByZWFjaCBzdGF0ZSBvZiB0aGUgYXJ0IHJlc3VsdHMgb24gYSBjaGFyYWN0ZXIgcHJlZGljdGlvbiB0YXNrIGFuZCBhIHBvbHlwaG9uaWMgbXVzaWMgcHJlZGljdGlvbiBvbmUuDQpEQSAgLSAyMDEyLzExLy8NClBZICAtIDIwMTINCkxBICAtIGVuDQpVUiAgLSBodHRwOi8vYXJ4aXYub3JnL2Ficy8xMjExLjUwNjMNCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. Pascanu, T. Mikolov, and Y. Bengio, “On the difficulty of training Recurrent Neural Networks,” <i>arXiv:1211.5063 [cs]</i>, Nov. 2012.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=On%20the%20difficulty%20of%20training%20Recurrent%20Neural%20Networks&amp;rft.jtitle=arXiv%3A1211.5063%20%5Bcs%5D&amp;rft.aufirst=Razvan&amp;rft.aulast=Pascanu&amp;rft.au=Razvan%20Pascanu&amp;rft.au=Tomas%20Mikolov&amp;rft.au=Yoshua%20Bengio&amp;rft.date=2012-11"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</span> <span class="containertitle">COURSERA: Neural networks for machine learning</span> (2012) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">T. Tieleman and G. Hinton, “<span class="doctitle">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude</span>,” <i>COURSERA: Neural networks for machine learning</i>, vol. 4, no. 2, pp. 26–31, 2012.</div>
  </div><div class="bib-extra">01262</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2012 type__journalArticle ""</span></div><div class="bib-venue">COURSERA: Neural networks for machine learning</div><div class="blinkitems"><div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{tieleman2012lecture,
	title = {Lecture 6.5-rmsprop: {Divide} the gradient by a running average of its recent magnitude},
	volume = {4},
	number = {2},
	journal = {COURSERA: Neural networks for machine learning},
	author = {Tieleman, Tijmen and Hinton, Geoffrey},
	year = {2012},
	note = {01262},
	pages = {26--31}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gTGVjdHVyZSA2LjUtcm1zcHJvcDogRGl2aWRlIHRoZSBncmFkaWVudCBieSBhIHJ1bm5pbmcgYXZlcmFnZSBvZiBpdHMgcmVjZW50IG1hZ25pdHVkZQ0KQVUgIC0gVGllbGVtYW4sIFRpam1lbg0KQVUgIC0gSGludG9uLCBHZW9mZnJleQ0KVDIgIC0gQ09VUlNFUkE6IE5ldXJhbCBuZXR3b3JrcyBmb3IgbWFjaGluZSBsZWFybmluZw0KREEgIC0gMjAxMi8vLw0KUFkgIC0gMjAxMg0KVkwgIC0gNA0KSVMgIC0gMg0KU1AgIC0gMjYNCkVQICAtIDMxDQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">T. Tieleman and G. Hinton, “Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude,” <i>COURSERA: Neural networks for machine learning</i>, vol. 4, no. 2, pp. 26–31, 2012.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Lecture%206.5-rmsprop%3A%20Divide%20the%20gradient%20by%20a%20running%20average%20of%20its%20recent%20magnitude&amp;rft.jtitle=COURSERA%3A%20Neural%20networks%20for%20machine%20learning&amp;rft.volume=4&amp;rft.issue=2&amp;rft.aufirst=Tijmen&amp;rft.aulast=Tieleman&amp;rft.au=Tijmen%20Tieleman&amp;rft.au=Geoffrey%20Hinton&amp;rft.date=2012&amp;rft.pages=26%E2%80%9331"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Sequential Model-Based Optimization for General Algorithm Configuration</span> <span class="containertitle"></span> (2011) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">F. Hutter, H. H. Hoos, and K. Leyton-Brown, “<span class="doctitle"><a class="doctitle" href="http://link.springer.com/10.1007/978-3-642-25566-3_40">Sequential Model-Based Optimization for General Algorithm Configuration</a></span>,” in <i>Learning and Intelligent Optimization</i>, vol. 6683, C. A. C. Coello, Ed. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011, pp. 507–523.</div>
  </div><div class="bib-extra">DOI: 10.1007/978-3-642-25566-3_40</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2011 type__bookSection ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However‚ manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently‚ automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however‚ this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper‚ we extend this paradigm for the first time to general algorithm configuration problems‚ allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT)‚ as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments‚ our procedure yielded state-of-the-art performance‚ and in many cases outperformed the previous best configuration approach.</div></div></div><div class="blink"><a   href="http://link.springer.com/10.1007/978-3-642-25566-3_40" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@incollection{hutter2011sequential,
	address = {Berlin, Heidelberg},
	title = {Sequential {Model}-{Based} {Optimization} for {General} {Algorithm} {Configuration}},
	volume = {6683},
	isbn = {978-3-642-25565-6 978-3-642-25566-3},
	url = {http://link.springer.com/10.1007/978-3-642-25566-3_40},
	language = {en},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer Berlin Heidelberg},
	author = {Hutter, Frank and Hoos, Holger H. and Leyton-Brown, Kevin},
	editor = {Coello, Carlos A. Coello},
	year = {2011},
	doi = {10.1007/978-3-642-25566-3_40},
	pages = {507--523}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('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');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">F. Hutter, H. H. Hoos, and K. Leyton-Brown, “Sequential Model-Based Optimization for General Algorithm Configuration,” in <i>Learning and Intelligent Optimization</i>, vol. 6683, C. A. C. Coello, Ed. Berlin, Heidelberg: Springer Berlin Heidelberg, 2011, pp. 507–523.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-3-642-25565-6%20978-3-642-25566-3&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Sequential%20Model-Based%20Optimization%20for%20General%20Algorithm%20Configuration&amp;rft.btitle=Learning%20and%20Intelligent%20Optimization&amp;rft.place=Berlin%2C%20Heidelberg&amp;rft.publisher=Springer%20Berlin%20Heidelberg&amp;rft.aufirst=Frank&amp;rft.aulast=Hutter&amp;rft.au=Frank%20Hutter&amp;rft.au=Holger%20H.%20Hoos&amp;rft.au=Kevin%20Leyton-Brown&amp;rft.au=Carlos%20A.%20Coello%20Coello&amp;rft.date=2011&amp;rft.pages=507%E2%80%93523&amp;rft.isbn=978-3-642-25565-6%20978-3-642-25566-3"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Deep Sparse Rectifier Neural Networks</span> <span class="containertitle"></span> (2011) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">X. Glorot, A. Bordes, and Y. Bengio, “<span class="doctitle">Deep Sparse Rectifier Neural Networks</span>,” p. 9, 2011.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2011 type__journalArticle ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons‚ the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero‚ creating sparse representations with true zeros‚ which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data‚ deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence‚ these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks‚ and closing the performance gap between neural networks learnt with and without unsupervised pre-training.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{glorot2011deep,
	title = {Deep {Sparse} {Rectifier} {Neural} {Networks}},
	language = {en},
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	year = {2011},
	pages = {9}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gRGVlcCBTcGFyc2UgUmVjdGlmaWVyIE5ldXJhbCBOZXR3b3Jrcw0KQVUgIC0gR2xvcm90LCBYYXZpZXINCkFVICAtIEJvcmRlcywgQW50b2luZQ0KQVUgIC0gQmVuZ2lvLCBZb3NodWENCkFCICAtIFdoaWxlIGxvZ2lzdGljIHNpZ21vaWQgbmV1cm9ucyBhcmUgbW9yZSBiaW9sb2dpY2FsbHkgcGxhdXNpYmxlIHRoYW4gaHlwZXJib2xpYyB0YW5nZW50IG5ldXJvbnMsIHRoZSBsYXR0ZXIgd29yayBiZXR0ZXIgZm9yIHRyYWluaW5nIG11bHRpLWxheWVyIG5ldXJhbCBuZXR3b3Jrcy4gVGhpcyBwYXBlciBzaG93cyB0aGF0IHJlY3RpZnlpbmcgbmV1cm9ucyBhcmUgYW4gZXZlbiBiZXR0ZXIgbW9kZWwgb2YgYmlvbG9naWNhbCBuZXVyb25zIGFuZCB5aWVsZCBlcXVhbCBvciBiZXR0ZXIgcGVyZm9ybWFuY2UgdGhhbiBoeXBlcmJvbGljIHRhbmdlbnQgbmV0d29ya3MgaW4gc3BpdGUgb2YgdGhlIGhhcmQgbm9uLWxpbmVhcml0eSBhbmQgbm9uLWRpZmZlcmVudGlhYmlsaXR5IGF0IHplcm8sIGNyZWF0aW5nIHNwYXJzZSByZXByZXNlbnRhdGlvbnMgd2l0aCB0cnVlIHplcm9zLCB3aGljaCBzZWVtIHJlbWFya2FibHkgc3VpdGFibGUgZm9yIG5hdHVyYWxseSBzcGFyc2UgZGF0YS4gRXZlbiB0aG91Z2ggdGhleSBjYW4gdGFrZSBhZHZhbnRhZ2Ugb2Ygc2VtaS1zdXBlcnZpc2VkIHNldHVwcyB3aXRoIGV4dHJhLXVubGFiZWxlZCBkYXRhLCBkZWVwIHJlY3RpZmllciBuZXR3b3JrcyBjYW4gcmVhY2ggdGhlaXIgYmVzdCBwZXJmb3JtYW5jZSB3aXRob3V0IHJlcXVpcmluZyBhbnkgdW5zdXBlcnZpc2VkIHByZS10cmFpbmluZyBvbiBwdXJlbHkgc3VwZXJ2aXNlZCB0YXNrcyB3aXRoIGxhcmdlIGxhYmVsZWQgZGF0YXNldHMuIEhlbmNlLCB0aGVzZSByZXN1bHRzIGNhbiBiZSBzZWVuIGFzIGEgbmV3IG1pbGVzdG9uZSBpbiB0aGUgYXR0ZW1wdHMgYXQgdW5kZXJzdGFuZGluZyB0aGUgZGlmZmljdWx0eSBpbiB0cmFpbmluZyBkZWVwIGJ1dCBwdXJlbHkgc3VwZXJ2aXNlZCBuZXVyYWwgbmV0d29ya3MsIGFuZCBjbG9zaW5nIHRoZSBwZXJmb3JtYW5jZSBnYXAgYmV0d2VlbiBuZXVyYWwgbmV0d29ya3MgbGVhcm50IHdpdGggYW5kIHdpdGhvdXQgdW5zdXBlcnZpc2VkIHByZS10cmFpbmluZy4NCkRBICAtIDIwMTEvLy8NClBZICAtIDIwMTENClNQICAtIDkNCkxBICAtIGVuDQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">X. Glorot, A. Bordes, and Y. Bengio, “Deep Sparse Rectifier Neural Networks,” p. 9, 2011.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Deep%20Sparse%20Rectifier%20Neural%20Networks&amp;rft.aufirst=Xavier&amp;rft.aulast=Glorot&amp;rft.au=Xavier%20Glorot&amp;rft.au=Antoine%20Bordes&amp;rft.au=Yoshua%20Bengio&amp;rft.date=2011&amp;rft.pages=9"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</span> <span class="containertitle">Journal of Machine Learning Research</span> (2011) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. Duchi, E. Hazan, and Y. Singer, “<span class="doctitle"><a class="doctitle" href="http://www.jmlr.org/papers/v12/duchi11a.html">Adaptive Subgradient Methods for Online Learning and Stochastic Optimization</a></span>,” <i>Journal of Machine Learning Research</i>, vol. 12, no. Jul, pp. 2121–2159, 2011.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2011 type__journalArticle ""</span></div><div class="bib-venue">Journal of Machine Learning Research</div><div class="blinkitems"><div><div class="blink"><a   href="http://www.jmlr.org/papers/v12/duchi11a.html" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{duchi2011adaptive,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	volume = {12},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v12/duchi11a.html},
	number = {Jul},
	journal = {Journal of Machine Learning Research},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	year = {2011},
	pages = {2121--2159}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gQWRhcHRpdmUgU3ViZ3JhZGllbnQgTWV0aG9kcyBmb3IgT25saW5lIExlYXJuaW5nIGFuZCBTdG9jaGFzdGljIE9wdGltaXphdGlvbg0KQVUgIC0gRHVjaGksIEpvaG4NCkFVICAtIEhhemFuLCBFbGFkDQpBVSAgLSBTaW5nZXIsIFlvcmFtDQpUMiAgLSBKb3VybmFsIG9mIE1hY2hpbmUgTGVhcm5pbmcgUmVzZWFyY2gNCkRBICAtIDIwMTEvLy8NClBZICAtIDIwMTENClZMICAtIDEyDQpJUyAgLSBKdWwNClNQICAtIDIxMjENCkVQICAtIDIxNTkNClNOICAtIElTU04gMTUzMy03OTI4DQpVUiAgLSBodHRwOi8vd3d3LmptbHIub3JnL3BhcGVycy92MTIvZHVjaGkxMWEuaHRtbA0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. Duchi, E. Hazan, and Y. Singer, “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization,” <i>Journal of Machine Learning Research</i>, vol. 12, no. Jul, pp. 2121–2159, 2011.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Adaptive%20Subgradient%20Methods%20for%20Online%20Learning%20and%20Stochastic%20Optimization&amp;rft.jtitle=Journal%20of%20Machine%20Learning%20Research&amp;rft.volume=12&amp;rft.issue=Jul&amp;rft.aufirst=John&amp;rft.aulast=Duchi&amp;rft.au=John%20Duchi&amp;rft.au=Elad%20Hazan&amp;rft.au=Yoram%20Singer&amp;rft.date=2011&amp;rft.pages=2121%E2%80%932159&amp;rft.issn=ISSN%201533-7928"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Akaike's Information Criterion</span> <span class="containertitle"></span> (2011) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">H. Akaike, “<span class="doctitle"><a class="doctitle" href="https://link.springer.com/referenceworkentry/10.1007/978-3- 642-04898-2_110">Akaike’s Information Criterion</a></span>,” in <i>International Encyclopedia of Statistical Science</i>, Springer, Berlin, Heidelberg, 2011, pp. 25–25.</div>
  </div><div class="bib-extra">00093 
DOI: 10.1007/978-3-642-04898-2_110</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2011 type__bookSection ""</span></div><div class="blinkitems"><div><div class="blink"><a   href="https://link.springer.com/referenceworkentry/10.1007/978-3- 642-04898-2_110" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@incollection{akaike2011akaikes,
	title = {Akaike's {Information} {Criterion}},
	url = {https://link.springer.com/referenceworkentry/10.1007/978-3- 642-04898-2_110},
	language = {en},
	booktitle = {International {Encyclopedia} of {Statistical} {Science}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Akaike, Hirotugu},
	year = {2011},
	doi = {10.1007/978-3-642-04898-2_110},
	note = {00093 },
	pages = {25--25}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ0hBUA0KVEkgIC0gQWthaWtlJ3MgSW5mb3JtYXRpb24gQ3JpdGVyaW9uDQpBVSAgLSBBa2Fpa2UsIEhpcm90dWd1DQpUMiAgLSBJbnRlcm5hdGlvbmFsIEVuY3ljbG9wZWRpYSBvZiBTdGF0aXN0aWNhbCBTY2llbmNlDQpEQSAgLSAyMDExLy8vDQpQWSAgLSAyMDExDQpTUCAgLSAyNQ0KRVAgIC0gMjUNCkxBICAtIGVuDQpQQiAgLSBTcHJpbmdlciwgQmVybGluLCBIZWlkZWxiZXJnDQpVUiAgLSBodHRwczovL2xpbmsuc3ByaW5nZXIuY29tL3JlZmVyZW5jZXdvcmtlbnRyeS8xMC4xMDA3Lzk3OC0zLSA2NDItMDQ4OTgtMl8xMTANCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">H. Akaike, “Akaike’s Information Criterion,” in <i>International Encyclopedia of Statistical Science</i>, Springer, Berlin, Heidelberg, 2011, pp. 25–25.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Akaike&apos;s%20Information%20Criterion&amp;rft.btitle=International%20Encyclopedia%20of%20Statistical%20Science&amp;rft.publisher=Springer%2C%20Berlin%2C%20Heidelberg&amp;rft.aufirst=Hirotugu&amp;rft.aulast=Akaike&amp;rft.au=Hirotugu%20Akaike&amp;rft.date=2011&amp;rft.pages=25%E2%80%9325"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">The effects of variable stationarity in a financial time-series on Artificial Neural Networks</span> <span class="containertitle"></span> (April 2011) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">M. Butler and D. Kazakov, “<span class="doctitle">The effects of variable stationarity in a financial time-series on Artificial Neural Networks</span>,” in <i>2011 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CIFEr)</i>, 2011, pp. 1–8.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2011 type__conferencePaper ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">This study investigates the characteristic of non-stationarity in a financial time-series and its effect on the learning process for Artificial Neural Networks (ANN). It is motivated by previous work where it was shown that non-stationarity is not static within a financial time series but quite variable in nature. Initially unit-root tests were performed to isolate segments that were stationary or non-stationary at a pre-determined significance level and then various tests were conducted based on forecasting accuracy. The hypothesis of this research is that when using the de-trended/original observations from the time series the trend/level stationary segments should produce lower error measures and when the series are differenced the difference stationary (non-stationary) segments should have lower error. The results to date reveal that the effects of variable stationarity on learning with ANNs are a function of forecasting time-horizon‚ strength of the linear-time trend‚ sample size and persistence of the stationary process.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@inproceedings{butler2011effects,
	title = {The effects of variable stationarity in a financial time-series on {Artificial} {Neural} {Networks}},
	doi = {10.1109/CIFER.2011.5953557},
	booktitle = {2011 {IEEE} {Symposium} on {Computational} {Intelligence} for {Financial} {Engineering} and {Economics} ({CIFEr})},
	author = {Butler, M. and Kazakov, D.},
	month = apr,
	year = {2011},
	note = {00000},
	pages = {1--8}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ09ORg0KVEkgIC0gVGhlIGVmZmVjdHMgb2YgdmFyaWFibGUgc3RhdGlvbmFyaXR5IGluIGEgZmluYW5jaWFsIHRpbWUtc2VyaWVzIG9uIEFydGlmaWNpYWwgTmV1cmFsIE5ldHdvcmtzDQpBVSAgLSBCdXRsZXIsIE0uDQpBVSAgLSBLYXpha292LCBELg0KQUIgIC0gVGhpcyBzdHVkeSBpbnZlc3RpZ2F0ZXMgdGhlIGNoYXJhY3RlcmlzdGljIG9mIG5vbi1zdGF0aW9uYXJpdHkgaW4gYSBmaW5hbmNpYWwgdGltZS1zZXJpZXMgYW5kIGl0cyBlZmZlY3Qgb24gdGhlIGxlYXJuaW5nIHByb2Nlc3MgZm9yIEFydGlmaWNpYWwgTmV1cmFsIE5ldHdvcmtzIChBTk4pLiBJdCBpcyBtb3RpdmF0ZWQgYnkgcHJldmlvdXMgd29yayB3aGVyZSBpdCB3YXMgc2hvd24gdGhhdCBub24tc3RhdGlvbmFyaXR5IGlzIG5vdCBzdGF0aWMgd2l0aGluIGEgZmluYW5jaWFsIHRpbWUgc2VyaWVzIGJ1dCBxdWl0ZSB2YXJpYWJsZSBpbiBuYXR1cmUuIEluaXRpYWxseSB1bml0LXJvb3QgdGVzdHMgd2VyZSBwZXJmb3JtZWQgdG8gaXNvbGF0ZSBzZWdtZW50cyB0aGF0IHdlcmUgc3RhdGlvbmFyeSBvciBub24tc3RhdGlvbmFyeSBhdCBhIHByZS1kZXRlcm1pbmVkIHNpZ25pZmljYW5jZSBsZXZlbCBhbmQgdGhlbiB2YXJpb3VzIHRlc3RzIHdlcmUgY29uZHVjdGVkIGJhc2VkIG9uIGZvcmVjYXN0aW5nIGFjY3VyYWN5LiBUaGUgaHlwb3RoZXNpcyBvZiB0aGlzIHJlc2VhcmNoIGlzIHRoYXQgd2hlbiB1c2luZyB0aGUgZGUtdHJlbmRlZC9vcmlnaW5hbCBvYnNlcnZhdGlvbnMgZnJvbSB0aGUgdGltZSBzZXJpZXMgdGhlIHRyZW5kL2xldmVsIHN0YXRpb25hcnkgc2VnbWVudHMgc2hvdWxkIHByb2R1Y2UgbG93ZXIgZXJyb3IgbWVhc3VyZXMgYW5kIHdoZW4gdGhlIHNlcmllcyBhcmUgZGlmZmVyZW5jZWQgdGhlIGRpZmZlcmVuY2Ugc3RhdGlvbmFyeSAobm9uLXN0YXRpb25hcnkpIHNlZ21lbnRzIHNob3VsZCBoYXZlIGxvd2VyIGVycm9yLiBUaGUgcmVzdWx0cyB0byBkYXRlIHJldmVhbCB0aGF0IHRoZSBlZmZlY3RzIG9mIHZhcmlhYmxlIHN0YXRpb25hcml0eSBvbiBsZWFybmluZyB3aXRoIEFOTnMgYXJlIGEgZnVuY3Rpb24gb2YgZm9yZWNhc3RpbmcgdGltZS1ob3Jpem9uLCBzdHJlbmd0aCBvZiB0aGUgbGluZWFyLXRpbWUgdHJlbmQsIHNhbXBsZSBzaXplIGFuZCBwZXJzaXN0ZW5jZSBvZiB0aGUgc3RhdGlvbmFyeSBwcm9jZXNzLg0KQzMgIC0gMjAxMSBJRUVFIFN5bXBvc2l1bSBvbiBDb21wdXRhdGlvbmFsIEludGVsbGlnZW5jZSBmb3IgRmluYW5jaWFsIEVuZ2luZWVyaW5nIGFuZCBFY29ub21pY3MgKENJRkVyKQ0KREEgIC0gMjAxMS8wNC8vDQpQWSAgLSAyMDExDQpETyAgLSAxMC4xMTA5L0NJRkVSLjIwMTEuNTk1MzU1Nw0KU1AgIC0gMQ0KRVAgIC0gOA0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">M. Butler and D. Kazakov, “The effects of variable stationarity in a financial time-series on Artificial Neural Networks,” in <i>2011 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics (CIFEr)</i>, 2011, pp. 1–8.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FCIFER.2011.5953557&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=The%20effects%20of%20variable%20stationarity%20in%20a%20financial%20time-series%20on%20Artificial%20Neural%20Networks&amp;rft.btitle=2011%20IEEE%20Symposium%20on%20Computational%20Intelligence%20for%20Financial%20Engineering%20and%20Economics%20(CIFEr)&amp;rft.aufirst=M.&amp;rft.aulast=Butler&amp;rft.au=M.%20Butler&amp;rft.au=D.%20Kazakov&amp;rft.date=2011-04&amp;rft.pages=1%E2%80%938"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">A Survey on Transfer Learning</span> <span class="containertitle">IEEE Transactions on Knowledge and Data Engineering</span> (October 2010) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">S. J. Pan and Q. Yang, “<span class="doctitle">A Survey on Transfer Learning</span>,” <i>IEEE Transactions on Knowledge and Data Engineering</i>, vol. 22, no. 10, pp. 1345–1359, Oct. 2010.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2010 type__journalArticle ""</span></div><div class="bib-venue">IEEE Transactions on Knowledge and Data Engineering</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However‚ in many real-world applications‚ this assumption may not hold. For example‚ we sometimes have a classification task in one domain of interest‚ but we only have sufficient training data in another domain of interest‚ where the latter data may be in a different feature space or follow a different data distribution. In such cases‚ knowledge transfer‚ if done successfully‚ would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years‚ transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification‚ regression‚ and clustering problems. In this survey‚ we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation‚ multitask learning and sample selection bias‚ as well as covariate shift. We also explore some potential future issues in transfer learning research.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{pan2010survey,
	title = {A {Survey} on {Transfer} {Learning}},
	volume = {22},
	issn = {1041-4347},
	doi = {10.1109/TKDE.2009.191},
	number = {10},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Pan, S. J. and Yang, Q.},
	month = oct,
	year = {2010},
	pages = {1345--1359}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gQSBTdXJ2ZXkgb24gVHJhbnNmZXIgTGVhcm5pbmcNCkFVICAtIFBhbiwgUy4gSi4NCkFVICAtIFlhbmcsIFEuDQpUMiAgLSBJRUVFIFRyYW5zYWN0aW9ucyBvbiBLbm93bGVkZ2UgYW5kIERhdGEgRW5naW5lZXJpbmcNCkFCICAtIEEgbWFqb3IgYXNzdW1wdGlvbiBpbiBtYW55IG1hY2hpbmUgbGVhcm5pbmcgYW5kIGRhdGEgbWluaW5nIGFsZ29yaXRobXMgaXMgdGhhdCB0aGUgdHJhaW5pbmcgYW5kIGZ1dHVyZSBkYXRhIG11c3QgYmUgaW4gdGhlIHNhbWUgZmVhdHVyZSBzcGFjZSBhbmQgaGF2ZSB0aGUgc2FtZSBkaXN0cmlidXRpb24uIEhvd2V2ZXIsIGluIG1hbnkgcmVhbC13b3JsZCBhcHBsaWNhdGlvbnMsIHRoaXMgYXNzdW1wdGlvbiBtYXkgbm90IGhvbGQuIEZvciBleGFtcGxlLCB3ZSBzb21ldGltZXMgaGF2ZSBhIGNsYXNzaWZpY2F0aW9uIHRhc2sgaW4gb25lIGRvbWFpbiBvZiBpbnRlcmVzdCwgYnV0IHdlIG9ubHkgaGF2ZSBzdWZmaWNpZW50IHRyYWluaW5nIGRhdGEgaW4gYW5vdGhlciBkb21haW4gb2YgaW50ZXJlc3QsIHdoZXJlIHRoZSBsYXR0ZXIgZGF0YSBtYXkgYmUgaW4gYSBkaWZmZXJlbnQgZmVhdHVyZSBzcGFjZSBvciBmb2xsb3cgYSBkaWZmZXJlbnQgZGF0YSBkaXN0cmlidXRpb24uIEluIHN1Y2ggY2FzZXMsIGtub3dsZWRnZSB0cmFuc2ZlciwgaWYgZG9uZSBzdWNjZXNzZnVsbHksIHdvdWxkIGdyZWF0bHkgaW1wcm92ZSB0aGUgcGVyZm9ybWFuY2Ugb2YgbGVhcm5pbmcgYnkgYXZvaWRpbmcgbXVjaCBleHBlbnNpdmUgZGF0YS1sYWJlbGluZyBlZmZvcnRzLiBJbiByZWNlbnQgeWVhcnMsIHRyYW5zZmVyIGxlYXJuaW5nIGhhcyBlbWVyZ2VkIGFzIGEgbmV3IGxlYXJuaW5nIGZyYW1ld29yayB0byBhZGRyZXNzIHRoaXMgcHJvYmxlbS4gVGhpcyBzdXJ2ZXkgZm9jdXNlcyBvbiBjYXRlZ29yaXppbmcgYW5kIHJldmlld2luZyB0aGUgY3VycmVudCBwcm9ncmVzcyBvbiB0cmFuc2ZlciBsZWFybmluZyBmb3IgY2xhc3NpZmljYXRpb24sIHJlZ3Jlc3Npb24sIGFuZCBjbHVzdGVyaW5nIHByb2JsZW1zLiBJbiB0aGlzIHN1cnZleSwgd2UgZGlzY3VzcyB0aGUgcmVsYXRpb25zaGlwIGJldHdlZW4gdHJhbnNmZXIgbGVhcm5pbmcgYW5kIG90aGVyIHJlbGF0ZWQgbWFjaGluZSBsZWFybmluZyB0ZWNobmlxdWVzIHN1Y2ggYXMgZG9tYWluIGFkYXB0YXRpb24sIG11bHRpdGFzayBsZWFybmluZyBhbmQgc2FtcGxlIHNlbGVjdGlvbiBiaWFzLCBhcyB3ZWxsIGFzIGNvdmFyaWF0ZSBzaGlmdC4gV2UgYWxzbyBleHBsb3JlIHNvbWUgcG90ZW50aWFsIGZ1dHVyZSBpc3N1ZXMgaW4gdHJhbnNmZXIgbGVhcm5pbmcgcmVzZWFyY2guDQpEQSAgLSAyMDEwLzEwLy8NClBZICAtIDIwMTANCkRPICAtIDEwLjExMDkvVEtERS4yMDA5LjE5MQ0KVkwgIC0gMjINCklTICAtIDEwDQpTUCAgLSAxMzQ1DQpFUCAgLSAxMzU5DQpTTiAgLSAxMDQxLTQzNDcNCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">S. J. Pan and Q. Yang, “A Survey on Transfer Learning,” <i>IEEE Transactions on Knowledge and Data Engineering</i>, vol. 22, no. 10, pp. 1345–1359, Oct. 2010.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FTKDE.2009.191&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=A%20Survey%20on%20Transfer%20Learning&amp;rft.jtitle=IEEE%20Transactions%20on%20Knowledge%20and%20Data%20Engineering&amp;rft.volume=22&amp;rft.issue=10&amp;rft.aufirst=S.%20J.&amp;rft.aulast=Pan&amp;rft.au=S.%20J.%20Pan&amp;rft.au=Q.%20Yang&amp;rft.date=2010-10&amp;rft.pages=1345%E2%80%931359&amp;rft.issn=1041-4347"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Statsmodels: Econometric and statistical modeling with python</span> <span class="containertitle"></span> (2010) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">S. Seabold and J. Perktold, “<span class="doctitle">Statsmodels: Econometric and statistical modeling with python</span>,” in <i>9th Python in Science Conference</i>, 2010.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2010 type__conferencePaper ""</span></div><div class="blinkitems"><div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@inproceedings{seabold2010statsmodels:,
	title = {Statsmodels: {Econometric} and statistical modeling with python},
	booktitle = {9th {Python} in {Science} {Conference}},
	author = {Seabold, Skipper and Perktold, Josef},
	year = {2010},
	note = {00000}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ09ORg0KVEkgIC0gU3RhdHNtb2RlbHM6IEVjb25vbWV0cmljIGFuZCBzdGF0aXN0aWNhbCBtb2RlbGluZyB3aXRoIHB5dGhvbg0KQVUgIC0gU2VhYm9sZCwgU2tpcHBlcg0KQVUgIC0gUGVya3RvbGQsIEpvc2VmDQpDMyAgLSA5dGggUHl0aG9uIGluIFNjaWVuY2UgQ29uZmVyZW5jZQ0KREEgIC0gMjAxMC8vLw0KUFkgIC0gMjAxMA0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">S. Seabold and J. Perktold, “Statsmodels: Econometric and statistical modeling with python,” in <i>9th Python in Science Conference</i>, 2010.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Statsmodels%3A%20Econometric%20and%20statistical%20modeling%20with%20python&amp;rft.btitle=9th%20Python%20in%20Science%20Conference&amp;rft.aufirst=Skipper&amp;rft.aulast=Seabold&amp;rft.au=Skipper%20Seabold&amp;rft.au=Josef%20Perktold&amp;rft.date=2010"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Data Structures for Statistical Computing in Python</span> <span class="containertitle"></span> (2010) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">W. McKinney, “<span class="doctitle"><a class="doctitle" href="http://conference.scipy.org/proceedings/scipy2010/ mckinney.html">Data Structures for Statistical Computing in Python</a></span>,” 2010, pp. 51–56.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2010 type__conferencePaper ""</span></div><div class="blinkitems"><div><div class="blink"><a   href="http://conference.scipy.org/proceedings/scipy2010/ mckinney.html" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@inproceedings{mckinney2010data,
	title = {Data {Structures} for {Statistical} {Computing} in {Python}},
	url = {http://conference.scipy.org/proceedings/scipy2010/ mckinney.html},
	author = {McKinney, Wes},
	year = {2010},
	note = {00000},
	pages = {51--56}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ09ORg0KVEkgIC0gRGF0YSBTdHJ1Y3R1cmVzIGZvciBTdGF0aXN0aWNhbCBDb21wdXRpbmcgaW4gUHl0aG9uDQpBVSAgLSBNY0tpbm5leSwgV2VzDQpEQSAgLSAyMDEwLy8vDQpQWSAgLSAyMDEwDQpTUCAgLSA1MQ0KRVAgIC0gNTYNClVSICAtIGh0dHA6Ly9jb25mZXJlbmNlLnNjaXB5Lm9yZy9wcm9jZWVkaW5ncy9zY2lweTIwMTAvIG1ja2lubmV5Lmh0bWwNCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">W. McKinney, “Data Structures for Statistical Computing in Python,” 2010, pp. 51–56.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Data%20Structures%20for%20Statistical%20Computing%20in%20Python&amp;rft.aufirst=Wes&amp;rft.aulast=McKinney&amp;rft.au=Wes%20McKinney&amp;rft.date=2010&amp;rft.pages=51%E2%80%9356"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Particle swarm optimization for parameter determination and feature selection of support vector machines</span> <span class="containertitle">Expert Systems with Applications</span> (November 2008) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">S.-W. Lin, K.-C. Ying, S.-C. Chen, and Z.-J. Lee, “<span class="doctitle"><a class="doctitle" href="http://www.sciencedirect.com/science/article/pii/ S0957417407003752">Particle swarm optimization for parameter determination and feature selection of support vector machines</a></span>,” <i>Expert Systems with Applications</i>, vol. 35, no. 4, pp. 1817–1824, Nov. 2008.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2008 type__journalArticle ""</span></div><div class="bib-venue">Expert Systems with Applications</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Support vector machine (SVM) is a popular pattern classification method with many diverse applications. Kernel parameter setting in the SVM training procedure‚ along with the feature selection‚ significantly influences the classification accuracy. This study simultaneously determines the parameter values while discovering a subset of features‚ without reducing SVM classification accuracy. A particle swarm optimization (PSO) based approach for parameter determination and feature selection of the SVM‚ termed PSO+SVM‚ is developed. Several public datasets are employed to calculate the classification accuracy rate in order to evaluate the developed PSO+SVM approach. The developed approach was compared with grid search‚ which is a conventional method of searching parameter values‚ and other approaches. Experimental results demonstrate that the classification accuracy rates of the developed approach surpass those of grid search and many other approaches‚ and that the developed PSO+SVM approach has a similar result to GA+SVM. Therefore‚ the PSO+SVM approach is valuable for parameter determination and feature selection in an SVM.</div></div></div><div class="blink"><a   href="http://www.sciencedirect.com/science/article/pii/ S0957417407003752" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{lin2008particle,
	title = {Particle swarm optimization for parameter determination and feature selection of support vector machines},
	volume = {35},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/ S0957417407003752},
	doi = {10.1016/j.eswa.2007.08.088},
	number = {4},
	journal = {Expert Systems with Applications},
	author = {Lin, Shih-Wei and Ying, Kuo-Ching and Chen, Shih-Chieh and Lee, Zne-Jung},
	month = nov,
	year = {2008},
	pages = {1817--1824}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('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');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">S.-W. Lin, K.-C. Ying, S.-C. Chen, and Z.-J. Lee, “Particle swarm optimization for parameter determination and feature selection of support vector machines,” <i>Expert Systems with Applications</i>, vol. 35, no. 4, pp. 1817–1824, Nov. 2008.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2Fj.eswa.2007.08.088&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Particle%20swarm%20optimization%20for%20parameter%20determination%20and%20feature%20selection%20of%20support%20vector%20machines&amp;rft.jtitle=Expert%20Systems%20with%20Applications&amp;rft.volume=35&amp;rft.issue=4&amp;rft.aufirst=Shih-Wei&amp;rft.aulast=Lin&amp;rft.au=Shih-Wei%20Lin&amp;rft.au=Kuo-Ching%20Ying&amp;rft.au=Shih-Chieh%20Chen&amp;rft.au=Zne-Jung%20Lee&amp;rft.date=2008-11&amp;rft.pages=1817%E2%80%931824&amp;rft.issn=0957-4174"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Equation discovery for macroeconomic modelling</span> <span class="containertitle"></span> (2008) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. Kazakov and T. Tsenova, “<span class="doctitle">Equation discovery for macroeconomic modelling</span>,” 2008.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2008 type__journalArticle ""</span></div><div class="blinkitems"><div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{kazakov2008equation,
	title = {Equation discovery for macroeconomic modelling},
	author = {Kazakov, Dimitar and Tsenova, Tsvetomira},
	year = {2008}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gRXF1YXRpb24gZGlzY292ZXJ5IGZvciBtYWNyb2Vjb25vbWljIG1vZGVsbGluZw0KQVUgIC0gS2F6YWtvdiwgRGltaXRhcg0KQVUgIC0gVHNlbm92YSwgVHN2ZXRvbWlyYQ0KREEgIC0gMjAwOC8vLw0KUFkgIC0gMjAwOA0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. Kazakov and T. Tsenova, “Equation discovery for macroeconomic modelling,” 2008.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Equation%20discovery%20for%20macroeconomic%20modelling&amp;rft.aufirst=Dimitar&amp;rft.aulast=Kazakov&amp;rft.au=Dimitar%20Kazakov&amp;rft.au=Tsvetomira%20Tsenova&amp;rft.date=2008"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Bayesian Information Criteria</span> <span class="containertitle"></span> (2008) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">“<span class="doctitle"><a class="doctitle" href="https://link.springer.com/chapter/10.1007/978-0-387-71887- 3_9">Bayesian Information Criteria</a></span>,” in <i>Information Criteria and Statistical Modeling</i>, Springer, New York, NY, 2008, pp. 211–237.</div>
  </div><div class="bib-extra">00640 
DOI: 10.1007/978-0-387-71887-3_9</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2008 type__bookSection ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">This chapter considers model selection and evaluation criteria from a Bayesian point of view. A general framework for constructing the Bayesian information criterion (BIC) is described. The BIC is also extended such that it can be applied to the evaluation of models estimated by regularization. Section 9.2 presents Akaike’s Bayesian information criterion (ABIC) developed for the evaluation of Bayesian models having prior distributions with hyperparameters. In the latter half of this chapter‚ we consider information criteria for the evaluation of predictive distributions of Bayesian models. In particular‚ Section 9.3 gives examples of analytical evaluations of bias correction for linear Gaussian Bayes models. Section 9.4 describes‚ for general Bayesian models‚ how to estimate the asymptotic biases and how to perform the second-order bias correction by means of Laplace’s method for integrals.</div></div></div><div class="blink"><a   href="https://link.springer.com/chapter/10.1007/978-0-387-71887- 3_9" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@incollection{noauthor2008bayesian,
	series = {Springer {Series} in {Statistics}},
	title = {Bayesian {Information} {Criteria}},
	isbn = {978-0-387-71886-6 978-0-387-71887-3},
	url = {https://link.springer.com/chapter/10.1007/978-0-387-71887- 3_9},
	language = {en},
	booktitle = {Information {Criteria} and {Statistical} {Modeling}},
	publisher = {Springer, New York, NY},
	year = {2008},
	doi = {10.1007/978-0-387-71887-3_9},
	note = {00640 },
	pages = {211--237}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ0hBUA0KVEkgIC0gQmF5ZXNpYW4gSW5mb3JtYXRpb24gQ3JpdGVyaWENClQyICAtIEluZm9ybWF0aW9uIENyaXRlcmlhIGFuZCBTdGF0aXN0aWNhbCBNb2RlbGluZw0KVDMgIC0gU3ByaW5nZXIgU2VyaWVzIGluIFN0YXRpc3RpY3MNCkFCICAtIFRoaXMgY2hhcHRlciBjb25zaWRlcnMgbW9kZWwgc2VsZWN0aW9uIGFuZCBldmFsdWF0aW9uIGNyaXRlcmlhIGZyb20gYSBCYXllc2lhbiBwb2ludCBvZiB2aWV3LiBBIGdlbmVyYWwgZnJhbWV3b3JrIGZvciBjb25zdHJ1Y3RpbmcgdGhlIEJheWVzaWFuIGluZm9ybWF0aW9uIGNyaXRlcmlvbiAoQklDKSBpcyBkZXNjcmliZWQuIFRoZSBCSUMgaXMgYWxzbyBleHRlbmRlZCBzdWNoIHRoYXQgaXQgY2FuIGJlIGFwcGxpZWQgdG8gdGhlIGV2YWx1YXRpb24gb2YgbW9kZWxzIGVzdGltYXRlZCBieSByZWd1bGFyaXphdGlvbi4gU2VjdGlvbiA5LjIgcHJlc2VudHMgQWthaWtl4oCZcyBCYXllc2lhbiBpbmZvcm1hdGlvbiBjcml0ZXJpb24gKEFCSUMpIGRldmVsb3BlZCBmb3IgdGhlIGV2YWx1YXRpb24gb2YgQmF5ZXNpYW4gbW9kZWxzIGhhdmluZyBwcmlvciBkaXN0cmlidXRpb25zIHdpdGggaHlwZXJwYXJhbWV0ZXJzLiBJbiB0aGUgbGF0dGVyIGhhbGYgb2YgdGhpcyBjaGFwdGVyLCB3ZSBjb25zaWRlciBpbmZvcm1hdGlvbiBjcml0ZXJpYSBmb3IgdGhlIGV2YWx1YXRpb24gb2YgcHJlZGljdGl2ZSBkaXN0cmlidXRpb25zIG9mIEJheWVzaWFuIG1vZGVscy4gSW4gcGFydGljdWxhciwgU2VjdGlvbiA5LjMgZ2l2ZXMgZXhhbXBsZXMgb2YgYW5hbHl0aWNhbCBldmFsdWF0aW9ucyBvZiBiaWFzIGNvcnJlY3Rpb24gZm9yIGxpbmVhciBHYXVzc2lhbiBCYXllcyBtb2RlbHMuIFNlY3Rpb24gOS40IGRlc2NyaWJlcywgZm9yIGdlbmVyYWwgQmF5ZXNpYW4gbW9kZWxzLCBob3cgdG8gZXN0aW1hdGUgdGhlIGFzeW1wdG90aWMgYmlhc2VzIGFuZCBob3cgdG8gcGVyZm9ybSB0aGUgc2Vjb25kLW9yZGVyIGJpYXMgY29ycmVjdGlvbiBieSBtZWFucyBvZiBMYXBsYWNl4oCZcyBtZXRob2QgZm9yIGludGVncmFscy4NCkRBICAtIDIwMDgvLy8NClBZICAtIDIwMDgNClNQICAtIDIxMQ0KRVAgIC0gMjM3DQpMQSAgLSBlbg0KUEIgIC0gU3ByaW5nZXIsIE5ldyBZb3JrLCBOWQ0KU04gIC0gOTc4LTAtMzg3LTcxODg2LTYgOTc4LTAtMzg3LTcxODg3LTMNClVSICAtIGh0dHBzOi8vbGluay5zcHJpbmdlci5jb20vY2hhcHRlci8xMC4xMDA3Lzk3OC0wLTM4Ny03MTg4Ny0gM185DQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">“Bayesian Information Criteria,” in <i>Information Criteria and Statistical Modeling</i>, Springer, New York, NY, 2008, pp. 211–237.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-0-387-71886-6%20978-0-387-71887-3&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Bayesian%20Information%20Criteria&amp;rft.btitle=Information%20Criteria%20and%20Statistical%20Modeling&amp;rft.publisher=Springer%2C%20New%20York%2C%20NY&amp;rft.series=Springer%20Series%20in%20Statistics&amp;rft.date=2008&amp;rft.pages=211%E2%80%93237&amp;rft.isbn=978-0-387-71886-6%20978-0-387-71887-3"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Automatic Time Series Forecasting: The forecast Package for R</span> <span class="containertitle">Journal of Statistical Software, Articles</span> (2008) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. Hyndman and Y. Khandakar, “<span class="doctitle"><a class="doctitle" href="https://www.jstatsoft.org/v027/i03">Automatic Time Series Forecasting: The forecast Package for R</a></span>,” <i>Journal of Statistical Software, Articles</i>, vol. 27, no. 3, pp. 1–22, 2008.</div>
  </div><div class="bib-extra">00002</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2008 type__journalArticle ""</span></div><div class="bib-venue">Journal of Statistical Software, Articles</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Automatic forecasts of large numbers of univariate time series are often needed in business and other contexts. We describe two automatic forecasting algorithms that have been implemented in the forecast package for R. The first is based on innovations state space models that underly exponential smoothing methods. The second is a step-wise algorithm for forecasting with ARIMA models. The algorithms are applicable to both seasonal and non-seasonal data‚ and are compared and illustrated using four real time series. We also briefly describe some of the other functionality available in the forecast package.</div></div></div><div class="blink"><a   href="https://www.jstatsoft.org/v027/i03" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{hyndman2008automatic,
	title = {Automatic {Time} {Series} {Forecasting}: {The} forecast {Package} for {R}},
	volume = {27},
	issn = {1548-7660},
	url = {https://www.jstatsoft.org/v027/i03},
	doi = {10.18637/jss.v027.i03},
	number = {3},
	journal = {Journal of Statistical Software, Articles},
	author = {Hyndman, Rob and Khandakar, Yeasmin},
	year = {2008},
	note = {00002},
	pages = {1--22}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gQXV0b21hdGljIFRpbWUgU2VyaWVzIEZvcmVjYXN0aW5nOiBUaGUgZm9yZWNhc3QgUGFja2FnZSBmb3IgUg0KQVUgIC0gSHluZG1hbiwgUm9iDQpBVSAgLSBLaGFuZGFrYXIsIFllYXNtaW4NClQyICAtIEpvdXJuYWwgb2YgU3RhdGlzdGljYWwgU29mdHdhcmUsIEFydGljbGVzDQpBQiAgLSBBdXRvbWF0aWMgZm9yZWNhc3RzIG9mIGxhcmdlIG51bWJlcnMgb2YgdW5pdmFyaWF0ZSB0aW1lIHNlcmllcyBhcmUgb2Z0ZW4gbmVlZGVkIGluIGJ1c2luZXNzIGFuZCBvdGhlciBjb250ZXh0cy4gV2UgZGVzY3JpYmUgdHdvIGF1dG9tYXRpYyBmb3JlY2FzdGluZyBhbGdvcml0aG1zIHRoYXQgaGF2ZSBiZWVuIGltcGxlbWVudGVkIGluIHRoZSBmb3JlY2FzdCBwYWNrYWdlIGZvciBSLiBUaGUgZmlyc3QgaXMgYmFzZWQgb24gaW5ub3ZhdGlvbnMgc3RhdGUgc3BhY2UgbW9kZWxzIHRoYXQgdW5kZXJseSBleHBvbmVudGlhbCBzbW9vdGhpbmcgbWV0aG9kcy4gVGhlIHNlY29uZCBpcyBhIHN0ZXAtd2lzZSBhbGdvcml0aG0gZm9yIGZvcmVjYXN0aW5nIHdpdGggQVJJTUEgbW9kZWxzLiBUaGUgYWxnb3JpdGhtcyBhcmUgYXBwbGljYWJsZSB0byBib3RoIHNlYXNvbmFsIGFuZCBub24tc2Vhc29uYWwgZGF0YSwgYW5kIGFyZSBjb21wYXJlZCBhbmQgaWxsdXN0cmF0ZWQgdXNpbmcgZm91ciByZWFsIHRpbWUgc2VyaWVzLiBXZSBhbHNvIGJyaWVmbHkgZGVzY3JpYmUgc29tZSBvZiB0aGUgb3RoZXIgZnVuY3Rpb25hbGl0eSBhdmFpbGFibGUgaW4gdGhlIGZvcmVjYXN0IHBhY2thZ2UuDQpEQSAgLSAyMDA4Ly8vDQpQWSAgLSAyMDA4DQpETyAgLSAxMC4xODYzNy9qc3MudjAyNy5pMDMNClZMICAtIDI3DQpJUyAgLSAzDQpTUCAgLSAxDQpFUCAgLSAyMg0KU04gIC0gMTU0OC03NjYwDQpVUiAgLSBodHRwczovL3d3dy5qc3RhdHNvZnQub3JnL3YwMjcvaTAzDQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. Hyndman and Y. Khandakar, “Automatic Time Series Forecasting: The forecast Package for R,” <i>Journal of Statistical Software, Articles</i>, vol. 27, no. 3, pp. 1–22, 2008.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.18637%2Fjss.v027.i03&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Automatic%20Time%20Series%20Forecasting%3A%20The%20forecast%20Package%20for%20R&amp;rft.jtitle=Journal%20of%20Statistical%20Software%2C%20Articles&amp;rft.volume=27&amp;rft.issue=3&amp;rft.aufirst=Rob&amp;rft.aulast=Hyndman&amp;rft.au=Rob%20Hyndman&amp;rft.au=Yeasmin%20Khandakar&amp;rft.date=2008&amp;rft.pages=1%E2%80%9322&amp;rft.issn=1548-7660"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Quarterly Time-Series Forecasting With Neural Networks</span> <span class="containertitle">IEEE Transactions on Neural Networks</span> (November 2007) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">G. P. Zhang and D. M. Kline, “<span class="doctitle">Quarterly Time-Series Forecasting With Neural Networks</span>,” <i>IEEE Transactions on Neural Networks</i>, vol. 18, no. 6, pp. 1800–1814, Nov. 2007.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2007 type__journalArticle ""</span></div><div class="bib-venue">IEEE Transactions on Neural Networks</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Forecasting of time series that have seasonal and other variations remains an important problem for forecasters. This paper presents a neural network (NN) approach to forecasting quarterly time series. With a large data set of 756 quarterly time series from the M3 forecasting competition‚ we conduct a comprehensive investigation of the effectiveness of several data preprocessing and modeling approaches. We consider two data preprocessing methods and 48 NN models with different possible combinations of lagged observations‚ seasonal dummy variables‚ trigonometric variables‚ and time index as inputs to the NN. Both parametric and nonparametric statistical analyses are performed to identify the best models under different circumstances and categorize similar models. Results indicate that simpler models‚ in general‚ outperform more complex models. In addition‚ data preprocessing especially with deseasonalization and detrending is very helpful in improving NN performance. Practical guidelines are also provided.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{zhang2007quarterly,
	title = {Quarterly {Time}-{Series} {Forecasting} {With} {Neural} {Networks}},
	volume = {18},
	issn = {1045-9227},
	doi = {10.1109/TNN.2007.896859},
	number = {6},
	journal = {IEEE Transactions on Neural Networks},
	author = {Zhang, G. P. and Kline, D. M.},
	month = nov,
	year = {2007},
	pages = {1800--1814}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gUXVhcnRlcmx5IFRpbWUtU2VyaWVzIEZvcmVjYXN0aW5nIFdpdGggTmV1cmFsIE5ldHdvcmtzDQpBVSAgLSBaaGFuZywgRy4gUC4NCkFVICAtIEtsaW5lLCBELiBNLg0KVDIgIC0gSUVFRSBUcmFuc2FjdGlvbnMgb24gTmV1cmFsIE5ldHdvcmtzDQpBQiAgLSBGb3JlY2FzdGluZyBvZiB0aW1lIHNlcmllcyB0aGF0IGhhdmUgc2Vhc29uYWwgYW5kIG90aGVyIHZhcmlhdGlvbnMgcmVtYWlucyBhbiBpbXBvcnRhbnQgcHJvYmxlbSBmb3IgZm9yZWNhc3RlcnMuIFRoaXMgcGFwZXIgcHJlc2VudHMgYSBuZXVyYWwgbmV0d29yayAoTk4pIGFwcHJvYWNoIHRvIGZvcmVjYXN0aW5nIHF1YXJ0ZXJseSB0aW1lIHNlcmllcy4gV2l0aCBhIGxhcmdlIGRhdGEgc2V0IG9mIDc1NiBxdWFydGVybHkgdGltZSBzZXJpZXMgZnJvbSB0aGUgTTMgZm9yZWNhc3RpbmcgY29tcGV0aXRpb24sIHdlIGNvbmR1Y3QgYSBjb21wcmVoZW5zaXZlIGludmVzdGlnYXRpb24gb2YgdGhlIGVmZmVjdGl2ZW5lc3Mgb2Ygc2V2ZXJhbCBkYXRhIHByZXByb2Nlc3NpbmcgYW5kIG1vZGVsaW5nIGFwcHJvYWNoZXMuIFdlIGNvbnNpZGVyIHR3byBkYXRhIHByZXByb2Nlc3NpbmcgbWV0aG9kcyBhbmQgNDggTk4gbW9kZWxzIHdpdGggZGlmZmVyZW50IHBvc3NpYmxlIGNvbWJpbmF0aW9ucyBvZiBsYWdnZWQgb2JzZXJ2YXRpb25zLCBzZWFzb25hbCBkdW1teSB2YXJpYWJsZXMsIHRyaWdvbm9tZXRyaWMgdmFyaWFibGVzLCBhbmQgdGltZSBpbmRleCBhcyBpbnB1dHMgdG8gdGhlIE5OLiBCb3RoIHBhcmFtZXRyaWMgYW5kIG5vbnBhcmFtZXRyaWMgc3RhdGlzdGljYWwgYW5hbHlzZXMgYXJlIHBlcmZvcm1lZCB0byBpZGVudGlmeSB0aGUgYmVzdCBtb2RlbHMgdW5kZXIgZGlmZmVyZW50IGNpcmN1bXN0YW5jZXMgYW5kIGNhdGVnb3JpemUgc2ltaWxhciBtb2RlbHMuIFJlc3VsdHMgaW5kaWNhdGUgdGhhdCBzaW1wbGVyIG1vZGVscywgaW4gZ2VuZXJhbCwgb3V0cGVyZm9ybSBtb3JlIGNvbXBsZXggbW9kZWxzLiBJbiBhZGRpdGlvbiwgZGF0YSBwcmVwcm9jZXNzaW5nIGVzcGVjaWFsbHkgd2l0aCBkZXNlYXNvbmFsaXphdGlvbiBhbmQgZGV0cmVuZGluZyBpcyB2ZXJ5IGhlbHBmdWwgaW4gaW1wcm92aW5nIE5OIHBlcmZvcm1hbmNlLiBQcmFjdGljYWwgZ3VpZGVsaW5lcyBhcmUgYWxzbyBwcm92aWRlZC4NCkRBICAtIDIwMDcvMTEvLw0KUFkgIC0gMjAwNw0KRE8gIC0gMTAuMTEwOS9UTk4uMjAwNy44OTY4NTkNClZMICAtIDE4DQpJUyAgLSA2DQpTUCAgLSAxODAwDQpFUCAgLSAxODE0DQpTTiAgLSAxMDQ1LTkyMjcNCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">G. P. Zhang and D. M. Kline, “Quarterly Time-Series Forecasting With Neural Networks,” <i>IEEE Transactions on Neural Networks</i>, vol. 18, no. 6, pp. 1800–1814, Nov. 2007.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FTNN.2007.896859&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Quarterly%20Time-Series%20Forecasting%20With%20Neural%20Networks&amp;rft.jtitle=IEEE%20Transactions%20on%20Neural%20Networks&amp;rft.volume=18&amp;rft.issue=6&amp;rft.aufirst=G.%20P.&amp;rft.aulast=Zhang&amp;rft.au=G.%20P.%20Zhang&amp;rft.au=D.%20M.%20Kline&amp;rft.date=2007-11&amp;rft.pages=1800%E2%80%931814&amp;rft.issn=1045-9227"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Matplotlib: A 2D Graphics Environment</span> <span class="containertitle">Computing in Science & Engineering</span> (May 2007) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. D. Hunter, “<span class="doctitle"><a class="doctitle" href="https://aip.scitation.org/doi/abs/10.1109/MCSE.2007.55">Matplotlib: A 2D Graphics Environment</a></span>,” <i>Computing in Science &amp; Engineering</i>, vol. 9, no. 3, pp. 90–95, May 2007.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2007 type__journalArticle ""</span></div><div class="bib-venue">Computing in Science & Engineering</div><div class="blinkitems"><div><div class="blink"><a   href="https://aip.scitation.org/doi/abs/10.1109/MCSE.2007.55" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{hunter2007matplotlib:,
	title = {Matplotlib: {A} 2D {Graphics} {Environment}},
	volume = {9},
	issn = {1521-9615},
	url = {https://aip.scitation.org/doi/abs/10.1109/MCSE.2007.55},
	doi = {10.1109/MCSE.2007.55},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Hunter, John D.},
	month = may,
	year = {2007},
	note = {00000},
	pages = {90--95}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gTWF0cGxvdGxpYjogQSAyRCBHcmFwaGljcyBFbnZpcm9ubWVudA0KQVUgIC0gSHVudGVyLCBKb2huIEQuDQpUMiAgLSBDb21wdXRpbmcgaW4gU2NpZW5jZSAmIEVuZ2luZWVyaW5nDQpEQSAgLSAyMDA3LzA1Ly8NClBZICAtIDIwMDcNCkRPICAtIDEwLjExMDkvTUNTRS4yMDA3LjU1DQpWTCAgLSA5DQpJUyAgLSAzDQpTUCAgLSA5MA0KRVAgIC0gOTUNClNOICAtIDE1MjEtOTYxNQ0KVVIgIC0gaHR0cHM6Ly9haXAuc2NpdGF0aW9uLm9yZy9kb2kvYWJzLzEwLjExMDkvTUNTRS4yMDA3LjU1DQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. D. Hunter, “Matplotlib: A 2D Graphics Environment,” <i>Computing in Science &amp; Engineering</i>, vol. 9, no. 3, pp. 90–95, May 2007.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FMCSE.2007.55&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Matplotlib%3A%20A%202D%20Graphics%20Environment&amp;rft.jtitle=Computing%20in%20Science%20%26%20Engineering&amp;rft.volume=9&amp;rft.issue=3&amp;rft.aufirst=John%20D.&amp;rft.aulast=Hunter&amp;rft.au=John%20D.%20Hunter&amp;rft.date=2007-05&amp;rft.pages=90%E2%80%9395&amp;rft.issn=1521-9615"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">25 years of time series forecasting</span> <span class="containertitle">International Journal of Forecasting</span> (January 2006) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. G. De Gooijer and R. J. Hyndman, “<span class="doctitle"><a class="doctitle" href="http://www.sciencedirect.com/science/article/pii/ S0169207006000021">25 years of time series forecasting</a></span>,” <i>International Journal of Forecasting</i>, vol. 22, no. 3, pp. 443–473, Jan. 2006.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2006 type__journalArticle ""</span></div><div class="bib-venue">International Journal of Forecasting</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">We review the past 25 years of research into time series forecasting. In this silver jubilee issue‚ we naturally highlight results published in journals managed by the International Institute of Forecasters (Journal of Forecasting 1982–1985 and International Journal of Forecasting 1985–2005). During this period‚ over one third of all papers published in these journals concerned time series forecasting. We also review highly influential works on time series forecasting that have been published elsewhere during this period. Enormous progress has been made in many areas‚ but we find that there are a large number of topics in need of further development. We conclude with comments on possible future research directions in this field.</div></div></div><div class="blink"><a   href="http://www.sciencedirect.com/science/article/pii/ S0169207006000021" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{de_gooijer_25_2006,
	series = {Twenty five years of forecasting},
	title = {25 years of time series forecasting},
	volume = {22},
	issn = {0169-2070},
	url = {http://www.sciencedirect.com/science/article/pii/ S0169207006000021},
	doi = {10.1016/j.ijforecast.2006.01.001},
	number = {3},
	journal = {International Journal of Forecasting},
	author = {De Gooijer, Jan G. and Hyndman, Rob J.},
	month = jan,
	year = {2006},
	pages = {443--473}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gMjUgeWVhcnMgb2YgdGltZSBzZXJpZXMgZm9yZWNhc3RpbmcNCkFVICAtIERlIEdvb2lqZXIsIEphbiBHLg0KQVUgIC0gSHluZG1hbiwgUm9iIEouDQpUMiAgLSBJbnRlcm5hdGlvbmFsIEpvdXJuYWwgb2YgRm9yZWNhc3RpbmcNClQzICAtIFR3ZW50eSBmaXZlIHllYXJzIG9mIGZvcmVjYXN0aW5nDQpBQiAgLSBXZSByZXZpZXcgdGhlIHBhc3QgMjUgeWVhcnMgb2YgcmVzZWFyY2ggaW50byB0aW1lIHNlcmllcyBmb3JlY2FzdGluZy4gSW4gdGhpcyBzaWx2ZXIganViaWxlZSBpc3N1ZSwgd2UgbmF0dXJhbGx5IGhpZ2hsaWdodCByZXN1bHRzIHB1Ymxpc2hlZCBpbiBqb3VybmFscyBtYW5hZ2VkIGJ5IHRoZSBJbnRlcm5hdGlvbmFsIEluc3RpdHV0ZSBvZiBGb3JlY2FzdGVycyAoSm91cm5hbCBvZiBGb3JlY2FzdGluZyAxOTgy4oCTMTk4NSBhbmQgSW50ZXJuYXRpb25hbCBKb3VybmFsIG9mIEZvcmVjYXN0aW5nIDE5ODXigJMyMDA1KS4gRHVyaW5nIHRoaXMgcGVyaW9kLCBvdmVyIG9uZSB0aGlyZCBvZiBhbGwgcGFwZXJzIHB1Ymxpc2hlZCBpbiB0aGVzZSBqb3VybmFscyBjb25jZXJuZWQgdGltZSBzZXJpZXMgZm9yZWNhc3RpbmcuIFdlIGFsc28gcmV2aWV3IGhpZ2hseSBpbmZsdWVudGlhbCB3b3JrcyBvbiB0aW1lIHNlcmllcyBmb3JlY2FzdGluZyB0aGF0IGhhdmUgYmVlbiBwdWJsaXNoZWQgZWxzZXdoZXJlIGR1cmluZyB0aGlzIHBlcmlvZC4gRW5vcm1vdXMgcHJvZ3Jlc3MgaGFzIGJlZW4gbWFkZSBpbiBtYW55IGFyZWFzLCBidXQgd2UgZmluZCB0aGF0IHRoZXJlIGFyZSBhIGxhcmdlIG51bWJlciBvZiB0b3BpY3MgaW4gbmVlZCBvZiBmdXJ0aGVyIGRldmVsb3BtZW50LiBXZSBjb25jbHVkZSB3aXRoIGNvbW1lbnRzIG9uIHBvc3NpYmxlIGZ1dHVyZSByZXNlYXJjaCBkaXJlY3Rpb25zIGluIHRoaXMgZmllbGQuDQpEQSAgLSAyMDA2LzAxLy8NClBZICAtIDIwMDYNCkRPICAtIDEwLjEwMTYvai5pamZvcmVjYXN0LjIwMDYuMDEuMDAxDQpWTCAgLSAyMg0KSVMgIC0gMw0KU1AgIC0gNDQzDQpFUCAgLSA0NzMNClNOICAtIDAxNjktMjA3MA0KVVIgIC0gaHR0cDovL3d3dy5zY2llbmNlZGlyZWN0LmNvbS9zY2llbmNlL2FydGljbGUvcGlpLyBTMDE2OTIwNzAwNjAwMDAyMQ0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. G. De Gooijer and R. J. Hyndman, “25 years of time series forecasting,” <i>International Journal of Forecasting</i>, vol. 22, no. 3, pp. 443–473, Jan. 2006.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ijforecast.2006.01.001&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=25%20years%20of%20time%20series%20forecasting&amp;rft.jtitle=International%20Journal%20of%20Forecasting&amp;rft.volume=22&amp;rft.issue=3&amp;rft.aufirst=Jan%20G.&amp;rft.aulast=De%20Gooijer&amp;rft.au=Jan%20G.%20De%20Gooijer&amp;rft.au=Rob%20J.%20Hyndman&amp;rft.date=2006&amp;rft.pages=443%E2%80%93473&amp;rft.issn=0169-2070"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">The Organization of Behavior: A Neuropsychological Theory</span> <span class="containertitle"></span> (April 2005) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. O. Hebb, <span class="doctitle"><i>The Organization of Behavior: A Neuropsychological Theory</i>.</span> Psychology Press, 2005.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2005 type__book ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Since its publication in 1949‚ D.O. Hebb’s‚ The Organization of Behavior has been one of the most influential books in the fields of psychology and neuroscience. However‚ the original edition has been unavailable since 1966‚ ensuring that Hebb’s comment that a classic normally means "cited but not read" is true in his case. This new edition rectifies a long-standing problem for behavioral neuroscientists–the inability to obtain one of the most cited publications in the field. The Organization of Behavior played a significant part in stimulating the investigation of the neural foundations of behavior and continues to be inspiring because it provides a general framework for relating behavior to synaptic organization through the dynamics of neural networks. D.O. Hebb was also the first to examine the mechanisms by which environment and experience can influence brain structure and function‚ and his ideas formed the basis for work on enriched environments as stimulants for behavioral development. References to Hebb‚ the Hebbian cell assembly‚ the Hebb synapse‚ and the Hebb rule increase each year. These forceful ideas of 1949 are now applied in engineering‚ robotics‚ and computer science‚ as well as neurophysiology‚ neuroscience‚ and psychology–a tribute to Hebb’s foresight in developing a foundational neuropsychological theory of the organization of behavior.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@book{hebb2005organization,
	title = {The {Organization} of {Behavior}: {A} {Neuropsychological} {Theory}},
	isbn = {978-1-135-63191-8},
	language = {en},
	publisher = {Psychology Press},
	author = {Hebb, D. O.},
	month = apr,
	year = {2005}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('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');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. O. Hebb, <i>The Organization of Behavior: A Neuropsychological Theory</i>. Psychology Press, 2005.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-1-135-63191-8&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The%20Organization%20of%20Behavior%3A%20A%20Neuropsychological%20Theory&amp;rft.publisher=Psychology%20Press&amp;rft.aufirst=D.%20O.&amp;rft.aulast=Hebb&amp;rft.au=D.%20O.%20Hebb&amp;rft.date=2005-04&amp;rft.isbn=978-1-135-63191-8"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Neural network forecasting for seasonal and trend time series</span> <span class="containertitle">European Journal of Operational Research</span> (January 2005) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">G. P. Zhang and M. Qi, “<span class="doctitle"><a class="doctitle" href="http://www.sciencedirect.com/science/article/pii/ S0377221703005484">Neural network forecasting for seasonal and trend time series</a></span>,” <i>European Journal of Operational Research</i>, vol. 160, no. 2, pp. 501–514, Jan. 2005.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2005 type__journalArticle ""</span></div><div class="bib-venue">European Journal of Operational Research</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Neural networks have been widely used as a promising method for time series forecasting. However‚ limited empirical studies on seasonal time series forecasting with neural networks yield mixed results. While some find that neural networks are able to model seasonality directly and prior deseasonalization is not necessary‚ others conclude just the opposite. In this paper‚ we investigate the issue of how to effectively model time series with both seasonal and trend patterns. In particular‚ we study the effectiveness of data preprocessing‚ including deseasonalization and detrending‚ on neural network modeling and forecasting performance. Both simulation and real data are examined and results are compared to those obtained from the Box–Jenkins seasonal autoregressive integrated moving average models. We find that neural networks are not able to capture seasonal or trend variations effectively with the unpreprocessed raw data and either detrending or deseasonalization can dramatically reduce forecasting errors. Moreover‚ a combined detrending and deseasonalization is found to be the most effective data preprocessing approach.</div></div></div><div class="blink"><a   href="http://www.sciencedirect.com/science/article/pii/ S0377221703005484" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{zhang2005neural,
	series = {Decision {Support} {Systems} in the {Internet} {Age}},
	title = {Neural network forecasting for seasonal and trend time series},
	volume = {160},
	issn = {0377-2217},
	url = {http://www.sciencedirect.com/science/article/pii/ S0377221703005484},
	doi = {10.1016/j.ejor.2003.08.037},
	number = {2},
	journal = {European Journal of Operational Research},
	author = {Zhang, G. Peter and Qi, Min},
	month = jan,
	year = {2005},
	pages = {501--514}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('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');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">G. P. Zhang and M. Qi, “Neural network forecasting for seasonal and trend time series,” <i>European Journal of Operational Research</i>, vol. 160, no. 2, pp. 501–514, Jan. 2005.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ejor.2003.08.037&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Neural%20network%20forecasting%20for%20seasonal%20and%20trend%20time%20series&amp;rft.jtitle=European%20Journal%20of%20Operational%20Research&amp;rft.volume=160&amp;rft.issue=2&amp;rft.aufirst=G.%20Peter&amp;rft.aulast=Zhang&amp;rft.au=G.%20Peter%20Zhang&amp;rft.au=Min%20Qi&amp;rft.date=2005&amp;rft.pages=501%E2%80%93514&amp;rft.issn=0377-2217"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Forecasting seasonals and trends by exponentially weighted moving averages</span> <span class="containertitle">International Journal of Forecasting</span> (January 2004) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">C. C. Holt, “<span class="doctitle"><a class="doctitle" href="http://www.sciencedirect.com/science/article/pii/ S0169207003001134">Forecasting seasonals and trends by exponentially weighted moving averages</a></span>,” <i>International Journal of Forecasting</i>, vol. 20, no. 1, pp. 5–10, Jan. 2004.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2004 type__journalArticle ""</span></div><div class="bib-venue">International Journal of Forecasting</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">The paper provides a systematic development of the forecasting expressions for exponential weighted moving averages. Methods for series with no trend‚ or additive or multiplicative trend are examined. Similarly‚ the methods cover non-seasonal‚ and seasonal series with additive or multiplicative error structures. The paper is a reprinted version of the 1957 report to the Office of Naval Research (ONR 52) and is being published here to provide greater accessibility.</div></div></div><div class="blink"><a   href="http://www.sciencedirect.com/science/article/pii/ S0169207003001134" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{holt2004forecasting,
	title = {Forecasting seasonals and trends by exponentially weighted moving averages},
	volume = {20},
	issn = {0169-2070},
	url = {http://www.sciencedirect.com/science/article/pii/ S0169207003001134},
	doi = {10.1016/j.ijforecast.2003.09.015},
	number = {1},
	journal = {International Journal of Forecasting},
	author = {Holt, Charles C.},
	month = jan,
	year = {2004},
	pages = {5--10}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gRm9yZWNhc3Rpbmcgc2Vhc29uYWxzIGFuZCB0cmVuZHMgYnkgZXhwb25lbnRpYWxseSB3ZWlnaHRlZCBtb3ZpbmcgYXZlcmFnZXMNCkFVICAtIEhvbHQsIENoYXJsZXMgQy4NClQyICAtIEludGVybmF0aW9uYWwgSm91cm5hbCBvZiBGb3JlY2FzdGluZw0KQUIgIC0gVGhlIHBhcGVyIHByb3ZpZGVzIGEgc3lzdGVtYXRpYyBkZXZlbG9wbWVudCBvZiB0aGUgZm9yZWNhc3RpbmcgZXhwcmVzc2lvbnMgZm9yIGV4cG9uZW50aWFsIHdlaWdodGVkIG1vdmluZyBhdmVyYWdlcy4gTWV0aG9kcyBmb3Igc2VyaWVzIHdpdGggbm8gdHJlbmQsIG9yIGFkZGl0aXZlIG9yIG11bHRpcGxpY2F0aXZlIHRyZW5kIGFyZSBleGFtaW5lZC4gU2ltaWxhcmx5LCB0aGUgbWV0aG9kcyBjb3ZlciBub24tc2Vhc29uYWwsIGFuZCBzZWFzb25hbCBzZXJpZXMgd2l0aCBhZGRpdGl2ZSBvciBtdWx0aXBsaWNhdGl2ZSBlcnJvciBzdHJ1Y3R1cmVzLiBUaGUgcGFwZXIgaXMgYSByZXByaW50ZWQgdmVyc2lvbiBvZiB0aGUgMTk1NyByZXBvcnQgdG8gdGhlIE9mZmljZSBvZiBOYXZhbCBSZXNlYXJjaCAoT05SIDUyKSBhbmQgaXMgYmVpbmcgcHVibGlzaGVkIGhlcmUgdG8gcHJvdmlkZSBncmVhdGVyIGFjY2Vzc2liaWxpdHkuDQpEQSAgLSAyMDA0LzAxLy8NClBZICAtIDIwMDQNCkRPICAtIDEwLjEwMTYvai5pamZvcmVjYXN0LjIwMDMuMDkuMDE1DQpWTCAgLSAyMA0KSVMgIC0gMQ0KU1AgIC0gNQ0KRVAgIC0gMTANClNOICAtIDAxNjktMjA3MA0KVVIgIC0gaHR0cDovL3d3dy5zY2llbmNlZGlyZWN0LmNvbS9zY2llbmNlL2FydGljbGUvcGlpLyBTMDE2OTIwNzAwMzAwMTEzNA0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">C. C. Holt, “Forecasting seasonals and trends by exponentially weighted moving averages,” <i>International Journal of Forecasting</i>, vol. 20, no. 1, pp. 5–10, Jan. 2004.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ijforecast.2003.09.015&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Forecasting%20seasonals%20and%20trends%20by%20exponentially%20weighted%20moving%20averages&amp;rft.jtitle=International%20Journal%20of%20Forecasting&amp;rft.volume=20&amp;rft.issue=1&amp;rft.aufirst=Charles%20C.&amp;rft.aulast=Holt&amp;rft.au=Charles%20C.%20Holt&amp;rft.date=2004&amp;rft.pages=5%E2%80%9310&amp;rft.issn=0169-2070"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Trend time series modeling and forecasting with neural networks</span> <span class="containertitle"></span> (March 2003) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">M. Qi and G. P. Zhang, “<span class="doctitle">Trend time series modeling and forecasting with neural networks</span>,” in <i>2003 IEEE International Conference on Computational Intelligence for Financial Engineering, 2003. Proceedings.</i>, 2003, pp. 331–337.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2003 type__conferencePaper ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Despite its great importance‚ there has been no general consensus on how to model the trends in time series data. Compared to traditional approaches‚ neural networks have shown some promise in time series forecasting. This paper investigates how to best model trend time series using neural networks. Four strategies (raw data‚ raw data with time index‚ detrending‚ and differencing) are used to model various simulated trend patterns (linear‚ nonlinear‚ deterministic‚ stochastic‚ and breaking trend). We find that with neural networks differencing often gives meritorious results regardless of the underlying DGPs. This finding is also confirmed by the real GNP series.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@inproceedings{qi2003trend,
	title = {Trend time series modeling and forecasting with neural networks},
	doi = {10.1109/CIFER.2003.1196279},
	booktitle = {2003 {IEEE} {International} {Conference} on {Computational} {Intelligence} for {Financial} {Engineering}, 2003. {Proceedings}.},
	author = {Qi, Min and Zhang, G. P.},
	month = mar,
	year = {2003},
	note = {00000},
	pages = {331--337}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ09ORg0KVEkgIC0gVHJlbmQgdGltZSBzZXJpZXMgbW9kZWxpbmcgYW5kIGZvcmVjYXN0aW5nIHdpdGggbmV1cmFsIG5ldHdvcmtzDQpBVSAgLSBRaSwgTWluDQpBVSAgLSBaaGFuZywgRy4gUC4NCkFCICAtIERlc3BpdGUgaXRzIGdyZWF0IGltcG9ydGFuY2UsIHRoZXJlIGhhcyBiZWVuIG5vIGdlbmVyYWwgY29uc2Vuc3VzIG9uIGhvdyB0byBtb2RlbCB0aGUgdHJlbmRzIGluIHRpbWUgc2VyaWVzIGRhdGEuIENvbXBhcmVkIHRvIHRyYWRpdGlvbmFsIGFwcHJvYWNoZXMsIG5ldXJhbCBuZXR3b3JrcyBoYXZlIHNob3duIHNvbWUgcHJvbWlzZSBpbiB0aW1lIHNlcmllcyBmb3JlY2FzdGluZy4gVGhpcyBwYXBlciBpbnZlc3RpZ2F0ZXMgaG93IHRvIGJlc3QgbW9kZWwgdHJlbmQgdGltZSBzZXJpZXMgdXNpbmcgbmV1cmFsIG5ldHdvcmtzLiBGb3VyIHN0cmF0ZWdpZXMgKHJhdyBkYXRhLCByYXcgZGF0YSB3aXRoIHRpbWUgaW5kZXgsIGRldHJlbmRpbmcsIGFuZCBkaWZmZXJlbmNpbmcpIGFyZSB1c2VkIHRvIG1vZGVsIHZhcmlvdXMgc2ltdWxhdGVkIHRyZW5kIHBhdHRlcm5zIChsaW5lYXIsIG5vbmxpbmVhciwgZGV0ZXJtaW5pc3RpYywgc3RvY2hhc3RpYywgYW5kIGJyZWFraW5nIHRyZW5kKS4gV2UgZmluZCB0aGF0IHdpdGggbmV1cmFsIG5ldHdvcmtzIGRpZmZlcmVuY2luZyBvZnRlbiBnaXZlcyBtZXJpdG9yaW91cyByZXN1bHRzIHJlZ2FyZGxlc3Mgb2YgdGhlIHVuZGVybHlpbmcgREdQcy4gVGhpcyBmaW5kaW5nIGlzIGFsc28gY29uZmlybWVkIGJ5IHRoZSByZWFsIEdOUCBzZXJpZXMuDQpDMyAgLSAyMDAzIElFRUUgSW50ZXJuYXRpb25hbCBDb25mZXJlbmNlIG9uIENvbXB1dGF0aW9uYWwgSW50ZWxsaWdlbmNlIGZvciBGaW5hbmNpYWwgRW5naW5lZXJpbmcsIDIwMDMuIFByb2NlZWRpbmdzLg0KREEgIC0gMjAwMy8wMy8vDQpQWSAgLSAyMDAzDQpETyAgLSAxMC4xMTA5L0NJRkVSLjIwMDMuMTE5NjI3OQ0KU1AgIC0gMzMxDQpFUCAgLSAzMzcNCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">M. Qi and G. P. Zhang, “Trend time series modeling and forecasting with neural networks,” in <i>2003 IEEE International Conference on Computational Intelligence for Financial Engineering, 2003. Proceedings.</i>, 2003, pp. 331–337.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FCIFER.2003.1196279&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Trend%20time%20series%20modeling%20and%20forecasting%20with%20neural%20networks&amp;rft.btitle=2003%20IEEE%20International%20Conference%20on%20Computational%20Intelligence%20for%20Financial%20Engineering%2C%202003.%20Proceedings.&amp;rft.aufirst=Min&amp;rft.aulast=Qi&amp;rft.au=Min%20Qi&amp;rft.au=G.%20P.%20Zhang&amp;rft.date=2003-03&amp;rft.pages=331%E2%80%93337"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">A state space framework for automatic forecasting using exponential smoothing methods</span> <span class="containertitle">International Journal of Forecasting</span> (July 2002) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. J. Hyndman, A. B. Koehler, R. D. Snyder, and S. Grose, “<span class="doctitle"><a class="doctitle" href="http://www.sciencedirect.com/science/article/pii/ S0169207001001108">A state space framework for automatic forecasting using exponential smoothing methods</a></span>,” <i>International Journal of Forecasting</i>, vol. 18, no. 3, pp. 439–454, Jul. 2002.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2002 type__journalArticle ""</span></div><div class="bib-venue">International Journal of Forecasting</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">We provide a new approach to automatic forecasting based on an extended range of exponential smoothing methods. Each method in our taxonomy of exponential smoothing methods provides forecasts that are equivalent to forecasts from a state space model. This equivalence allows: (1) easy calculation of the likelihood‚ the AIC and other model selection criteria; (2) computation of prediction intervals for each method; and (3) random simulation from the underlying state space model. We demonstrate the methods by applying them to the data from the M-competition and the M3-competition. The method provides forecast accuracy comparable to the best methods in the competitions; it is particularly good for short forecast horizons with seasonal data.</div></div></div><div class="blink"><a   href="http://www.sciencedirect.com/science/article/pii/ S0169207001001108" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{hyndman2002state,
	title = {A state space framework for automatic forecasting using exponential smoothing methods},
	volume = {18},
	issn = {0169-2070},
	url = {http://www.sciencedirect.com/science/article/pii/ S0169207001001108},
	doi = {10.1016/S0169-2070(01)00110-8},
	number = {3},
	journal = {International Journal of Forecasting},
	author = {Hyndman, Rob J and Koehler, Anne B and Snyder, Ralph D and Grose, Simone},
	month = jul,
	year = {2002},
	pages = {439--454}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gQSBzdGF0ZSBzcGFjZSBmcmFtZXdvcmsgZm9yIGF1dG9tYXRpYyBmb3JlY2FzdGluZyB1c2luZyBleHBvbmVudGlhbCBzbW9vdGhpbmcgbWV0aG9kcw0KQVUgIC0gSHluZG1hbiwgUm9iIEoNCkFVICAtIEtvZWhsZXIsIEFubmUgQg0KQVUgIC0gU255ZGVyLCBSYWxwaCBEDQpBVSAgLSBHcm9zZSwgU2ltb25lDQpUMiAgLSBJbnRlcm5hdGlvbmFsIEpvdXJuYWwgb2YgRm9yZWNhc3RpbmcNCkFCICAtIFdlIHByb3ZpZGUgYSBuZXcgYXBwcm9hY2ggdG8gYXV0b21hdGljIGZvcmVjYXN0aW5nIGJhc2VkIG9uIGFuIGV4dGVuZGVkIHJhbmdlIG9mIGV4cG9uZW50aWFsIHNtb290aGluZyBtZXRob2RzLiBFYWNoIG1ldGhvZCBpbiBvdXIgdGF4b25vbXkgb2YgZXhwb25lbnRpYWwgc21vb3RoaW5nIG1ldGhvZHMgcHJvdmlkZXMgZm9yZWNhc3RzIHRoYXQgYXJlIGVxdWl2YWxlbnQgdG8gZm9yZWNhc3RzIGZyb20gYSBzdGF0ZSBzcGFjZSBtb2RlbC4gVGhpcyBlcXVpdmFsZW5jZSBhbGxvd3M6ICgxKSBlYXN5IGNhbGN1bGF0aW9uIG9mIHRoZSBsaWtlbGlob29kLCB0aGUgQUlDIGFuZCBvdGhlciBtb2RlbCBzZWxlY3Rpb24gY3JpdGVyaWE7ICgyKSBjb21wdXRhdGlvbiBvZiBwcmVkaWN0aW9uIGludGVydmFscyBmb3IgZWFjaCBtZXRob2Q7IGFuZCAoMykgcmFuZG9tIHNpbXVsYXRpb24gZnJvbSB0aGUgdW5kZXJseWluZyBzdGF0ZSBzcGFjZSBtb2RlbC4gV2UgZGVtb25zdHJhdGUgdGhlIG1ldGhvZHMgYnkgYXBwbHlpbmcgdGhlbSB0byB0aGUgZGF0YSBmcm9tIHRoZSBNLWNvbXBldGl0aW9uIGFuZCB0aGUgTTMtY29tcGV0aXRpb24uIFRoZSBtZXRob2QgcHJvdmlkZXMgZm9yZWNhc3QgYWNjdXJhY3kgY29tcGFyYWJsZSB0byB0aGUgYmVzdCBtZXRob2RzIGluIHRoZSBjb21wZXRpdGlvbnM7IGl0IGlzIHBhcnRpY3VsYXJseSBnb29kIGZvciBzaG9ydCBmb3JlY2FzdCBob3Jpem9ucyB3aXRoIHNlYXNvbmFsIGRhdGEuDQpEQSAgLSAyMDAyLzA3Ly8NClBZICAtIDIwMDINCkRPICAtIDEwLjEwMTYvUzAxNjktMjA3MCgwMSkwMDExMC04DQpWTCAgLSAxOA0KSVMgIC0gMw0KU1AgIC0gNDM5DQpFUCAgLSA0NTQNClNOICAtIDAxNjktMjA3MA0KVVIgIC0gaHR0cDovL3d3dy5zY2llbmNlZGlyZWN0LmNvbS9zY2llbmNlL2FydGljbGUvcGlpLyBTMDE2OTIwNzAwMTAwMTEwOA0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. J. Hyndman, A. B. Koehler, R. D. Snyder, and S. Grose, “A state space framework for automatic forecasting using exponential smoothing methods,” <i>International Journal of Forecasting</i>, vol. 18, no. 3, pp. 439–454, Jul. 2002.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2FS0169-2070(01)00110-8&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=A%20state%20space%20framework%20for%20automatic%20forecasting%20using%20exponential%20smoothing%20methods&amp;rft.jtitle=International%20Journal%20of%20Forecasting&amp;rft.volume=18&amp;rft.issue=3&amp;rft.aufirst=Rob%20J&amp;rft.aulast=Hyndman&amp;rft.au=Rob%20J%20Hyndman&amp;rft.au=Anne%20B%20Koehler&amp;rft.au=Ralph%20D%20Snyder&amp;rft.au=Simone%20Grose&amp;rft.date=2002-07&amp;rft.pages=439%E2%80%93454&amp;rft.issn=0169-2070"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">SciPy: Open source scientific tools for Python</span> <span class="containertitle"></span> (2001) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">E. Jones, T. Oliphant, P. Peterson, and others, <span class="doctitle"><a class="doctitle" href="http://www.scipy.org/"><i>SciPy: Open source scientific tools for Python</i>.</a></span> 2001.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__2001 type__book ""</span></div><div class="blinkitems"><div><div class="blink"><a   href="http://www.scipy.org/" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@book{jones2001scipy:,
	title = {{SciPy}: {Open} source scientific tools for {Python}},
	url = {http://www.scipy.org/},
	author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu and {others}},
	year = {2001},
	note = {00000}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQk9PSw0KVEkgIC0gU2NpUHk6IE9wZW4gc291cmNlIHNjaWVudGlmaWMgdG9vbHMgZm9yIFB5dGhvbg0KQVUgIC0gSm9uZXMsIEVyaWMNCkFVICAtIE9saXBoYW50LCBUcmF2aXMNCkFVICAtIFBldGVyc29uLCBQZWFydQ0KQVUgIC0gb3RoZXJzDQpEQSAgLSAyMDAxLy8vDQpQWSAgLSAyMDAxDQpVUiAgLSBodHRwOi8vd3d3LnNjaXB5Lm9yZy8NCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">E. Jones, T. Oliphant, P. Peterson, and others, <i>SciPy: Open source scientific tools for Python</i>. 2001.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=SciPy%3A%20Open%20source%20scientific%20tools%20for%20Python&amp;rft.aufirst=Eric&amp;rft.aulast=Jones&amp;rft.au=Eric%20Jones&amp;rft.au=Travis%20Oliphant&amp;rft.au=Pearu%20Peterson&amp;rft.au=others&amp;rft.date=2001"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Efficient BackProp</span> <span class="containertitle"></span> (1998) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller, “<span class="doctitle"><a class="doctitle" href="https://link.springer.com/chapter/10.1007/3-540-49430-8_2">Efficient BackProp</a></span>,” in <i>Neural Networks: Tricks of the Trade</i>, Springer, Berlin, Heidelberg, 1998, pp. 9–50.</div>
  </div><div class="bib-extra">DOI: 10.1007/3-540-49430-8_2</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1998 type__bookSection ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks‚ ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.</div></div></div><div class="blink"><a   href="https://link.springer.com/chapter/10.1007/3-540-49430-8_2" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@incollection{lecun1998efficient,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Efficient {BackProp}},
	isbn = {978-3-540-65311-0 978-3-540-49430-0},
	url = {https://link.springer.com/chapter/10.1007/3-540-49430-8_2},
	language = {en},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {LeCun, Yann and Bottou, Leon and Orr, Genevieve B. and Müller, Klaus-Robert},
	year = {1998},
	doi = {10.1007/3-540-49430-8_2},
	pages = {9--50}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ0hBUA0KVEkgIC0gRWZmaWNpZW50IEJhY2tQcm9wDQpBVSAgLSBMZUN1biwgWWFubg0KQVUgIC0gQm90dG91LCBMZW9uDQpBVSAgLSBPcnIsIEdlbmV2aWV2ZSBCLg0KQVUgIC0gTcO8bGxlciwgS2xhdXMtUm9iZXJ0DQpUMiAgLSBOZXVyYWwgTmV0d29ya3M6IFRyaWNrcyBvZiB0aGUgVHJhZGUNClQzICAtIExlY3R1cmUgTm90ZXMgaW4gQ29tcHV0ZXIgU2NpZW5jZQ0KQUIgIC0gVGhlIGNvbnZlcmdlbmNlIG9mIGJhY2stcHJvcGFnYXRpb24gbGVhcm5pbmcgaXMgYW5hbHl6ZWQgc28gYXMgdG8gZXhwbGFpbiBjb21tb24gcGhlbm9tZW5vbiBvYnNlcnZlZGIgeSBwcmFjdGl0aW9uZXJzLiBNYW55IHVuZGVzaXJhYmxlIGJlaGF2aW9ycyBvZiBiYWNrcHJvcCBjYW4gYmUgYXZvaWRlZCB3aXRoIHRyaWNrcyB0aGF0IGFyZSByYXJlbHkgZXhwb3NlZGluIHNlcmlvdXMgdGVjaG5pY2FsIHB1YmxpY2F0aW9ucy4gVGhpcyBwYXBlciBnaXZlcyBzb21lIG9mIHRob3NlIHRyaWNrcywgYW5kby5lcnMgZXhwbGFuYXRpb25zIG9mIHdoeSB0aGV5IHdvcmsuIE1hbnkgYXV0aG9ycyBoYXZlIHN1Z2dlc3RlZCB0aGF0IHNlY29uZC1vcmRlciBvcHRpbWl6YXRpb24gbWV0aG9kcyBhcmUgYWR2YW50YWdlb3VzIGZvciBuZXVyYWwgbmV0IHRyYWluaW5nLiBJdCBpcyBzaG93biB0aGF0IG1vc3Qg4oCcY2xhc3NpY2Fs4oCdIHNlY29uZC1vcmRlciBtZXRob2RzIGFyZSBpbXByYWN0aWNhbCBmb3IgbGFyZ2UgbmV1cmFsIG5ldHdvcmtzLiBBIGZldyBtZXRob2RzIGFyZSBwcm9wb3NlZCB0aGF0IGRvIG5vdCBoYXZlIHRoZXNlIGxpbWl0YXRpb25zLg0KREEgIC0gMTk5OC8vLw0KUFkgIC0gMTk5OA0KU1AgIC0gOQ0KRVAgIC0gNTANCkxBICAtIGVuDQpQQiAgLSBTcHJpbmdlciwgQmVybGluLCBIZWlkZWxiZXJnDQpTTiAgLSA5NzgtMy01NDAtNjUzMTEtMCA5NzgtMy01NDAtNDk0MzAtMA0KVVIgIC0gaHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9jaGFwdGVyLzEwLjEwMDcvMy01NDAtNDk0MzAtOF8yDQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">Y. LeCun, L. Bottou, G. B. Orr, and K.-R. Müller, “Efficient BackProp,” in <i>Neural Networks: Tricks of the Trade</i>, Springer, Berlin, Heidelberg, 1998, pp. 9–50.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=urn%3Aisbn%3A978-3-540-65311-0%20978-3-540-49430-0&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Efficient%20BackProp&amp;rft.btitle=Neural%20Networks%3A%20Tricks%20of%20the%20Trade&amp;rft.publisher=Springer%2C%20Berlin%2C%20Heidelberg&amp;rft.series=Lecture%20Notes%20in%20Computer%20Science&amp;rft.aufirst=Yann&amp;rft.aulast=LeCun&amp;rft.au=Yann%20LeCun&amp;rft.au=Leon%20Bottou&amp;rft.au=Genevieve%20B.%20Orr&amp;rft.au=Klaus-Robert%20M%C3%BCller&amp;rft.date=1998&amp;rft.pages=9%E2%80%9350&amp;rft.isbn=978-3-540-65311-0%20978-3-540-49430-0"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">New Capabilities and Methods of the X-12-ARIMA Seasonal-Adjustment Program</span> <span class="containertitle">Journal of Business & Economic Statistics</span> (April 1998) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. F. Findley, B. C. Monsell, W. R. Bell, M. C. Otto, and B.-C. Chen, “<span class="doctitle"><a class="doctitle" href="http://www.tandfonline.com/doi/abs/10.1080/ 07350015.1998.10524743">New Capabilities and Methods of the X-12-ARIMA Seasonal-Adjustment Program</a></span>,” <i>Journal of Business &amp; Economic Statistics</i>, vol. 16, no. 2, pp. 127–152, Apr. 1998.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1998 type__journalArticle ""</span></div><div class="bib-venue">Journal of Business & Economic Statistics</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">X-12-ARIMA is the Census Bureau’s new seasonal-adjustment program. It provides four types of enhancements to X-ll-ARIMA—(1) alternative seasonal‚ trading-day‚ and holiday effect adjustment capabilities that include adjustments for effects estimated with user-defined regressors; additional seasonal and trend filter options; and an alternative seasonal-trend-irregular decomposition; (2) new diagnostics of the quality and stability of the adjustments achieved under the options selected; (3) extensive time series modeling and model-selection capabilities for linear regression models with ARIMA errors‚ with optional robust estimation of coefficients; (4) a new user interface with features to facilitate batch processing large numbers of series.</div></div></div><div class="blink"><a   href="http://www.tandfonline.com/doi/abs/10.1080/ 07350015.1998.10524743" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{findley1998new,
	title = {New {Capabilities} and {Methods} of the {X}-12-{ARIMA} {Seasonal}-{Adjustment} {Program}},
	volume = {16},
	issn = {0735-0015},
	url = {http://www.tandfonline.com/doi/abs/10.1080/ 07350015.1998.10524743},
	doi = {10.1080/07350015.1998.10524743},
	number = {2},
	journal = {Journal of Business \& Economic Statistics},
	author = {Findley, David F. and Monsell, Brian C. and Bell, William R. and Otto, Mark C. and Chen, Bor-Chung},
	month = apr,
	year = {1998},
	pages = {127--152}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gTmV3IENhcGFiaWxpdGllcyBhbmQgTWV0aG9kcyBvZiB0aGUgWC0xMi1BUklNQSBTZWFzb25hbC1BZGp1c3RtZW50IFByb2dyYW0NCkFVICAtIEZpbmRsZXksIERhdmlkIEYuDQpBVSAgLSBNb25zZWxsLCBCcmlhbiBDLg0KQVUgIC0gQmVsbCwgV2lsbGlhbSBSLg0KQVUgIC0gT3R0bywgTWFyayBDLg0KQVUgIC0gQ2hlbiwgQm9yLUNodW5nDQpUMiAgLSBKb3VybmFsIG9mIEJ1c2luZXNzICYgRWNvbm9taWMgU3RhdGlzdGljcw0KQUIgIC0gWC0xMi1BUklNQSBpcyB0aGUgQ2Vuc3VzIEJ1cmVhdSdzIG5ldyBzZWFzb25hbC1hZGp1c3RtZW50IHByb2dyYW0uIEl0IHByb3ZpZGVzIGZvdXIgdHlwZXMgb2YgZW5oYW5jZW1lbnRzIHRvIFgtbGwtQVJJTUHigJQoMSkgYWx0ZXJuYXRpdmUgc2Vhc29uYWwsIHRyYWRpbmctZGF5LCBhbmQgaG9saWRheSBlZmZlY3QgYWRqdXN0bWVudCBjYXBhYmlsaXRpZXMgdGhhdCBpbmNsdWRlIGFkanVzdG1lbnRzIGZvciBlZmZlY3RzIGVzdGltYXRlZCB3aXRoIHVzZXItZGVmaW5lZCByZWdyZXNzb3JzOyBhZGRpdGlvbmFsIHNlYXNvbmFsIGFuZCB0cmVuZCBmaWx0ZXIgb3B0aW9uczsgYW5kIGFuIGFsdGVybmF0aXZlIHNlYXNvbmFsLXRyZW5kLWlycmVndWxhciBkZWNvbXBvc2l0aW9uOyAoMikgbmV3IGRpYWdub3N0aWNzIG9mIHRoZSBxdWFsaXR5IGFuZCBzdGFiaWxpdHkgb2YgdGhlIGFkanVzdG1lbnRzIGFjaGlldmVkIHVuZGVyIHRoZSBvcHRpb25zIHNlbGVjdGVkOyAoMykgZXh0ZW5zaXZlIHRpbWUgc2VyaWVzIG1vZGVsaW5nIGFuZCBtb2RlbC1zZWxlY3Rpb24gY2FwYWJpbGl0aWVzIGZvciBsaW5lYXIgcmVncmVzc2lvbiBtb2RlbHMgd2l0aCBBUklNQSBlcnJvcnMsIHdpdGggb3B0aW9uYWwgcm9idXN0IGVzdGltYXRpb24gb2YgY29lZmZpY2llbnRzOyAoNCkgYSBuZXcgdXNlciBpbnRlcmZhY2Ugd2l0aCBmZWF0dXJlcyB0byBmYWNpbGl0YXRlIGJhdGNoIHByb2Nlc3NpbmcgbGFyZ2UgbnVtYmVycyBvZiBzZXJpZXMuDQpEQSAgLSAxOTk4LzA0Ly8NClBZICAtIDE5OTgNCkRPICAtIDEwLjEwODAvMDczNTAwMTUuMTk5OC4xMDUyNDc0Mw0KVkwgIC0gMTYNCklTICAtIDINClNQICAtIDEyNw0KRVAgIC0gMTUyDQpTTiAgLSAwNzM1LTAwMTUNClVSICAtIGh0dHA6Ly93d3cudGFuZGZvbmxpbmUuY29tL2RvaS9hYnMvMTAuMTA4MC8gMDczNTAwMTUuMTk5OC4xMDUyNDc0Mw0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. F. Findley, B. C. Monsell, W. R. Bell, M. C. Otto, and B.-C. Chen, “New Capabilities and Methods of the X-12-ARIMA Seasonal-Adjustment Program,” <i>Journal of Business &amp; Economic Statistics</i>, vol. 16, no. 2, pp. 127–152, Apr. 1998.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1080%2F07350015.1998.10524743&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=New%20Capabilities%20and%20Methods%20of%20the%20X-12-ARIMA%20Seasonal-Adjustment%20Program&amp;rft.jtitle=Journal%20of%20Business%20%26%20Economic%20Statistics&amp;rft.volume=16&amp;rft.issue=2&amp;rft.aufirst=David%20F.&amp;rft.aulast=Findley&amp;rft.au=David%20F.%20Findley&amp;rft.au=Brian%20C.%20Monsell&amp;rft.au=William%20R.%20Bell&amp;rft.au=Mark%20C.%20Otto&amp;rft.au=Bor-Chung%20Chen&amp;rft.date=1998-04&amp;rft.pages=127%E2%80%93152&amp;rft.issn=0735-0015"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Long Short-Term Memory</span> <span class="containertitle">Neural Computation</span> (November 1997) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">S. Hochreiter and J. Schmidhuber, “<span class="doctitle"><a class="doctitle" href="https://doi.org/10.1162/neco.1997.9.8.1735">Long Short-Term Memory</a></span>,” <i>Neural Computation</i>, vol. 9, no. 8, pp. 1735–1780, Nov. 1997.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1997 type__journalArticle ""</span></div><div class="bib-venue">Neural Computation</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Learning to store information over extended time intervals by recurrent backpropagation takes a very long time‚ mostly because of insufficient‚ decaying error backflow. We briefly review Hochreiter’s (1991) analysis of this problem‚ then address it by introducing a novel‚ efficient‚ gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm‚ LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local‚ distributed‚ real-valued‚ and noisy pattern representations. In comparisons with real-time recurrent learning‚ back propagation through time‚ recurrent cascade correlation‚ Elman nets‚ and neural sequence chunking‚ LSTM leads to many more successful runs‚ and learns much faster. LSTM also solves complex‚ artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.</div></div></div><div class="blink"><a   href="https://doi.org/10.1162/neco.1997.9.8.1735" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{hochreiter1997long,
	title = {Long {Short}-{Term} {Memory}},
	volume = {9},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	number = {8},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	pages = {1735--1780}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('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');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">S. Hochreiter and J. Schmidhuber, “Long Short-Term Memory,” <i>Neural Computation</i>, vol. 9, no. 8, pp. 1735–1780, Nov. 1997.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.1997.9.8.1735&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Long%20Short-Term%20Memory&amp;rft.jtitle=Neural%20Computation&amp;rft.volume=9&amp;rft.issue=8&amp;rft.aufirst=Sepp&amp;rft.aulast=Hochreiter&amp;rft.au=Sepp%20Hochreiter&amp;rft.au=J%C3%BCrgen%20Schmidhuber&amp;rft.date=1997-11&amp;rft.pages=1735%E2%80%931780&amp;rft.issn=0899-7667"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Multitask Learning</span> <span class="containertitle">Machine Learning</span> (July 1997) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. Caruana, “<span class="doctitle"><a class="doctitle" href="https://link.springer.com/article/10.1023/A:1007379606734">Multitask Learning</a></span>,” <i>Machine Learning</i>, vol. 28, no. 1, pp. 41–75, Jul. 1997.</div>
  </div><div class="bib-extra">02964</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1997 type__journalArticle ""</span></div><div class="bib-venue">Machine Learning</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL‚ presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals‚ and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works‚ and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression‚ and sketch an algorithm for multitask learning in decision trees. Because multitask learning works‚ can be applied to many different kinds of domains‚ and can be used with different learning algorithms‚ we conjecture there will be many opportunities for its use on real-world problems.</div></div></div><div class="blink"><a   href="https://link.springer.com/article/10.1023/A:1007379606734" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{caruana1997multitask,
	title = {Multitask {Learning}},
	volume = {28},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1023/A:1007379606734},
	doi = {10.1023/A:1007379606734},
	language = {en},
	number = {1},
	journal = {Machine Learning},
	author = {Caruana, Rich},
	month = jul,
	year = {1997},
	note = {02964},
	pages = {41--75}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gTXVsdGl0YXNrIExlYXJuaW5nDQpBVSAgLSBDYXJ1YW5hLCBSaWNoDQpUMiAgLSBNYWNoaW5lIExlYXJuaW5nDQpBQiAgLSBNdWx0aXRhc2sgTGVhcm5pbmcgaXMgYW4gYXBwcm9hY2ggdG8gaW5kdWN0aXZlIHRyYW5zZmVyIHRoYXQgaW1wcm92ZXMgZ2VuZXJhbGl6YXRpb24gYnkgdXNpbmcgdGhlIGRvbWFpbiBpbmZvcm1hdGlvbiBjb250YWluZWQgaW4gdGhlIHRyYWluaW5nIHNpZ25hbHMgb2YgcmVsYXRlZCB0YXNrcyBhcyBhbiBpbmR1Y3RpdmUgYmlhcy4gSXQgZG9lcyB0aGlzIGJ5IGxlYXJuaW5nIHRhc2tzIGluIHBhcmFsbGVsIHdoaWxlIHVzaW5nIGEgc2hhcmVkIHJlcHJlc2VudGF0aW9uOyB3aGF0IGlzIGxlYXJuZWQgZm9yIGVhY2ggdGFzayBjYW4gaGVscCBvdGhlciB0YXNrcyBiZSBsZWFybmVkIGJldHRlci4gVGhpcyBwYXBlciByZXZpZXdzIHByaW9yIHdvcmsgb24gTVRMLCBwcmVzZW50cyBuZXcgZXZpZGVuY2UgdGhhdCBNVEwgaW4gYmFja3Byb3AgbmV0cyBkaXNjb3ZlcnMgdGFzayByZWxhdGVkbmVzcyB3aXRob3V0IHRoZSBuZWVkIG9mIHN1cGVydmlzb3J5IHNpZ25hbHMsIGFuZCBwcmVzZW50cyBuZXcgcmVzdWx0cyBmb3IgTVRMIHdpdGggay1uZWFyZXN0IG5laWdoYm9yIGFuZCBrZXJuZWwgcmVncmVzc2lvbi4gSW4gdGhpcyBwYXBlciB3ZSBkZW1vbnN0cmF0ZSBtdWx0aXRhc2sgbGVhcm5pbmcgaW4gdGhyZWUgZG9tYWlucy4gV2UgZXhwbGFpbiBob3cgbXVsdGl0YXNrIGxlYXJuaW5nIHdvcmtzLCBhbmQgc2hvdyB0aGF0IHRoZXJlIGFyZSBtYW55IG9wcG9ydHVuaXRpZXMgZm9yIG11bHRpdGFzayBsZWFybmluZyBpbiByZWFsIGRvbWFpbnMuIFdlIHByZXNlbnQgYW4gYWxnb3JpdGhtIGFuZCByZXN1bHRzIGZvciBtdWx0aXRhc2sgbGVhcm5pbmcgd2l0aCBjYXNlLWJhc2VkIG1ldGhvZHMgbGlrZSBrLW5lYXJlc3QgbmVpZ2hib3IgYW5kIGtlcm5lbCByZWdyZXNzaW9uLCBhbmQgc2tldGNoIGFuIGFsZ29yaXRobSBmb3IgbXVsdGl0YXNrIGxlYXJuaW5nIGluIGRlY2lzaW9uIHRyZWVzLiBCZWNhdXNlIG11bHRpdGFzayBsZWFybmluZyB3b3JrcywgY2FuIGJlIGFwcGxpZWQgdG8gbWFueSBkaWZmZXJlbnQga2luZHMgb2YgZG9tYWlucywgYW5kIGNhbiBiZSB1c2VkIHdpdGggZGlmZmVyZW50IGxlYXJuaW5nIGFsZ29yaXRobXMsIHdlIGNvbmplY3R1cmUgdGhlcmUgd2lsbCBiZSBtYW55IG9wcG9ydHVuaXRpZXMgZm9yIGl0cyB1c2Ugb24gcmVhbC13b3JsZCBwcm9ibGVtcy4NCkRBICAtIDE5OTcvMDcvLw0KUFkgIC0gMTk5Nw0KRE8gIC0gMTAuMTAyMy9BOjEwMDczNzk2MDY3MzQNClZMICAtIDI4DQpJUyAgLSAxDQpTUCAgLSA0MQ0KRVAgIC0gNzUNCkxBICAtIGVuDQpTTiAgLSAwODg1LTYxMjUsIDE1NzMtMDU2NQ0KVVIgIC0gaHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMjMvQToxMDA3Mzc5NjA2NzM0DQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. Caruana, “Multitask Learning,” <i>Machine Learning</i>, vol. 28, no. 1, pp. 41–75, Jul. 1997.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1023%2FA%3A1007379606734&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Multitask%20Learning&amp;rft.jtitle=Machine%20Learning&amp;rft.volume=28&amp;rft.issue=1&amp;rft.aufirst=Rich&amp;rft.aulast=Caruana&amp;rft.au=Rich%20Caruana&amp;rft.date=1997-07&amp;rft.pages=41%E2%80%9375&amp;rft.issn=0885-6125%2C%201573-0565"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Declarative bias in equation discovery</span> <span class="containertitle"></span> (1997) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">L. Todorovski and S. Dzeroski, “<span class="doctitle">Declarative bias in equation discovery</span>,” in <i>ICML</i>, 1997, pp. 376–384.</div>
  </div><div class="bib-extra">00150</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1997 type__conferencePaper ""</span></div><div class="blinkitems"><div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@inproceedings{todorovski1997declarative,
	title = {Declarative bias in equation discovery},
	booktitle = {{ICML}},
	author = {Todorovski, Ljupco and Dzeroski, Saso},
	year = {1997},
	note = {00150},
	pages = {376--384}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ09ORg0KVEkgIC0gRGVjbGFyYXRpdmUgYmlhcyBpbiBlcXVhdGlvbiBkaXNjb3ZlcnkNCkFVICAtIFRvZG9yb3Zza2ksIExqdXBjbw0KQVUgIC0gRHplcm9za2ksIFNhc28NCkMzICAtIElDTUwNCkRBICAtIDE5OTcvLy8NClBZICAtIDE5OTcNClNQICAtIDM3Ng0KRVAgIC0gMzg0DQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">L. Todorovski and S. Dzeroski, “Declarative bias in equation discovery,” in <i>ICML</i>, 1997, pp. 376–384.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Declarative%20bias%20in%20equation%20discovery&amp;rft.btitle=ICML&amp;rft.aufirst=Ljupco&amp;rft.aulast=Todorovski&amp;rft.au=Ljupco%20Todorovski&amp;rft.au=Saso%20Dzeroski&amp;rft.date=1997&amp;rft.pages=376%E2%80%93384"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Design and regularization of neural networks: the optimal use of a validation set</span> <span class="containertitle"></span> (September 1996) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. Larsen, L. K. Hansen, C. Svarer, and M. Ohlsson, “<span class="doctitle">Design and regularization of neural networks: the optimal use of a validation set</span>,” in <i>Neural Networks for Signal Processing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop</i>, 1996, pp. 62–71.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1996 type__conferencePaper ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">We derive novel algorithms for estimation of regularization parameters and for optimization of neural net architectures based on a validation set. Regularisation parameters are estimated using an iterative gradient descent scheme. Architecture optimization is performed by approximative combinatorial search among the relevant subsets of an initial neural network architecture by employing a validation set based optimal brain damage/surgeon (OBD/OBS) or a mean field combinatorial optimization approach. Numerical results with linear models and feed-forward neural networks demonstrate the viability of the methods</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@inproceedings{larsen1996design,
	title = {Design and regularization of neural networks: the optimal use of a validation set},
	doi = {10.1109/NNSP.1996.548336},
	booktitle = {Neural {Networks} for {Signal} {Processing} {VI}. {Proceedings} of the 1996 {IEEE} {Signal} {Processing} {Society} {Workshop}},
	author = {Larsen, J. and Hansen, L. K. and Svarer, C. and Ohlsson, M.},
	month = sep,
	year = {1996},
	pages = {62--71}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ09ORg0KVEkgIC0gRGVzaWduIGFuZCByZWd1bGFyaXphdGlvbiBvZiBuZXVyYWwgbmV0d29ya3M6IHRoZSBvcHRpbWFsIHVzZSBvZiBhIHZhbGlkYXRpb24gc2V0DQpBVSAgLSBMYXJzZW4sIEouDQpBVSAgLSBIYW5zZW4sIEwuIEsuDQpBVSAgLSBTdmFyZXIsIEMuDQpBVSAgLSBPaGxzc29uLCBNLg0KQUIgIC0gV2UgZGVyaXZlIG5vdmVsIGFsZ29yaXRobXMgZm9yIGVzdGltYXRpb24gb2YgcmVndWxhcml6YXRpb24gcGFyYW1ldGVycyBhbmQgZm9yIG9wdGltaXphdGlvbiBvZiBuZXVyYWwgbmV0IGFyY2hpdGVjdHVyZXMgYmFzZWQgb24gYSB2YWxpZGF0aW9uIHNldC4gUmVndWxhcmlzYXRpb24gcGFyYW1ldGVycyBhcmUgZXN0aW1hdGVkIHVzaW5nIGFuIGl0ZXJhdGl2ZSBncmFkaWVudCBkZXNjZW50IHNjaGVtZS4gQXJjaGl0ZWN0dXJlIG9wdGltaXphdGlvbiBpcyBwZXJmb3JtZWQgYnkgYXBwcm94aW1hdGl2ZSBjb21iaW5hdG9yaWFsIHNlYXJjaCBhbW9uZyB0aGUgcmVsZXZhbnQgc3Vic2V0cyBvZiBhbiBpbml0aWFsIG5ldXJhbCBuZXR3b3JrIGFyY2hpdGVjdHVyZSBieSBlbXBsb3lpbmcgYSB2YWxpZGF0aW9uIHNldCBiYXNlZCBvcHRpbWFsIGJyYWluIGRhbWFnZS9zdXJnZW9uIChPQkQvT0JTKSBvciBhIG1lYW4gZmllbGQgY29tYmluYXRvcmlhbCBvcHRpbWl6YXRpb24gYXBwcm9hY2guIE51bWVyaWNhbCByZXN1bHRzIHdpdGggbGluZWFyIG1vZGVscyBhbmQgZmVlZC1mb3J3YXJkIG5ldXJhbCBuZXR3b3JrcyBkZW1vbnN0cmF0ZSB0aGUgdmlhYmlsaXR5IG9mIHRoZSBtZXRob2RzDQpDMyAgLSBOZXVyYWwgTmV0d29ya3MgZm9yIFNpZ25hbCBQcm9jZXNzaW5nIFZJLiBQcm9jZWVkaW5ncyBvZiB0aGUgMTk5NiBJRUVFIFNpZ25hbCBQcm9jZXNzaW5nIFNvY2lldHkgV29ya3Nob3ANCkRBICAtIDE5OTYvMDkvLw0KUFkgIC0gMTk5Ng0KRE8gIC0gMTAuMTEwOS9OTlNQLjE5OTYuNTQ4MzM2DQpTUCAgLSA2Mg0KRVAgIC0gNzENCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. Larsen, L. K. Hansen, C. Svarer, and M. Ohlsson, “Design and regularization of neural networks: the optimal use of a validation set,” in <i>Neural Networks for Signal Processing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop</i>, 1996, pp. 62–71.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FNNSP.1996.548336&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Design%20and%20regularization%20of%20neural%20networks%3A%20the%20optimal%20use%20of%20a%20validation%20set&amp;rft.btitle=Neural%20Networks%20for%20Signal%20Processing%20VI.%20Proceedings%20of%20the%201996%20IEEE%20Signal%20Processing%20Society%20Workshop&amp;rft.aufirst=J.&amp;rft.aulast=Larsen&amp;rft.au=J.%20Larsen&amp;rft.au=L.%20K.%20Hansen&amp;rft.au=C.%20Svarer&amp;rft.au=M.%20Ohlsson&amp;rft.date=1996-09&amp;rft.pages=62%E2%80%9371"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Designing a neural network for forecasting financial and economic time series</span> <span class="containertitle">Neurocomputing</span> (April 1996) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">I. Kaastra and M. Boyd, “<span class="doctitle"><a class="doctitle" href="http://www.sciencedirect.com/science/article/pii/ 0925231295000399">Designing a neural network for forecasting financial and economic time series</a></span>,” <i>Neurocomputing</i>, vol. 10, no. 3, pp. 215–236, Apr. 1996.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1996 type__journalArticle ""</span></div><div class="bib-venue">Neurocomputing</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Artificial neural networks are universal and highly flexible function approximators first used in the fields of cognitive science and engineering. In recent years‚ neural network applications in finance for such tasks as pattern recognition‚ classification‚ and time series forecasting have dramatically increased. However‚ the large number of parameters that must be selected to develop a neural network forecasting model have meant that the design process still involves much trial and error. The objective of this paper is to provide a practical introductory guide in the design of a neural network for forecasting economic time series data. An eight-step procedure to design a neural network forecasting model is explained including a discussion of tradeoffs in parameter selection‚ some common pitfalls‚ and points of disagreement among practitioners.</div></div></div><div class="blink"><a   href="http://www.sciencedirect.com/science/article/pii/ 0925231295000399" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{kaastra1996designing,
	series = {Financial {Applications}, {Part} {II}},
	title = {Designing a neural network for forecasting financial and economic time series},
	volume = {10},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/ 0925231295000399},
	doi = {10.1016/0925-2312(95)00039-9},
	number = {3},
	journal = {Neurocomputing},
	author = {Kaastra, Iebeling and Boyd, Milton},
	month = apr,
	year = {1996},
	note = {00000},
	pages = {215--236}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gRGVzaWduaW5nIGEgbmV1cmFsIG5ldHdvcmsgZm9yIGZvcmVjYXN0aW5nIGZpbmFuY2lhbCBhbmQgZWNvbm9taWMgdGltZSBzZXJpZXMNCkFVICAtIEthYXN0cmEsIEllYmVsaW5nDQpBVSAgLSBCb3lkLCBNaWx0b24NClQyICAtIE5ldXJvY29tcHV0aW5nDQpUMyAgLSBGaW5hbmNpYWwgQXBwbGljYXRpb25zLCBQYXJ0IElJDQpBQiAgLSBBcnRpZmljaWFsIG5ldXJhbCBuZXR3b3JrcyBhcmUgdW5pdmVyc2FsIGFuZCBoaWdobHkgZmxleGlibGUgZnVuY3Rpb24gYXBwcm94aW1hdG9ycyBmaXJzdCB1c2VkIGluIHRoZSBmaWVsZHMgb2YgY29nbml0aXZlIHNjaWVuY2UgYW5kIGVuZ2luZWVyaW5nLiBJbiByZWNlbnQgeWVhcnMsIG5ldXJhbCBuZXR3b3JrIGFwcGxpY2F0aW9ucyBpbiBmaW5hbmNlIGZvciBzdWNoIHRhc2tzIGFzIHBhdHRlcm4gcmVjb2duaXRpb24sIGNsYXNzaWZpY2F0aW9uLCBhbmQgdGltZSBzZXJpZXMgZm9yZWNhc3RpbmcgaGF2ZSBkcmFtYXRpY2FsbHkgaW5jcmVhc2VkLiBIb3dldmVyLCB0aGUgbGFyZ2UgbnVtYmVyIG9mIHBhcmFtZXRlcnMgdGhhdCBtdXN0IGJlIHNlbGVjdGVkIHRvIGRldmVsb3AgYSBuZXVyYWwgbmV0d29yayBmb3JlY2FzdGluZyBtb2RlbCBoYXZlIG1lYW50IHRoYXQgdGhlIGRlc2lnbiBwcm9jZXNzIHN0aWxsIGludm9sdmVzIG11Y2ggdHJpYWwgYW5kIGVycm9yLiBUaGUgb2JqZWN0aXZlIG9mIHRoaXMgcGFwZXIgaXMgdG8gcHJvdmlkZSBhIHByYWN0aWNhbCBpbnRyb2R1Y3RvcnkgZ3VpZGUgaW4gdGhlIGRlc2lnbiBvZiBhIG5ldXJhbCBuZXR3b3JrIGZvciBmb3JlY2FzdGluZyBlY29ub21pYyB0aW1lIHNlcmllcyBkYXRhLiBBbiBlaWdodC1zdGVwIHByb2NlZHVyZSB0byBkZXNpZ24gYSBuZXVyYWwgbmV0d29yayBmb3JlY2FzdGluZyBtb2RlbCBpcyBleHBsYWluZWQgaW5jbHVkaW5nIGEgZGlzY3Vzc2lvbiBvZiB0cmFkZW9mZnMgaW4gcGFyYW1ldGVyIHNlbGVjdGlvbiwgc29tZSBjb21tb24gcGl0ZmFsbHMsIGFuZCBwb2ludHMgb2YgZGlzYWdyZWVtZW50IGFtb25nIHByYWN0aXRpb25lcnMuDQpEQSAgLSAxOTk2LzA0Ly8NClBZICAtIDE5OTYNCkRPICAtIDEwLjEwMTYvMDkyNS0yMzEyKDk1KTAwMDM5LTkNClZMICAtIDEwDQpJUyAgLSAzDQpTUCAgLSAyMTUNCkVQICAtIDIzNg0KU04gIC0gMDkyNS0yMzEyDQpVUiAgLSBodHRwOi8vd3d3LnNjaWVuY2VkaXJlY3QuY29tL3NjaWVuY2UvYXJ0aWNsZS9waWkvIDA5MjUyMzEyOTUwMDAzOTkNCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">I. Kaastra and M. Boyd, “Designing a neural network for forecasting financial and economic time series,” <i>Neurocomputing</i>, vol. 10, no. 3, pp. 215–236, Apr. 1996.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2F0925-2312(95)00039-9&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Designing%20a%20neural%20network%20for%20forecasting%20financial%20and%20economic%20time%20series&amp;rft.jtitle=Neurocomputing&amp;rft.volume=10&amp;rft.issue=3&amp;rft.aufirst=Iebeling&amp;rft.aulast=Kaastra&amp;rft.au=Iebeling%20Kaastra&amp;rft.au=Milton%20Boyd&amp;rft.date=1996-04&amp;rft.pages=215%E2%80%93236&amp;rft.issn=0925-2312"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Particle swarm optimization</span> <span class="containertitle"></span> (November 1995) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. Kennedy and R. Eberhart, “<span class="doctitle">Particle swarm optimization</span>,” in <i>, IEEE International Conference on Neural Networks, 1995. Proceedings</i>, 1995, vol. 4, pp. 1942–1948 vol.4.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1995 type__conferencePaper ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined‚ and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described‚ and applications‚ including nonlinear function optimization and neural network training‚ are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@inproceedings{kennedy1995particle,
	title = {Particle swarm optimization},
	volume = {4},
	doi = {10.1109/ICNN.1995.488968},
	booktitle = {, {IEEE} {International} {Conference} on {Neural} {Networks}, 1995. {Proceedings}},
	author = {Kennedy, J. and Eberhart, R.},
	month = nov,
	year = {1995},
	note = {00000},
	pages = {1942--1948 vol.4}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ09ORg0KVEkgIC0gUGFydGljbGUgc3dhcm0gb3B0aW1pemF0aW9uDQpBVSAgLSBLZW5uZWR5LCBKLg0KQVUgIC0gRWJlcmhhcnQsIFIuDQpBQiAgLSBBIGNvbmNlcHQgZm9yIHRoZSBvcHRpbWl6YXRpb24gb2Ygbm9ubGluZWFyIGZ1bmN0aW9ucyB1c2luZyBwYXJ0aWNsZSBzd2FybSBtZXRob2RvbG9neSBpcyBpbnRyb2R1Y2VkLiBUaGUgZXZvbHV0aW9uIG9mIHNldmVyYWwgcGFyYWRpZ21zIGlzIG91dGxpbmVkLCBhbmQgYW4gaW1wbGVtZW50YXRpb24gb2Ygb25lIG9mIHRoZSBwYXJhZGlnbXMgaXMgZGlzY3Vzc2VkLiBCZW5jaG1hcmsgdGVzdGluZyBvZiB0aGUgcGFyYWRpZ20gaXMgZGVzY3JpYmVkLCBhbmQgYXBwbGljYXRpb25zLCBpbmNsdWRpbmcgbm9ubGluZWFyIGZ1bmN0aW9uIG9wdGltaXphdGlvbiBhbmQgbmV1cmFsIG5ldHdvcmsgdHJhaW5pbmcsIGFyZSBwcm9wb3NlZC4gVGhlIHJlbGF0aW9uc2hpcHMgYmV0d2VlbiBwYXJ0aWNsZSBzd2FybSBvcHRpbWl6YXRpb24gYW5kIGJvdGggYXJ0aWZpY2lhbCBsaWZlIGFuZCBnZW5ldGljIGFsZ29yaXRobXMgYXJlIGRlc2NyaWJlZA0KQzMgIC0gLCBJRUVFIEludGVybmF0aW9uYWwgQ29uZmVyZW5jZSBvbiBOZXVyYWwgTmV0d29ya3MsIDE5OTUuIFByb2NlZWRpbmdzDQpEQSAgLSAxOTk1LzExLy8NClBZICAtIDE5OTUNCkRPICAtIDEwLjExMDkvSUNOTi4xOTk1LjQ4ODk2OA0KVkwgIC0gNA0KU1AgIC0gMTk0Mg0KRVAgIC0gMTk0OCB2b2wuNA0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. Kennedy and R. Eberhart, “Particle swarm optimization,” in <i>, IEEE International Conference on Neural Networks, 1995. Proceedings</i>, 1995, vol. 4, pp. 1942–1948 vol.4.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2FICNN.1995.488968&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=proceeding&amp;rft.atitle=Particle%20swarm%20optimization&amp;rft.btitle=%2C%20IEEE%20International%20Conference%20on%20Neural%20Networks%2C%201995.%20Proceedings&amp;rft.aufirst=J.&amp;rft.aulast=Kennedy&amp;rft.au=J.%20Kennedy&amp;rft.au=R.%20Eberhart&amp;rft.date=1995-11&amp;rft.pages=1942%E2%80%931948%20vol.4"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Learning long-term dependencies with gradient descent is difficult</span> <span class="containertitle">IEEE Transactions on Neural Networks</span> (March 1994) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">Y. Bengio, P. Simard, and P. Frasconi, “<span class="doctitle">Learning long-term dependencies with gradient descent is difficult</span>,” <i>IEEE Transactions on Neural Networks</i>, vol. 5, no. 2, pp. 157–166, Mar. 1994.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1994 type__journalArticle ""</span></div><div class="bib-venue">IEEE Transactions on Neural Networks</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Recurrent neural networks can be used to map input sequences to output sequences‚ such as for recognition‚ production or prediction problems. However‚ practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem‚ alternatives to standard gradient descent are considered</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{bengio1994learning,
	title = {Learning long-term dependencies with gradient descent is difficult},
	volume = {5},
	issn = {1045-9227},
	doi = {10.1109/72.279181},
	number = {2},
	journal = {IEEE Transactions on Neural Networks},
	author = {Bengio, Y. and Simard, P. and Frasconi, P.},
	month = mar,
	year = {1994},
	pages = {157--166}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gTGVhcm5pbmcgbG9uZy10ZXJtIGRlcGVuZGVuY2llcyB3aXRoIGdyYWRpZW50IGRlc2NlbnQgaXMgZGlmZmljdWx0DQpBVSAgLSBCZW5naW8sIFkuDQpBVSAgLSBTaW1hcmQsIFAuDQpBVSAgLSBGcmFzY29uaSwgUC4NClQyICAtIElFRUUgVHJhbnNhY3Rpb25zIG9uIE5ldXJhbCBOZXR3b3Jrcw0KQUIgIC0gUmVjdXJyZW50IG5ldXJhbCBuZXR3b3JrcyBjYW4gYmUgdXNlZCB0byBtYXAgaW5wdXQgc2VxdWVuY2VzIHRvIG91dHB1dCBzZXF1ZW5jZXMsIHN1Y2ggYXMgZm9yIHJlY29nbml0aW9uLCBwcm9kdWN0aW9uIG9yIHByZWRpY3Rpb24gcHJvYmxlbXMuIEhvd2V2ZXIsIHByYWN0aWNhbCBkaWZmaWN1bHRpZXMgaGF2ZSBiZWVuIHJlcG9ydGVkIGluIHRyYWluaW5nIHJlY3VycmVudCBuZXVyYWwgbmV0d29ya3MgdG8gcGVyZm9ybSB0YXNrcyBpbiB3aGljaCB0aGUgdGVtcG9yYWwgY29udGluZ2VuY2llcyBwcmVzZW50IGluIHRoZSBpbnB1dC9vdXRwdXQgc2VxdWVuY2VzIHNwYW4gbG9uZyBpbnRlcnZhbHMuIFdlIHNob3cgd2h5IGdyYWRpZW50IGJhc2VkIGxlYXJuaW5nIGFsZ29yaXRobXMgZmFjZSBhbiBpbmNyZWFzaW5nbHkgZGlmZmljdWx0IHByb2JsZW0gYXMgdGhlIGR1cmF0aW9uIG9mIHRoZSBkZXBlbmRlbmNpZXMgdG8gYmUgY2FwdHVyZWQgaW5jcmVhc2VzLiBUaGVzZSByZXN1bHRzIGV4cG9zZSBhIHRyYWRlLW9mZiBiZXR3ZWVuIGVmZmljaWVudCBsZWFybmluZyBieSBncmFkaWVudCBkZXNjZW50IGFuZCBsYXRjaGluZyBvbiBpbmZvcm1hdGlvbiBmb3IgbG9uZyBwZXJpb2RzLiBCYXNlZCBvbiBhbiB1bmRlcnN0YW5kaW5nIG9mIHRoaXMgcHJvYmxlbSwgYWx0ZXJuYXRpdmVzIHRvIHN0YW5kYXJkIGdyYWRpZW50IGRlc2NlbnQgYXJlIGNvbnNpZGVyZWQNCkRBICAtIDE5OTQvMDMvLw0KUFkgIC0gMTk5NA0KRE8gIC0gMTAuMTEwOS83Mi4yNzkxODENClZMICAtIDUNCklTICAtIDINClNQICAtIDE1Nw0KRVAgIC0gMTY2DQpTTiAgLSAxMDQ1LTkyMjcNCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term dependencies with gradient descent is difficult,” <i>IEEE Transactions on Neural Networks</i>, vol. 5, no. 2, pp. 157–166, Mar. 1994.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1109%2F72.279181&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Learning%20long-term%20dependencies%20with%20gradient%20descent%20is%20difficult&amp;rft.jtitle=IEEE%20Transactions%20on%20Neural%20Networks&amp;rft.volume=5&amp;rft.issue=2&amp;rft.aufirst=Y.&amp;rft.aulast=Bengio&amp;rft.au=Y.%20Bengio&amp;rft.au=P.%20Simard&amp;rft.au=P.%20Frasconi&amp;rft.date=1994-03&amp;rft.pages=157%E2%80%93166&amp;rft.issn=1045-9227"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Chapter 49 Arch models</span> <span class="containertitle"></span> (January 1994) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">T. Bollerslev, R. F. Engle, and D. B. Nelson, “<span class="doctitle"><a class="doctitle" href="http://www.sciencedirect.com/science/article/pii/ S1573441205800182">Chapter 49 Arch models</a></span>,” in <i>Handbook of Econometrics</i>, vol. 4, Elsevier, 1994, pp. 2959–3038.</div>
  </div><div class="bib-extra">DOI: 10.1016/S1573-4412(05)80018-2</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1994 type__bookSection ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">This chapter evaluates the most important theoretical developments in ARCH type modeling of time-varying conditional variances. The coverage include the specification of univariate parametric ARCH models‚ general inference procedures‚ conditions for stationarity and ergodicity‚ continuous time methods‚ aggregation and forecasting of ARCH models‚ multivariate conditional covariance formulations‚ and the use of model selection criteria in an ARCH context. Additionally‚ the chapter contains a discussion of the empirical regularities pertaining to the temporal variation in financial market volatility. Motivated in part by recent results on optimal filtering‚ a new conditional variance model for better characterizing stock return volatility is also presented.</div></div></div><div class="blink"><a   href="http://www.sciencedirect.com/science/article/pii/ S1573441205800182" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@incollection{bollerslev1994chapter,
	title = {Chapter 49 {Arch} models},
	volume = {4},
	url = {http://www.sciencedirect.com/science/article/pii/ S1573441205800182},
	booktitle = {Handbook of {Econometrics}},
	publisher = {Elsevier},
	author = {Bollerslev, Tim and Engle, Robert F. and Nelson, Daniel B.},
	month = jan,
	year = {1994},
	doi = {10.1016/S1573-4412(05)80018-2},
	pages = {2959--3038}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQ0hBUA0KVEkgIC0gQ2hhcHRlciA0OSBBcmNoIG1vZGVscw0KQVUgIC0gQm9sbGVyc2xldiwgVGltDQpBVSAgLSBFbmdsZSwgUm9iZXJ0IEYuDQpBVSAgLSBOZWxzb24sIERhbmllbCBCLg0KVDIgIC0gSGFuZGJvb2sgb2YgRWNvbm9tZXRyaWNzDQpBQiAgLSBUaGlzIGNoYXB0ZXIgZXZhbHVhdGVzIHRoZSBtb3N0IGltcG9ydGFudCB0aGVvcmV0aWNhbCBkZXZlbG9wbWVudHMgaW4gQVJDSCB0eXBlIG1vZGVsaW5nIG9mIHRpbWUtdmFyeWluZyBjb25kaXRpb25hbCB2YXJpYW5jZXMuIFRoZSBjb3ZlcmFnZSBpbmNsdWRlIHRoZSBzcGVjaWZpY2F0aW9uIG9mIHVuaXZhcmlhdGUgcGFyYW1ldHJpYyBBUkNIIG1vZGVscywgZ2VuZXJhbCBpbmZlcmVuY2UgcHJvY2VkdXJlcywgY29uZGl0aW9ucyBmb3Igc3RhdGlvbmFyaXR5IGFuZCBlcmdvZGljaXR5LCBjb250aW51b3VzIHRpbWUgbWV0aG9kcywgYWdncmVnYXRpb24gYW5kIGZvcmVjYXN0aW5nIG9mIEFSQ0ggbW9kZWxzLCBtdWx0aXZhcmlhdGUgY29uZGl0aW9uYWwgY292YXJpYW5jZSBmb3JtdWxhdGlvbnMsIGFuZCB0aGUgdXNlIG9mIG1vZGVsIHNlbGVjdGlvbiBjcml0ZXJpYSBpbiBhbiBBUkNIIGNvbnRleHQuIEFkZGl0aW9uYWxseSwgdGhlIGNoYXB0ZXIgY29udGFpbnMgYSBkaXNjdXNzaW9uIG9mIHRoZSBlbXBpcmljYWwgcmVndWxhcml0aWVzIHBlcnRhaW5pbmcgdG8gdGhlIHRlbXBvcmFsIHZhcmlhdGlvbiBpbiBmaW5hbmNpYWwgbWFya2V0IHZvbGF0aWxpdHkuIE1vdGl2YXRlZCBpbiBwYXJ0IGJ5IHJlY2VudCByZXN1bHRzIG9uIG9wdGltYWwgZmlsdGVyaW5nLCBhIG5ldyBjb25kaXRpb25hbCB2YXJpYW5jZSBtb2RlbCBmb3IgYmV0dGVyIGNoYXJhY3Rlcml6aW5nIHN0b2NrIHJldHVybiB2b2xhdGlsaXR5IGlzIGFsc28gcHJlc2VudGVkLg0KREEgIC0gMTk5NC8wMS8vDQpQWSAgLSAxOTk0DQpWTCAgLSA0DQpTUCAgLSAyOTU5DQpFUCAgLSAzMDM4DQpQQiAgLSBFbHNldmllcg0KVVIgIC0gaHR0cDovL3d3dy5zY2llbmNlZGlyZWN0LmNvbS9zY2llbmNlL2FydGljbGUvcGlpLyBTMTU3MzQ0MTIwNTgwMDE4Mg0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">T. Bollerslev, R. F. Engle, and D. B. Nelson, “Chapter 49 Arch models,” in <i>Handbook of Econometrics</i>, vol. 4, Elsevier, 1994, pp. 2959–3038.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Chapter%2049%20Arch%20models&amp;rft.btitle=Handbook%20of%20Econometrics&amp;rft.publisher=Elsevier&amp;rft.aufirst=Tim&amp;rft.aulast=Bollerslev&amp;rft.au=Tim%20Bollerslev&amp;rft.au=Robert%20F.%20Engle&amp;rft.au=Daniel%20B.%20Nelson&amp;rft.date=1994&amp;rft.pages=2959%E2%80%933038"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Neural Net Architectures for Temporal Sequence Processing</span> <span class="containertitle"></span> (March 1993) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">M. Mozer, “<span class="doctitle">Neural Net Architectures for Temporal Sequence Processing</span>,” Mar. 1993.</div>
  </div><div class="bib-extra">00381</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1993 type__journalArticle ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">I present a general taxonomy of neural net architectures for processing time-varying patterns. This taxonomy subsumes many existing architectures in the literature‚ and points to several promising architectures that have yet to be examined. Any architecture that processes timevarying patterns requires two conceptually distinct components: a short-term memory that holds on to relevant past events and an associator that uses the short-term memory to classify or predict. My taxonomy is based on a characterization of short-term memory models along the dimensions of form‚ content‚ and adaptability. Experiments on predicting future values of a financial time series (US dollar–Swiss franc exchange rates) are presented using several alternative memory models. The results of these experiments serve as a baseline against which more sophisticated architectures can be compared. Neural networks have proven to be a promising alternative to traditional techniques for nonlinear temporal pre...</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{mozer1993neural,
	title = {Neural {Net} {Architectures} for {Temporal} {Sequence} {Processing}},
	author = {Mozer, Michael},
	month = mar,
	year = {1993},
	note = {00381}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gTmV1cmFsIE5ldCBBcmNoaXRlY3R1cmVzIGZvciBUZW1wb3JhbCBTZXF1ZW5jZSBQcm9jZXNzaW5nDQpBVSAgLSBNb3plciwgTWljaGFlbA0KQUIgIC0gSSBwcmVzZW50IGEgZ2VuZXJhbCB0YXhvbm9teSBvZiBuZXVyYWwgbmV0IGFyY2hpdGVjdHVyZXMgZm9yIHByb2Nlc3NpbmcgdGltZS12YXJ5aW5nIHBhdHRlcm5zLiBUaGlzIHRheG9ub215IHN1YnN1bWVzIG1hbnkgZXhpc3RpbmcgYXJjaGl0ZWN0dXJlcyBpbiB0aGUgbGl0ZXJhdHVyZSwgYW5kIHBvaW50cyB0byBzZXZlcmFsIHByb21pc2luZyBhcmNoaXRlY3R1cmVzIHRoYXQgaGF2ZSB5ZXQgdG8gYmUgZXhhbWluZWQuIEFueSBhcmNoaXRlY3R1cmUgdGhhdCBwcm9jZXNzZXMgdGltZXZhcnlpbmcgcGF0dGVybnMgcmVxdWlyZXMgdHdvIGNvbmNlcHR1YWxseSBkaXN0aW5jdCBjb21wb25lbnRzOiBhIHNob3J0LXRlcm0gbWVtb3J5IHRoYXQgaG9sZHMgb24gdG8gcmVsZXZhbnQgcGFzdCBldmVudHMgYW5kIGFuIGFzc29jaWF0b3IgdGhhdCB1c2VzIHRoZSBzaG9ydC10ZXJtIG1lbW9yeSB0byBjbGFzc2lmeSBvciBwcmVkaWN0LiBNeSB0YXhvbm9teSBpcyBiYXNlZCBvbiBhIGNoYXJhY3Rlcml6YXRpb24gb2Ygc2hvcnQtdGVybSBtZW1vcnkgbW9kZWxzIGFsb25nIHRoZSBkaW1lbnNpb25zIG9mIGZvcm0sIGNvbnRlbnQsIGFuZCBhZGFwdGFiaWxpdHkuIEV4cGVyaW1lbnRzIG9uIHByZWRpY3RpbmcgZnV0dXJlIHZhbHVlcyBvZiBhIGZpbmFuY2lhbCB0aW1lIHNlcmllcyAoVVMgZG9sbGFy4oCTU3dpc3MgZnJhbmMgZXhjaGFuZ2UgcmF0ZXMpIGFyZSBwcmVzZW50ZWQgdXNpbmcgc2V2ZXJhbCBhbHRlcm5hdGl2ZSBtZW1vcnkgbW9kZWxzLiBUaGUgcmVzdWx0cyBvZiB0aGVzZSBleHBlcmltZW50cyBzZXJ2ZSBhcyBhIGJhc2VsaW5lIGFnYWluc3Qgd2hpY2ggbW9yZSBzb3BoaXN0aWNhdGVkIGFyY2hpdGVjdHVyZXMgY2FuIGJlIGNvbXBhcmVkLiBOZXVyYWwgbmV0d29ya3MgaGF2ZSBwcm92ZW4gdG8gYmUgYSBwcm9taXNpbmcgYWx0ZXJuYXRpdmUgdG8gdHJhZGl0aW9uYWwgdGVjaG5pcXVlcyBmb3Igbm9ubGluZWFyIHRlbXBvcmFsIHByZS4uLg0KREEgIC0gMTk5My8wMy8vDQpQWSAgLSAxOTkzDQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">M. Mozer, “Neural Net Architectures for Temporal Sequence Processing,” Mar. 1993.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Neural%20Net%20Architectures%20for%20Temporal%20Sequence%20Processing&amp;rft.aufirst=Michael&amp;rft.aulast=Mozer&amp;rft.au=Michael%20Mozer&amp;rft.date=1993-03"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Testing the null hypothesis of stationarity against the alternative of a unit root: How sure are we that economic time series have a unit root?</span> <span class="containertitle">Journal of Econometrics</span> (October 1992) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. Kwiatkowski, P. C. B. Phillips, P. Schmidt, and Y. Shin, “<span class="doctitle"><a class="doctitle" href="http://www.sciencedirect.com/science/article/pii/ 030440769290104Y">Testing the null hypothesis of stationarity against the alternative of a unit root: How sure are we that economic time series have a unit root?</a></span>,” <i>Journal of Econometrics</i>, vol. 54, no. 1, pp. 159–178, Oct. 1992.</div>
  </div><div class="bib-extra">10948</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1992 type__journalArticle ""</span></div><div class="bib-venue">Journal of Econometrics</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">We propose a test of the null hypothesis that an observable series is stationary around a deterministic trend. The series is expressed as the sum of deterministic trend‚ random walk‚ and stationary error‚ and the test is the LM test of the hypothesis that the random walk has zero variance. The asymptotic distribution of the statistic is derived under the null and under the alternative that the series is difference-stationary. Finite sample size and power are considered in a Monte Carlo experiment. The test is applied to the Nelson-Plosser data‚ and for many of these series the hypothesis of trend stationarity cannot be rejected.</div></div></div><div class="blink"><a   href="http://www.sciencedirect.com/science/article/pii/ 030440769290104Y" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{kwiatkowski1992testing,
	title = {Testing the null hypothesis of stationarity against the alternative of a unit root: {How} sure are we that economic time series have a unit root?},
	volume = {54},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/ 030440769290104Y},
	doi = {10.1016/0304-4076(92)90104-Y},
	number = {1},
	journal = {Journal of Econometrics},
	author = {Kwiatkowski, Denis and Phillips, Peter C. B. and Schmidt, Peter and Shin, Yongcheol},
	month = oct,
	year = {1992},
	note = {10948},
	pages = {159--178}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gVGVzdGluZyB0aGUgbnVsbCBoeXBvdGhlc2lzIG9mIHN0YXRpb25hcml0eSBhZ2FpbnN0IHRoZSBhbHRlcm5hdGl2ZSBvZiBhIHVuaXQgcm9vdDogSG93IHN1cmUgYXJlIHdlIHRoYXQgZWNvbm9taWMgdGltZSBzZXJpZXMgaGF2ZSBhIHVuaXQgcm9vdD8NCkFVICAtIEt3aWF0a293c2tpLCBEZW5pcw0KQVUgIC0gUGhpbGxpcHMsIFBldGVyIEMuIEIuDQpBVSAgLSBTY2htaWR0LCBQZXRlcg0KQVUgIC0gU2hpbiwgWW9uZ2NoZW9sDQpUMiAgLSBKb3VybmFsIG9mIEVjb25vbWV0cmljcw0KQUIgIC0gV2UgcHJvcG9zZSBhIHRlc3Qgb2YgdGhlIG51bGwgaHlwb3RoZXNpcyB0aGF0IGFuIG9ic2VydmFibGUgc2VyaWVzIGlzIHN0YXRpb25hcnkgYXJvdW5kIGEgZGV0ZXJtaW5pc3RpYyB0cmVuZC4gVGhlIHNlcmllcyBpcyBleHByZXNzZWQgYXMgdGhlIHN1bSBvZiBkZXRlcm1pbmlzdGljIHRyZW5kLCByYW5kb20gd2FsaywgYW5kIHN0YXRpb25hcnkgZXJyb3IsIGFuZCB0aGUgdGVzdCBpcyB0aGUgTE0gdGVzdCBvZiB0aGUgaHlwb3RoZXNpcyB0aGF0IHRoZSByYW5kb20gd2FsayBoYXMgemVybyB2YXJpYW5jZS4gVGhlIGFzeW1wdG90aWMgZGlzdHJpYnV0aW9uIG9mIHRoZSBzdGF0aXN0aWMgaXMgZGVyaXZlZCB1bmRlciB0aGUgbnVsbCBhbmQgdW5kZXIgdGhlIGFsdGVybmF0aXZlIHRoYXQgdGhlIHNlcmllcyBpcyBkaWZmZXJlbmNlLXN0YXRpb25hcnkuIEZpbml0ZSBzYW1wbGUgc2l6ZSBhbmQgcG93ZXIgYXJlIGNvbnNpZGVyZWQgaW4gYSBNb250ZSBDYXJsbyBleHBlcmltZW50LiBUaGUgdGVzdCBpcyBhcHBsaWVkIHRvIHRoZSBOZWxzb24tUGxvc3NlciBkYXRhLCBhbmQgZm9yIG1hbnkgb2YgdGhlc2Ugc2VyaWVzIHRoZSBoeXBvdGhlc2lzIG9mIHRyZW5kIHN0YXRpb25hcml0eSBjYW5ub3QgYmUgcmVqZWN0ZWQuDQpEQSAgLSAxOTkyLzEwLy8NClBZICAtIDE5OTINCkRPICAtIDEwLjEwMTYvMDMwNC00MDc2KDkyKTkwMTA0LVkNClZMICAtIDU0DQpJUyAgLSAxDQpTUCAgLSAxNTkNCkVQICAtIDE3OA0KU04gIC0gMDMwNC00MDc2DQpVUiAgLSBodHRwOi8vd3d3LnNjaWVuY2VkaXJlY3QuY29tL3NjaWVuY2UvYXJ0aWNsZS9waWkvIDAzMDQ0MDc2OTI5MDEwNFkNCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. Kwiatkowski, P. C. B. Phillips, P. Schmidt, and Y. Shin, “Testing the null hypothesis of stationarity against the alternative of a unit root: How sure are we that economic time series have a unit root?,” <i>Journal of Econometrics</i>, vol. 54, no. 1, pp. 159–178, Oct. 1992.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2F0304-4076(92)90104-Y&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Testing%20the%20null%20hypothesis%20of%20stationarity%20against%20the%20alternative%20of%20a%20unit%20root%3A%20How%20sure%20are%20we%20that%20economic%20time%20series%20have%20a%20unit%20root%3F&amp;rft.jtitle=Journal%20of%20Econometrics&amp;rft.volume=54&amp;rft.issue=1&amp;rft.aufirst=Denis&amp;rft.aulast=Kwiatkowski&amp;rft.au=Denis%20Kwiatkowski&amp;rft.au=Peter%20C.%20B.%20Phillips&amp;rft.au=Peter%20Schmidt&amp;rft.au=Yongcheol%20Shin&amp;rft.date=1992-10&amp;rft.pages=159%E2%80%93178&amp;rft.issn=0304-4076"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Backpropagation Applied to Handwritten Zip Code Recognition</span> <span class="containertitle">Neural Computation</span> (December 1989) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">Y. LeCun <i>et al.</i>, “<span class="doctitle"><a class="doctitle" href="https://doi.org/10.1162/neco.1989.1.4.541">Backpropagation Applied to Handwritten Zip Code Recognition</a></span>,” <i>Neural Computation</i>, vol. 1, no. 4, pp. 541–551, Dec. 1989.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1989 type__journalArticle ""</span></div><div class="bib-venue">Neural Computation</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation‚ going from the normalized image of the character to the final classification.</div></div></div><div class="blink"><a   href="https://doi.org/10.1162/neco.1989.1.4.541" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{lecun1989backpropagation,
	title = {Backpropagation {Applied} to {Handwritten} {Zip} {Code} {Recognition}},
	volume = {1},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1989.1.4.541},
	doi = {10.1162/neco.1989.1.4.541},
	number = {4},
	journal = {Neural Computation},
	author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
	month = dec,
	year = {1989},
	pages = {541--551}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gQmFja3Byb3BhZ2F0aW9uIEFwcGxpZWQgdG8gSGFuZHdyaXR0ZW4gWmlwIENvZGUgUmVjb2duaXRpb24NCkFVICAtIExlQ3VuLCBZLg0KQVUgIC0gQm9zZXIsIEIuDQpBVSAgLSBEZW5rZXIsIEouIFMuDQpBVSAgLSBIZW5kZXJzb24sIEQuDQpBVSAgLSBIb3dhcmQsIFIuIEUuDQpBVSAgLSBIdWJiYXJkLCBXLg0KQVUgIC0gSmFja2VsLCBMLiBELg0KVDIgIC0gTmV1cmFsIENvbXB1dGF0aW9uDQpBQiAgLSBUaGUgYWJpbGl0eSBvZiBsZWFybmluZyBuZXR3b3JrcyB0byBnZW5lcmFsaXplIGNhbiBiZSBncmVhdGx5IGVuaGFuY2VkIGJ5IHByb3ZpZGluZyBjb25zdHJhaW50cyBmcm9tIHRoZSB0YXNrIGRvbWFpbi4gVGhpcyBwYXBlciBkZW1vbnN0cmF0ZXMgaG93IHN1Y2ggY29uc3RyYWludHMgY2FuIGJlIGludGVncmF0ZWQgaW50byBhIGJhY2twcm9wYWdhdGlvbiBuZXR3b3JrIHRocm91Z2ggdGhlIGFyY2hpdGVjdHVyZSBvZiB0aGUgbmV0d29yay4gVGhpcyBhcHByb2FjaCBoYXMgYmVlbiBzdWNjZXNzZnVsbHkgYXBwbGllZCB0byB0aGUgcmVjb2duaXRpb24gb2YgaGFuZHdyaXR0ZW4gemlwIGNvZGUgZGlnaXRzIHByb3ZpZGVkIGJ5IHRoZSBVLlMuIFBvc3RhbCBTZXJ2aWNlLiBBIHNpbmdsZSBuZXR3b3JrIGxlYXJucyB0aGUgZW50aXJlIHJlY29nbml0aW9uIG9wZXJhdGlvbiwgZ29pbmcgZnJvbSB0aGUgbm9ybWFsaXplZCBpbWFnZSBvZiB0aGUgY2hhcmFjdGVyIHRvIHRoZSBmaW5hbCBjbGFzc2lmaWNhdGlvbi4NCkRBICAtIDE5ODkvMTIvLw0KUFkgIC0gMTk4OQ0KRE8gIC0gMTAuMTE2Mi9uZWNvLjE5ODkuMS40LjU0MQ0KVkwgIC0gMQ0KSVMgIC0gNA0KU1AgIC0gNTQxDQpFUCAgLSA1NTENClNOICAtIDA4OTktNzY2Nw0KVVIgIC0gaHR0cHM6Ly9kb2kub3JnLzEwLjExNjIvbmVjby4xOTg5LjEuNC41NDENCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">Y. LeCun <i>et al.</i>, “Backpropagation Applied to Handwritten Zip Code Recognition,” <i>Neural Computation</i>, vol. 1, no. 4, pp. 541–551, Dec. 1989.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.1989.1.4.541&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Backpropagation%20Applied%20to%20Handwritten%20Zip%20Code%20Recognition&amp;rft.jtitle=Neural%20Computation&amp;rft.volume=1&amp;rft.issue=4&amp;rft.aufirst=Y.&amp;rft.aulast=LeCun&amp;rft.au=Y.%20LeCun&amp;rft.au=B.%20Boser&amp;rft.au=J.%20S.%20Denker&amp;rft.au=D.%20Henderson&amp;rft.au=R.%20E.%20Howard&amp;rft.au=W.%20Hubbard&amp;rft.au=L.%20D.%20Jackel&amp;rft.date=1989-12&amp;rft.pages=541%E2%80%93551&amp;rft.issn=0899-7667"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Multilayer feedforward networks are universal approximators</span> <span class="containertitle">Neural Networks</span> (January 1989) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">K. Hornik, M. Stinchcombe, and H. White, “<span class="doctitle"><a class="doctitle" href="http://www.sciencedirect.com/science/article/pii/ 0893608089900208">Multilayer feedforward networks are universal approximators</a></span>,” <i>Neural Networks</i>, vol. 2, no. 5, pp. 359–366, Jan. 1989.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1989 type__journalArticle ""</span></div><div class="bib-venue">Neural Networks</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy‚ provided sufficiently many hidden units are available. In this sense‚ multilayer feedforward networks are a class of universal approximators.</div></div></div><div class="blink"><a   href="http://www.sciencedirect.com/science/article/pii/ 0893608089900208" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{hornik1989multilayer,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/ 0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	number = {5},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	pages = {359--366}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gTXVsdGlsYXllciBmZWVkZm9yd2FyZCBuZXR3b3JrcyBhcmUgdW5pdmVyc2FsIGFwcHJveGltYXRvcnMNCkFVICAtIEhvcm5paywgS3VydA0KQVUgIC0gU3RpbmNoY29tYmUsIE1heHdlbGwNCkFVICAtIFdoaXRlLCBIYWxiZXJ0DQpUMiAgLSBOZXVyYWwgTmV0d29ya3MNCkFCICAtIFRoaXMgcGFwZXIgcmlnb3JvdXNseSBlc3RhYmxpc2hlcyB0aGF0IHN0YW5kYXJkIG11bHRpbGF5ZXIgZmVlZGZvcndhcmQgbmV0d29ya3Mgd2l0aCBhcyBmZXcgYXMgb25lIGhpZGRlbiBsYXllciB1c2luZyBhcmJpdHJhcnkgc3F1YXNoaW5nIGZ1bmN0aW9ucyBhcmUgY2FwYWJsZSBvZiBhcHByb3hpbWF0aW5nIGFueSBCb3JlbCBtZWFzdXJhYmxlIGZ1bmN0aW9uIGZyb20gb25lIGZpbml0ZSBkaW1lbnNpb25hbCBzcGFjZSB0byBhbm90aGVyIHRvIGFueSBkZXNpcmVkIGRlZ3JlZSBvZiBhY2N1cmFjeSwgcHJvdmlkZWQgc3VmZmljaWVudGx5IG1hbnkgaGlkZGVuIHVuaXRzIGFyZSBhdmFpbGFibGUuIEluIHRoaXMgc2Vuc2UsIG11bHRpbGF5ZXIgZmVlZGZvcndhcmQgbmV0d29ya3MgYXJlIGEgY2xhc3Mgb2YgdW5pdmVyc2FsIGFwcHJveGltYXRvcnMuDQpEQSAgLSAxOTg5LzAxLy8NClBZICAtIDE5ODkNCkRPICAtIDEwLjEwMTYvMDg5My02MDgwKDg5KTkwMDIwLTgNClZMICAtIDINCklTICAtIDUNClNQICAtIDM1OQ0KRVAgIC0gMzY2DQpTTiAgLSAwODkzLTYwODANClVSICAtIGh0dHA6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL3BpaS8gMDg5MzYwODA4OTkwMDIwOA0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward networks are universal approximators,” <i>Neural Networks</i>, vol. 2, no. 5, pp. 359–366, Jan. 1989.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2F0893-6080(89)90020-8&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Multilayer%20feedforward%20networks%20are%20universal%20approximators&amp;rft.jtitle=Neural%20Networks&amp;rft.volume=2&amp;rft.issue=5&amp;rft.aufirst=Kurt&amp;rft.aulast=Hornik&amp;rft.au=Kurt%20Hornik&amp;rft.au=Maxwell%20Stinchcombe&amp;rft.au=Halbert%20White&amp;rft.date=1989&amp;rft.pages=359%E2%80%93366&amp;rft.issn=0893-6080"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Testing for a Unit Root in Time Series Regression</span> <span class="containertitle">Biometrika</span> (1988) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">P. C. B. Phillips and P. Perron, “<span class="doctitle"><a class="doctitle" href="http://www.jstor.org/stable/2336182">Testing for a Unit Root in Time Series Regression</a></span>,” <i>Biometrika</i>, vol. 75, no. 2, pp. 335–346, 1988.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1988 venue_short__Biometrika type__journalArticle ""</span></div><div class="bib-venue">Biometrika</div><div class="bib-venue-short">Biometrika</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">This paper proposes new tests for detecting the presence of a unit root in quite general time series models. Our approach is nonparametric with respect to nuisance parameters and thereby allows for a very wide class of weakly dependent and possibly heterogeneously distributed data. The tests accommodate models with a fitted drift and a time trend so that they may be used to discriminate between unit root nonstationarity and stationarity about a deterministic trend. The limiting distributions of the statistics are obtained under both the unit root null and a sequence of local alternatives. The latter noncentral distribution theory yields local asymptotic power functions for the tests and facilitates comparisons with alternative procedures due to Dickey &amp; Fuller. Simulations are reported on the performance of the new tests in finite samples.</div></div></div><div class="blink"><a   href="http://www.jstor.org/stable/2336182" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{phillips1988testing,
	title = {Testing for a {Unit} {Root} in {Time} {Series} {Regression}},
	volume = {75},
	issn = {0006-3444},
	url = {http://www.jstor.org/stable/2336182},
	doi = {10.2307/2336182},
	number = {2},
	journal = {Biometrika},
	author = {Phillips, Peter C. B. and Perron, Pierre},
	year = {1988},
	pages = {335--346}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gVGVzdGluZyBmb3IgYSBVbml0IFJvb3QgaW4gVGltZSBTZXJpZXMgUmVncmVzc2lvbg0KQVUgIC0gUGhpbGxpcHMsIFBldGVyIEMuIEIuDQpBVSAgLSBQZXJyb24sIFBpZXJyZQ0KVDIgIC0gQmlvbWV0cmlrYQ0KQUIgIC0gVGhpcyBwYXBlciBwcm9wb3NlcyBuZXcgdGVzdHMgZm9yIGRldGVjdGluZyB0aGUgcHJlc2VuY2Ugb2YgYSB1bml0IHJvb3QgaW4gcXVpdGUgZ2VuZXJhbCB0aW1lIHNlcmllcyBtb2RlbHMuIE91ciBhcHByb2FjaCBpcyBub25wYXJhbWV0cmljIHdpdGggcmVzcGVjdCB0byBudWlzYW5jZSBwYXJhbWV0ZXJzIGFuZCB0aGVyZWJ5IGFsbG93cyBmb3IgYSB2ZXJ5IHdpZGUgY2xhc3Mgb2Ygd2Vha2x5IGRlcGVuZGVudCBhbmQgcG9zc2libHkgaGV0ZXJvZ2VuZW91c2x5IGRpc3RyaWJ1dGVkIGRhdGEuIFRoZSB0ZXN0cyBhY2NvbW1vZGF0ZSBtb2RlbHMgd2l0aCBhIGZpdHRlZCBkcmlmdCBhbmQgYSB0aW1lIHRyZW5kIHNvIHRoYXQgdGhleSBtYXkgYmUgdXNlZCB0byBkaXNjcmltaW5hdGUgYmV0d2VlbiB1bml0IHJvb3Qgbm9uc3RhdGlvbmFyaXR5IGFuZCBzdGF0aW9uYXJpdHkgYWJvdXQgYSBkZXRlcm1pbmlzdGljIHRyZW5kLiBUaGUgbGltaXRpbmcgZGlzdHJpYnV0aW9ucyBvZiB0aGUgc3RhdGlzdGljcyBhcmUgb2J0YWluZWQgdW5kZXIgYm90aCB0aGUgdW5pdCByb290IG51bGwgYW5kIGEgc2VxdWVuY2Ugb2YgbG9jYWwgYWx0ZXJuYXRpdmVzLiBUaGUgbGF0dGVyIG5vbmNlbnRyYWwgZGlzdHJpYnV0aW9uIHRoZW9yeSB5aWVsZHMgbG9jYWwgYXN5bXB0b3RpYyBwb3dlciBmdW5jdGlvbnMgZm9yIHRoZSB0ZXN0cyBhbmQgZmFjaWxpdGF0ZXMgY29tcGFyaXNvbnMgd2l0aCBhbHRlcm5hdGl2ZSBwcm9jZWR1cmVzIGR1ZSB0byBEaWNrZXkgJiBGdWxsZXIuIFNpbXVsYXRpb25zIGFyZSByZXBvcnRlZCBvbiB0aGUgcGVyZm9ybWFuY2Ugb2YgdGhlIG5ldyB0ZXN0cyBpbiBmaW5pdGUgc2FtcGxlcy4NCkRBICAtIDE5ODgvLy8NClBZICAtIDE5ODgNCkRPICAtIDEwLjIzMDcvMjMzNjE4Mg0KVkwgIC0gNzUNCklTICAtIDINClNQICAtIDMzNQ0KRVAgIC0gMzQ2DQpTTiAgLSAwMDA2LTM0NDQNClVSICAtIGh0dHA6Ly93d3cuanN0b3Iub3JnL3N0YWJsZS8yMzM2MTgyDQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">P. C. B. Phillips and P. Perron, “Testing for a Unit Root in Time Series Regression,” <i>Biometrika</i>, vol. 75, no. 2, pp. 335–346, 1988.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.2307%2F2336182&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Testing%20for%20a%20Unit%20Root%20in%20Time%20Series%20Regression&amp;rft.jtitle=Biometrika&amp;rft.volume=75&amp;rft.issue=2&amp;rft.aufirst=Peter%20C.%20B.&amp;rft.aulast=Phillips&amp;rft.au=Peter%20C.%20B.%20Phillips&amp;rft.au=Pierre%20Perron&amp;rft.date=1988&amp;rft.pages=335%E2%80%93346&amp;rft.issn=0006-3444"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Generalization of backpropagation with application to a recurrent gas market model</span> <span class="containertitle">Neural Networks</span> (January 1988) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">P. J. Werbos, “<span class="doctitle"><a class="doctitle" href="http://www.sciencedirect.com/science/article/pii/ 089360808890007X">Generalization of backpropagation with application to a recurrent gas market model</a></span>,” <i>Neural Networks</i>, vol. 1, no. 4, pp. 339–356, Jan. 1988.</div>
  </div><div class="bib-extra">00712</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1988 type__journalArticle ""</span></div><div class="bib-venue">Neural Networks</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Backpropagation is often viewed as a method for adapting artificial neural networks to classify patterns. Based on parts of the book by Rumelhart and colleagues‚ many authors equate backpropagation with the generalized delta rule applied to fully-connected feedforward networks. This paper will summarize a more general formulation of backpropagation‚ developed in 1974‚ which does more justice to the roots of the method in numerical analysis and statistics‚ and also does more justice to creative approaches expressed by neural modelers in the past year or two. It will discuss applications of backpropagation to forecasting over time (where errors have been halved by using methods other than least squares)‚ to optimization‚ to sensitivity analysis‚ and to brain research. This paper will go on to derive a generalization of backpropagation to recurrent systems (which input their own output)‚ such as hybrids of perceptron-style networks and Grossberg/Hopfield networks. Unlike the proposal of Rumelhart‚ Hinton‚ and Williams‚ this generalization does not require the storage of intermediate iterations to deal with continuous recurrence. This generalization was applied in 1981 to a model of natural gas markets‚ where it located sources of forecast uncertainty related to the use of least squares to estimate the model parameters in the first place.</div></div></div><div class="blink"><a   href="http://www.sciencedirect.com/science/article/pii/ 089360808890007X" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{werbos1988generalization,
	title = {Generalization of backpropagation with application to a recurrent gas market model},
	volume = {1},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/ 089360808890007X},
	doi = {10.1016/0893-6080(88)90007-X},
	number = {4},
	journal = {Neural Networks},
	author = {Werbos, Paul J.},
	month = jan,
	year = {1988},
	note = {00712},
	pages = {339--356}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('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');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">P. J. Werbos, “Generalization of backpropagation with application to a recurrent gas market model,” <i>Neural Networks</i>, vol. 1, no. 4, pp. 339–356, Jan. 1988.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2F0893-6080(88)90007-X&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Generalization%20of%20backpropagation%20with%20application%20to%20a%20recurrent%20gas%20market%20model&amp;rft.jtitle=Neural%20Networks&amp;rft.volume=1&amp;rft.issue=4&amp;rft.aufirst=Paul%20J.&amp;rft.aulast=Werbos&amp;rft.au=Paul%20J.%20Werbos&amp;rft.date=1988&amp;rft.pages=339%E2%80%93356&amp;rft.issn=0893-6080"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">The Utility Driven Dynamic Error Propagation Network</span> <span class="containertitle"></span> (1987) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">A. J. Robinson and F. Fallside, “<span class="doctitle">The Utility Driven Dynamic Error Propagation Network</span>,” Engineering Department, Cambridge University, Cambridge, UK, CUED/F-INFENG/TR.1, 1987.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1987 type__report ""</span></div><div class="blinkitems"><div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@techreport{robinson1987utility,
	address = {Cambridge, UK},
	title = {The {Utility} {Driven} {Dynamic} {Error} {Propagation} {Network}},
	number = {CUED/F-INFENG/TR.1},
	institution = {Engineering Department, Cambridge University},
	author = {Robinson, A. J. and Fallside, Frank},
	year = {1987}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gUlBSVA0KVEkgIC0gVGhlIFV0aWxpdHkgRHJpdmVuIER5bmFtaWMgRXJyb3IgUHJvcGFnYXRpb24gTmV0d29yaw0KQVUgIC0gUm9iaW5zb24sIEEuIEouDQpBVSAgLSBGYWxsc2lkZSwgRnJhbmsNCkNZICAtIENhbWJyaWRnZSwgVUsNCkRBICAtIDE5ODcvLy8NClBZICAtIDE5ODcNClBCICAtIEVuZ2luZWVyaW5nIERlcGFydG1lbnQsIENhbWJyaWRnZSBVbml2ZXJzaXR5DQpTTiAgLSBDVUVEL0YtSU5GRU5HL1RSLjENCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">A. J. Robinson and F. Fallside, “The Utility Driven Dynamic Error Propagation Network,” Engineering Department, Cambridge University, Cambridge, UK, CUED/F-INFENG/TR.1, 1987.</div>
  </div></div></div></div></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Forecasting the volatility of currency exchange rates</span> <span class="containertitle">International Journal of Forecasting</span> (January 1987) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">S. J. Taylor, “<span class="doctitle"><a class="doctitle" href="http://www.sciencedirect.com/science/article/pii/ 0169207087900859">Forecasting the volatility of currency exchange rates</a></span>,” <i>International Journal of Forecasting</i>, vol. 3, no. 1, pp. 159–170, Jan. 1987.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1987 type__journalArticle ""</span></div><div class="bib-venue">International Journal of Forecasting</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Currency volatility is defined to be the standard deviation of day-to-day changes in the logarithm of the exchange rate. After a discussion of statistical models for exchange rates‚ the paper describes methods for choosing and assessing volatility forecasts using open‚ high‚ low and close prices. Results for DM/ futures prices at the IMM in Chicago from 1977 to 1983 show high and low prices are valuable when seeking accurate volatility forecasts. The best forecasts are a weighted average of present and past high‚ low and close prices‚ with adjustments for weekend and holiday effects. The forecasts can be used to value currency options.</div></div></div><div class="blink"><a   href="http://www.sciencedirect.com/science/article/pii/ 0169207087900859" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{taylor1987forecasting,
	series = {Special {Issue} on {Exchange} {Rate} {Forecasting}},
	title = {Forecasting the volatility of currency exchange rates},
	volume = {3},
	issn = {0169-2070},
	url = {http://www.sciencedirect.com/science/article/pii/ 0169207087900859},
	doi = {10.1016/0169-2070(87)90085-9},
	number = {1},
	journal = {International Journal of Forecasting},
	author = {Taylor, Stephen J.},
	month = jan,
	year = {1987},
	pages = {159--170}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gRm9yZWNhc3RpbmcgdGhlIHZvbGF0aWxpdHkgb2YgY3VycmVuY3kgZXhjaGFuZ2UgcmF0ZXMNCkFVICAtIFRheWxvciwgU3RlcGhlbiBKLg0KVDIgIC0gSW50ZXJuYXRpb25hbCBKb3VybmFsIG9mIEZvcmVjYXN0aW5nDQpUMyAgLSBTcGVjaWFsIElzc3VlIG9uIEV4Y2hhbmdlIFJhdGUgRm9yZWNhc3RpbmcNCkFCICAtIEN1cnJlbmN5IHZvbGF0aWxpdHkgaXMgZGVmaW5lZCB0byBiZSB0aGUgc3RhbmRhcmQgZGV2aWF0aW9uIG9mIGRheS10by1kYXkgY2hhbmdlcyBpbiB0aGUgbG9nYXJpdGhtIG9mIHRoZSBleGNoYW5nZSByYXRlLiBBZnRlciBhIGRpc2N1c3Npb24gb2Ygc3RhdGlzdGljYWwgbW9kZWxzIGZvciBleGNoYW5nZSByYXRlcywgdGhlIHBhcGVyIGRlc2NyaWJlcyBtZXRob2RzIGZvciBjaG9vc2luZyBhbmQgYXNzZXNzaW5nIHZvbGF0aWxpdHkgZm9yZWNhc3RzIHVzaW5nIG9wZW4sIGhpZ2gsIGxvdyBhbmQgY2xvc2UgcHJpY2VzLiBSZXN1bHRzIGZvciBETS8kIGZ1dHVyZXMgcHJpY2VzIGF0IHRoZSBJTU0gaW4gQ2hpY2FnbyBmcm9tIDE5NzcgdG8gMTk4MyBzaG93IGhpZ2ggYW5kIGxvdyBwcmljZXMgYXJlIHZhbHVhYmxlIHdoZW4gc2Vla2luZyBhY2N1cmF0ZSB2b2xhdGlsaXR5IGZvcmVjYXN0cy4gVGhlIGJlc3QgZm9yZWNhc3RzIGFyZSBhIHdlaWdodGVkIGF2ZXJhZ2Ugb2YgcHJlc2VudCBhbmQgcGFzdCBoaWdoLCBsb3cgYW5kIGNsb3NlIHByaWNlcywgd2l0aCBhZGp1c3RtZW50cyBmb3Igd2Vla2VuZCBhbmQgaG9saWRheSBlZmZlY3RzLiBUaGUgZm9yZWNhc3RzIGNhbiBiZSB1c2VkIHRvIHZhbHVlIGN1cnJlbmN5IG9wdGlvbnMuDQpEQSAgLSAxOTg3LzAxLy8NClBZICAtIDE5ODcNCkRPICAtIDEwLjEwMTYvMDE2OS0yMDcwKDg3KTkwMDg1LTkNClZMICAtIDMNCklTICAtIDENClNQICAtIDE1OQ0KRVAgIC0gMTcwDQpTTiAgLSAwMTY5LTIwNzANClVSICAtIGh0dHA6Ly93d3cuc2NpZW5jZWRpcmVjdC5jb20vc2NpZW5jZS9hcnRpY2xlL3BpaS8gMDE2OTIwNzA4NzkwMDg1OQ0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">S. J. Taylor, “Forecasting the volatility of currency exchange rates,” <i>International Journal of Forecasting</i>, vol. 3, no. 1, pp. 159–170, Jan. 1987.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1016%2F0169-2070(87)90085-9&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Forecasting%20the%20volatility%20of%20currency%20exchange%20rates&amp;rft.jtitle=International%20Journal%20of%20Forecasting&amp;rft.volume=3&amp;rft.issue=1&amp;rft.aufirst=Stephen%20J.&amp;rft.aulast=Taylor&amp;rft.au=Stephen%20J.%20Taylor&amp;rft.date=1987&amp;rft.pages=159%E2%80%93170&amp;rft.issn=0169-2070"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Learning Internal Representations by Error Propagation</span> <span class="containertitle"></span> (September 1985) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “<span class="doctitle"><a class="doctitle" href="http://www.dtic.mil/docs/citations/ADA164453">Learning Internal Representations by Error Propagation</a></span>,” CALIFORNIA UNIV SAN DIEGO LA JOLLA INST FOR COGNITIVE SCIENCE, CALIFORNIA UNIV SAN DIEGO LA JOLLA INST FOR COGNITIVE SCIENCE, ICS-8506, Sep. 1985.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1985 type__report ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">This paper presents a generalization of the perception learning procedure for learning the correct sets of connections for arbitrary networks. The rule‚ falled the generalized delta rule‚ is a simple scheme for implementing a gradient descent method for finding weights that minimize the sum squared error of the sytem’s performance. The major theoretical contribution of the work is the procedure called error propagation‚ whereby the gradient can be determined by individual units of the network based only on locally available information. The major empirical contribution of the work is to show that the problem of local minima not serious in this application of gradient descent. Keywords: Learning; networks; Perceptrons; Adaptive systems; Learning machines; and Back propagation.</div></div></div><div class="blink"><a   href="http://www.dtic.mil/docs/citations/ADA164453" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@techreport{rumelhart1985learning,
	title = {Learning {Internal} {Representations} by {Error} {Propagation}},
	url = {http://www.dtic.mil/docs/citations/ADA164453},
	language = {en},
	number = {ICS-8506},
	institution = {CALIFORNIA UNIV SAN DIEGO LA JOLLA INST FOR COGNITIVE SCIENCE, CALIFORNIA UNIV SAN DIEGO LA JOLLA INST FOR COGNITIVE SCIENCE},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = sep,
	year = {1985}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gUlBSVA0KVEkgIC0gTGVhcm5pbmcgSW50ZXJuYWwgUmVwcmVzZW50YXRpb25zIGJ5IEVycm9yIFByb3BhZ2F0aW9uDQpBVSAgLSBSdW1lbGhhcnQsIERhdmlkIEUuDQpBVSAgLSBIaW50b24sIEdlb2ZmcmV5IEUuDQpBVSAgLSBXaWxsaWFtcywgUm9uYWxkIEouDQpBQiAgLSBUaGlzIHBhcGVyIHByZXNlbnRzIGEgZ2VuZXJhbGl6YXRpb24gb2YgdGhlIHBlcmNlcHRpb24gbGVhcm5pbmcgcHJvY2VkdXJlIGZvciBsZWFybmluZyB0aGUgY29ycmVjdCBzZXRzIG9mIGNvbm5lY3Rpb25zIGZvciBhcmJpdHJhcnkgbmV0d29ya3MuIFRoZSBydWxlLCBmYWxsZWQgdGhlIGdlbmVyYWxpemVkIGRlbHRhIHJ1bGUsIGlzIGEgc2ltcGxlIHNjaGVtZSBmb3IgaW1wbGVtZW50aW5nIGEgZ3JhZGllbnQgZGVzY2VudCBtZXRob2QgZm9yIGZpbmRpbmcgd2VpZ2h0cyB0aGF0IG1pbmltaXplIHRoZSBzdW0gc3F1YXJlZCBlcnJvciBvZiB0aGUgc3l0ZW0ncyBwZXJmb3JtYW5jZS4gVGhlIG1ham9yIHRoZW9yZXRpY2FsIGNvbnRyaWJ1dGlvbiBvZiB0aGUgd29yayBpcyB0aGUgcHJvY2VkdXJlIGNhbGxlZCBlcnJvciBwcm9wYWdhdGlvbiwgd2hlcmVieSB0aGUgZ3JhZGllbnQgY2FuIGJlIGRldGVybWluZWQgYnkgaW5kaXZpZHVhbCB1bml0cyBvZiB0aGUgbmV0d29yayBiYXNlZCBvbmx5IG9uIGxvY2FsbHkgYXZhaWxhYmxlIGluZm9ybWF0aW9uLiBUaGUgbWFqb3IgZW1waXJpY2FsIGNvbnRyaWJ1dGlvbiBvZiB0aGUgd29yayBpcyB0byBzaG93IHRoYXQgdGhlIHByb2JsZW0gb2YgbG9jYWwgbWluaW1hIG5vdCBzZXJpb3VzIGluIHRoaXMgYXBwbGljYXRpb24gb2YgZ3JhZGllbnQgZGVzY2VudC4gS2V5d29yZHM6IExlYXJuaW5nOyBuZXR3b3JrczsgUGVyY2VwdHJvbnM7IEFkYXB0aXZlIHN5c3RlbXM7IExlYXJuaW5nIG1hY2hpbmVzOyBhbmQgQmFjayBwcm9wYWdhdGlvbi4NCkRBICAtIDE5ODUvMDkvLw0KUFkgIC0gMTk4NQ0KTEEgIC0gZW4NClBCICAtIENBTElGT1JOSUEgVU5JViBTQU4gRElFR08gTEEgSk9MTEEgSU5TVCBGT1IgQ09HTklUSVZFIFNDSUVOQ0UsIENBTElGT1JOSUEgVU5JViBTQU4gRElFR08gTEEgSk9MTEEgSU5TVCBGT1IgQ09HTklUSVZFIFNDSUVOQ0UNClNOICAtIElDUy04NTA2DQpVUiAgLSBodHRwOi8vd3d3LmR0aWMubWlsL2RvY3MvY2l0YXRpb25zL0FEQTE2NDQ1Mw0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning Internal Representations by Error Propagation,” CALIFORNIA UNIV SAN DIEGO LA JOLLA INST FOR COGNITIVE SCIENCE, CALIFORNIA UNIV SAN DIEGO LA JOLLA INST FOR COGNITIVE SCIENCE, ICS-8506, Sep. 1985.</div>
  </div></div></div></div></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation</span> <span class="containertitle">Econometrica</span> (1982) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. F. Engle, “<span class="doctitle"><a class="doctitle" href="http://www.jstor.org/stable/1912773">Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation</a></span>,” <i>Econometrica</i>, vol. 50, no. 4, pp. 987–1007, 1982.</div>
  </div><div class="bib-extra">23957</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1982 type__journalArticle ""</span></div><div class="bib-venue">Econometrica</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Traditional econometric models assume a constant one-period forecast variance. To generalize this implausible assumption‚ a new class of stochastic processes called autoregressive conditional heteroscedastic (ARCH) processes are introduced in this paper. These are mean zero‚ serially uncorrelated processes with nonconstant variances conditional on the past‚ but constant unconditional variances. For such processes‚ the recent past gives information about the one-period forecast variance. A regression model is then introduced with disturbances following an ARCH process. Maximum likelihood estimators are described and a simple scoring iteration formulated. Ordinary least squares maintains its optimality properties in this set-up‚ but maximum likelihood is more efficient. The relative efficiency is calculated and can be infinite. To test whether the disturbances follow an ARCH process‚ the Lagrange multiplier procedure is employed. The test is based simply on the autocorrelation of the squared OLS residuals. This model is used to estimate the means and variances of inflation in the U.K. The ARCH effect is found to be significant and the estimated variances increase substantially during the chaotic seventies.</div></div></div><div class="blink"><a   href="http://www.jstor.org/stable/1912773" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{engle1982autoregressive,
	title = {Autoregressive {Conditional} {Heteroscedasticity} with {Estimates} of the {Variance} of {United} {Kingdom} {Inflation}},
	volume = {50},
	issn = {0012-9682},
	url = {http://www.jstor.org/stable/1912773},
	doi = {10.2307/1912773},
	number = {4},
	journal = {Econometrica},
	author = {Engle, Robert F.},
	year = {1982},
	note = {23957},
	pages = {987--1007}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('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');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">R. F. Engle, “Autoregressive Conditional Heteroscedasticity with Estimates of the Variance of United Kingdom Inflation,” <i>Econometrica</i>, vol. 50, no. 4, pp. 987–1007, 1982.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.2307%2F1912773&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Autoregressive%20Conditional%20Heteroscedasticity%20with%20Estimates%20of%20the%20Variance%20of%20United%20Kingdom%20Inflation&amp;rft.jtitle=Econometrica&amp;rft.volume=50&amp;rft.issue=4&amp;rft.aufirst=Robert%20F.&amp;rft.aulast=Engle&amp;rft.au=Robert%20F.%20Engle&amp;rft.date=1982&amp;rft.pages=987%E2%80%931007&amp;rft.issn=0012-9682"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Distribution of the Estimators for Autoregressive Time Series With a Unit Root</span> <span class="containertitle">Journal of the American Statistical Association</span> (1979) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. A. Dickey and W. A. Fuller, “<span class="doctitle"><a class="doctitle" href="http://www.jstor.org/stable/2286348">Distribution of the Estimators for Autoregressive Time Series With a Unit Root</a></span>,” <i>Journal of the American Statistical Association</i>, vol. 74, no. 366, pp. 427–431, 1979.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1979 type__journalArticle ""</span></div><div class="bib-venue">Journal of the American Statistical Association</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Let n observations Y_\textrm1</div></div></div><div class="blink"><a   href="http://www.jstor.org/stable/2286348" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{dickey1979distribution,
	title = {Distribution of the {Estimators} for {Autoregressive} {Time} {Series} {With} a {Unit} {Root}},
	volume = {74},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2286348},
	doi = {10.2307/2286348},
	$$_{\textrm{2}}$$_{\textrm{n}}$$_{\textrm{t}}$$_{\textrm{t - 1}}$$_{\textrm{t}}$$_{\textrm{0}}$\$ is a fixed constant and \{\vphantom{\}}e$_{\textrm{t}}$$_{\textrm{t = 1}}$$^{\textrm{n}}$$^{\textrm{2}}$\$. Properties of the regression estimator of ρ are obtained under the assumption that ρ = ± 1. Representations for the limit distributions of the estimator of ρ and of the regression t test are derived. The estimator of ρ and the regression t test furnish methods of testing the hypothesis that ρ = 1.},
	number = {366},
	journal = {Journal of the American Statistical Association},
	author = {Dickey, David A. and Fuller, Wayne A.},
	year = {1979},
	pages = {427--431}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gRGlzdHJpYnV0aW9uIG9mIHRoZSBFc3RpbWF0b3JzIGZvciBBdXRvcmVncmVzc2l2ZSBUaW1lIFNlcmllcyBXaXRoIGEgVW5pdCBSb290DQpBVSAgLSBEaWNrZXksIERhdmlkIEEuDQpBVSAgLSBGdWxsZXIsIFdheW5lIEEuDQpUMiAgLSBKb3VybmFsIG9mIHRoZSBBbWVyaWNhbiBTdGF0aXN0aWNhbCBBc3NvY2lhdGlvbg0KQUIgIC0gTGV0IG4gb2JzZXJ2YXRpb25zIFk8c3ViPjE8L3N1Yj48c3ViPjI8L3N1Yj48c3ViPm48L3N1Yj48c3ViPnQ8L3N1Yj48c3ViPnQgLSAxPC9zdWI+PHN1Yj50PC9zdWI+PHN1Yj4wPC9zdWI+JCBpcyBhIGZpeGVkIGNvbnN0YW50IGFuZCB7ZTxzdWI+dDwvc3ViPjxzdWI+dCA9IDE8L3N1Yj48c3VwPm48L3N1cD48c3VwPjI8L3N1cD4kLiBQcm9wZXJ0aWVzIG9mIHRoZSByZWdyZXNzaW9uIGVzdGltYXRvciBvZiDPgSBhcmUgb2J0YWluZWQgdW5kZXIgdGhlIGFzc3VtcHRpb24gdGhhdCDPgSA9IMKxIDEuIFJlcHJlc2VudGF0aW9ucyBmb3IgdGhlIGxpbWl0IGRpc3RyaWJ1dGlvbnMgb2YgdGhlIGVzdGltYXRvciBvZiDPgSBhbmQgb2YgdGhlIHJlZ3Jlc3Npb24gdCB0ZXN0IGFyZSBkZXJpdmVkLiBUaGUgZXN0aW1hdG9yIG9mIM+BIGFuZCB0aGUgcmVncmVzc2lvbiB0IHRlc3QgZnVybmlzaCBtZXRob2RzIG9mIHRlc3RpbmcgdGhlIGh5cG90aGVzaXMgdGhhdCDPgSA9IDEuDQpEQSAgLSAxOTc5Ly8vDQpQWSAgLSAxOTc5DQpETyAgLSAxMC4yMzA3LzIyODYzNDgNClZMICAtIDc0DQpJUyAgLSAzNjYNClNQICAtIDQyNw0KRVAgIC0gNDMxDQpTTiAgLSAwMTYyLTE0NTkNClVSICAtIGh0dHA6Ly93d3cuanN0b3Iub3JnL3N0YWJsZS8yMjg2MzQ4DQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">D. A. Dickey and W. A. Fuller, “Distribution of the Estimators for Autoregressive Time Series With a Unit Root,” <i>Journal of the American Statistical Association</i>, vol. 74, no. 366, pp. 427–431, 1979.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.2307%2F2286348&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Distribution%20of%20the%20Estimators%20for%20Autoregressive%20Time%20Series%20With%20a%20Unit%20Root&amp;rft.jtitle=Journal%20of%20the%20American%20Statistical%20Association&amp;rft.volume=74&amp;rft.issue=366&amp;rft.aufirst=David%20A.&amp;rft.aulast=Dickey&amp;rft.au=David%20A.%20Dickey&amp;rft.au=Wayne%20A.%20Fuller&amp;rft.date=1979&amp;rft.pages=427%E2%80%93431&amp;rft.issn=0162-1459"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Beyond regression: new tools for prediction and analysis in the behavioral sciences</span> <span class="containertitle"></span> (1975) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">P. J. Werbos, “<span class="doctitle">Beyond regression: new tools for prediction and analysis in the behavioral sciences</span>,” PhD Thesis, 1975.</div>
  </div><div class="bib-extra">00014</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1975 type__thesis ""</span></div><div class="blinkitems"><div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@phdthesis{werbos1975beyond,
	type = {{PhD} {Thesis}},
	title = {Beyond regression: new tools for prediction and analysis in the behavioral sciences},
	language = {English},
	author = {Werbos, Paul J},
	year = {1975},
	note = {00014}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gVEhFUw0KVEkgIC0gQmV5b25kIHJlZ3Jlc3Npb246IG5ldyB0b29scyBmb3IgcHJlZGljdGlvbiBhbmQgYW5hbHlzaXMgaW4gdGhlIGJlaGF2aW9yYWwgc2NpZW5jZXMNCkFVICAtIFdlcmJvcywgUGF1bCBKDQpEQSAgLSAxOTc1Ly8vDQpQWSAgLSAxOTc1DQpMQSAgLSBFbmdsaXNoDQpNMyAgLSBQaEQgVGhlc2lzDQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">P. J. Werbos, “Beyond regression: new tools for prediction and analysis in the behavioral sciences,” PhD Thesis, 1975.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=Beyond%20regression%3A%20new%20tools%20for%20prediction%20and%20analysis%20in%20the%20behavioral%20sciences&amp;rft.degree=PhD%20Thesis&amp;rft.aufirst=Paul%20J&amp;rft.aulast=Werbos&amp;rft.au=Paul%20J%20Werbos&amp;rft.date=1975"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Adaptive switching circuits</span> <span class="containertitle"></span> (1960) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">B. Widrow and M. E. Hoff, “<span class="doctitle">Adaptive switching circuits</span>,” STANFORD UNIV CA STANFORD ELECTRONICS LABS, 1960.</div>
  </div><div class="bib-extra">05223</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1960 type__report ""</span></div><div class="blinkitems"><div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@techreport{widrow1960adaptive,
	title = {Adaptive switching circuits},
	institution = {STANFORD UNIV CA STANFORD ELECTRONICS LABS},
	author = {Widrow, Bernard and Hoff, Marcian E},
	year = {1960},
	note = {05223}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gUlBSVA0KVEkgIC0gQWRhcHRpdmUgc3dpdGNoaW5nIGNpcmN1aXRzDQpBVSAgLSBXaWRyb3csIEJlcm5hcmQNCkFVICAtIEhvZmYsIE1hcmNpYW4gRQ0KREEgIC0gMTk2MC8vLw0KUFkgIC0gMTk2MA0KUEIgIC0gU1RBTkZPUkQgVU5JViBDQSBTVEFORk9SRCBFTEVDVFJPTklDUyBMQUJTDQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">B. Widrow and M. E. Hoff, “Adaptive switching circuits,” STANFORD UNIV CA STANFORD ELECTRONICS LABS, 1960.</div>
  </div></div></div></div></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Forecasting Sales by Exponentially Weighted Moving Averages</span> <span class="containertitle">Management Science</span> (April 1960) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">P. R. Winters, “<span class="doctitle"><a class="doctitle" href="https://pubsonline.informs.org/doi/abs/10.1287/ mnsc.6.3.324">Forecasting Sales by Exponentially Weighted Moving Averages</a></span>,” <i>Management Science</i>, vol. 6, no. 3, pp. 324–342, Apr. 1960.</div>
  </div><div class="bib-extra">01602</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1960 type__journalArticle ""</span></div><div class="bib-venue">Management Science</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">The growing use of computers for mechanized inventory control and production planning has brought with it the need for explicit forecasts of sales and usage for individual products and materials. These forecasts must be made on a routine basis for thousands of products‚ so that they must be made quickly‚ and‚ both in terms of computing time and information storage‚ cheaply; they should be responsive to changing conditions. The paper presents a method of forecasting sales which has these desirable characteristics‚ and which in terms of ability to forecast compares favorably with other‚ more traditional methods. Several models of the exponential forecasting system are presented‚ along with several examples of application.</div></div></div><div class="blink"><a   href="https://pubsonline.informs.org/doi/abs/10.1287/ mnsc.6.3.324" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{winters1960forecasting,
	title = {Forecasting {Sales} by {Exponentially} {Weighted} {Moving} {Averages}},
	volume = {6},
	issn = {0025-1909},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/ mnsc.6.3.324},
	doi = {10.1287/mnsc.6.3.324},
	number = {3},
	journal = {Management Science},
	author = {Winters, Peter R.},
	month = apr,
	year = {1960},
	note = {01602},
	pages = {324--342}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gRm9yZWNhc3RpbmcgU2FsZXMgYnkgRXhwb25lbnRpYWxseSBXZWlnaHRlZCBNb3ZpbmcgQXZlcmFnZXMNCkFVICAtIFdpbnRlcnMsIFBldGVyIFIuDQpUMiAgLSBNYW5hZ2VtZW50IFNjaWVuY2UNCkFCICAtIFRoZSBncm93aW5nIHVzZSBvZiBjb21wdXRlcnMgZm9yIG1lY2hhbml6ZWQgaW52ZW50b3J5IGNvbnRyb2wgYW5kIHByb2R1Y3Rpb24gcGxhbm5pbmcgaGFzIGJyb3VnaHQgd2l0aCBpdCB0aGUgbmVlZCBmb3IgZXhwbGljaXQgZm9yZWNhc3RzIG9mIHNhbGVzIGFuZCB1c2FnZSBmb3IgaW5kaXZpZHVhbCBwcm9kdWN0cyBhbmQgbWF0ZXJpYWxzLiBUaGVzZSBmb3JlY2FzdHMgbXVzdCBiZSBtYWRlIG9uIGEgcm91dGluZSBiYXNpcyBmb3IgdGhvdXNhbmRzIG9mIHByb2R1Y3RzLCBzbyB0aGF0IHRoZXkgbXVzdCBiZSBtYWRlIHF1aWNrbHksIGFuZCwgYm90aCBpbiB0ZXJtcyBvZiBjb21wdXRpbmcgdGltZSBhbmQgaW5mb3JtYXRpb24gc3RvcmFnZSwgY2hlYXBseTsgdGhleSBzaG91bGQgYmUgcmVzcG9uc2l2ZSB0byBjaGFuZ2luZyBjb25kaXRpb25zLiBUaGUgcGFwZXIgcHJlc2VudHMgYSBtZXRob2Qgb2YgZm9yZWNhc3Rpbmcgc2FsZXMgd2hpY2ggaGFzIHRoZXNlIGRlc2lyYWJsZSBjaGFyYWN0ZXJpc3RpY3MsIGFuZCB3aGljaCBpbiB0ZXJtcyBvZiBhYmlsaXR5IHRvIGZvcmVjYXN0IGNvbXBhcmVzIGZhdm9yYWJseSB3aXRoIG90aGVyLCBtb3JlIHRyYWRpdGlvbmFsIG1ldGhvZHMuIFNldmVyYWwgbW9kZWxzIG9mIHRoZSBleHBvbmVudGlhbCBmb3JlY2FzdGluZyBzeXN0ZW0gYXJlIHByZXNlbnRlZCwgYWxvbmcgd2l0aCBzZXZlcmFsIGV4YW1wbGVzIG9mIGFwcGxpY2F0aW9uLg0KREEgIC0gMTk2MC8wNC8vDQpQWSAgLSAxOTYwDQpETyAgLSAxMC4xMjg3L21uc2MuNi4zLjMyNA0KVkwgIC0gNg0KSVMgIC0gMw0KU1AgIC0gMzI0DQpFUCAgLSAzNDINClNOICAtIDAwMjUtMTkwOQ0KVVIgIC0gaHR0cHM6Ly9wdWJzb25saW5lLmluZm9ybXMub3JnL2RvaS9hYnMvMTAuMTI4Ny8gbW5zYy42LjMuMzI0DQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">P. R. Winters, “Forecasting Sales by Exponentially Weighted Moving Averages,” <i>Management Science</i>, vol. 6, no. 3, pp. 324–342, Apr. 1960.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1287%2Fmnsc.6.3.324&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Forecasting%20Sales%20by%20Exponentially%20Weighted%20Moving%20Averages&amp;rft.jtitle=Management%20Science&amp;rft.volume=6&amp;rft.issue=3&amp;rft.aufirst=Peter%20R.&amp;rft.aulast=Winters&amp;rft.au=Peter%20R.%20Winters&amp;rft.date=1960-04&amp;rft.pages=324%E2%80%93342&amp;rft.issn=0025-1909"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">The perceptron: A probabilistic model for information storage and organization in the brain</span> <span class="containertitle">Psychological Review</span> (1958) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">F. Rosenblatt, “<span class="doctitle">The perceptron: A probabilistic model for information storage and organization in the brain</span>,” <i>Psychological Review</i>, vol. 65, no. 6, pp. 386–408, 1958.</div>
  </div><div class="bib-extra">07783</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1958 type__journalArticle ""</span></div><div class="bib-venue">Psychological Review</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">To answer the questions of how information about the physical world is sensed‚ in what form is information remembered‚ and how does information retained in memory influence recognition and behavior‚ a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references. (PsycINFO Database Record (c) 2016 APA‚ all rights reserved)</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{rosenblatt1958perceptron:,
	title = {The perceptron: {A} probabilistic model for information storage and organization in the brain},
	volume = {65},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	doi = {10.1037/h0042519},
	number = {6},
	journal = {Psychological Review},
	author = {Rosenblatt, F.},
	year = {1958},
	note = {07783},
	pages = {386--408}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gVGhlIHBlcmNlcHRyb246IEEgcHJvYmFiaWxpc3RpYyBtb2RlbCBmb3IgaW5mb3JtYXRpb24gc3RvcmFnZSBhbmQgb3JnYW5pemF0aW9uIGluIHRoZSBicmFpbg0KQVUgIC0gUm9zZW5ibGF0dCwgRi4NClQyICAtIFBzeWNob2xvZ2ljYWwgUmV2aWV3DQpBQiAgLSBUbyBhbnN3ZXIgdGhlIHF1ZXN0aW9ucyBvZiBob3cgaW5mb3JtYXRpb24gYWJvdXQgdGhlIHBoeXNpY2FsIHdvcmxkIGlzIHNlbnNlZCwgaW4gd2hhdCBmb3JtIGlzIGluZm9ybWF0aW9uIHJlbWVtYmVyZWQsIGFuZCBob3cgZG9lcyBpbmZvcm1hdGlvbiByZXRhaW5lZCBpbiBtZW1vcnkgaW5mbHVlbmNlIHJlY29nbml0aW9uIGFuZCBiZWhhdmlvciwgYSB0aGVvcnkgaXMgZGV2ZWxvcGVkIGZvciBhIGh5cG90aGV0aWNhbCBuZXJ2b3VzIHN5c3RlbSBjYWxsZWQgYSBwZXJjZXB0cm9uLiBUaGUgdGhlb3J5IHNlcnZlcyBhcyBhIGJyaWRnZSBiZXR3ZWVuIGJpb3BoeXNpY3MgYW5kIHBzeWNob2xvZ3kuIEl0IGlzIHBvc3NpYmxlIHRvIHByZWRpY3QgbGVhcm5pbmcgY3VydmVzIGZyb20gbmV1cm9sb2dpY2FsIHZhcmlhYmxlcyBhbmQgdmljZSB2ZXJzYS4gVGhlIHF1YW50aXRhdGl2ZSBzdGF0aXN0aWNhbCBhcHByb2FjaCBpcyBmcnVpdGZ1bCBpbiB0aGUgdW5kZXJzdGFuZGluZyBvZiB0aGUgb3JnYW5pemF0aW9uIG9mIGNvZ25pdGl2ZSBzeXN0ZW1zLiAxOCByZWZlcmVuY2VzLiAoUHN5Y0lORk8gRGF0YWJhc2UgUmVjb3JkIChjKSAyMDE2IEFQQSwgYWxsIHJpZ2h0cyByZXNlcnZlZCkNCkRBICAtIDE5NTgvLy8NClBZICAtIDE5NTgNCkRPICAtIDEwLjEwMzcvaDAwNDI1MTkNClZMICAtIDY1DQpJUyAgLSA2DQpTUCAgLSAzODYNCkVQICAtIDQwOA0KU04gIC0gMTkzOS0xNDcxKEVsZWN0cm9uaWMpLDAwMzMtMjk1WChQcmludCkNCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">F. Rosenblatt, “The perceptron: A probabilistic model for information storage and organization in the brain,” <i>Psychological Review</i>, vol. 65, no. 6, pp. 386–408, 1958.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1037%2Fh0042519&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=The%20perceptron%3A%20A%20probabilistic%20model%20for%20information%20storage%20and%20organization%20in%20the%20brain&amp;rft.jtitle=Psychological%20Review&amp;rft.volume=65&amp;rft.issue=6&amp;rft.aufirst=F.&amp;rft.aulast=Rosenblatt&amp;rft.au=F.%20Rosenblatt&amp;rft.date=1958&amp;rft.pages=386%E2%80%93408&amp;rft.issn=1939-1471(Electronic)%2C0033-295X(Print)"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Individual Comparisons by Ranking Methods</span> <span class="containertitle">Biometrics Bulletin</span> (1945) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">F. Wilcoxon, “<span class="doctitle"><a class="doctitle" href="http://www.jstor.org/stable/3001968">Individual Comparisons by Ranking Methods</a></span>,” <i>Biometrics Bulletin</i>, vol. 1, no. 6, pp. 80–83, 1945.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1945 type__journalArticle ""</span></div><div class="bib-venue">Biometrics Bulletin</div><div class="blinkitems"><div><div class="blink"><a   href="http://www.jstor.org/stable/3001968" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{wilcoxon1945individual,
	title = {Individual {Comparisons} by {Ranking} {Methods}},
	volume = {1},
	issn = {0099-4987},
	url = {http://www.jstor.org/stable/3001968},
	doi = {10.2307/3001968},
	number = {6},
	journal = {Biometrics Bulletin},
	author = {Wilcoxon, Frank},
	year = {1945},
	note = {00000},
	pages = {80--83}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gSW5kaXZpZHVhbCBDb21wYXJpc29ucyBieSBSYW5raW5nIE1ldGhvZHMNCkFVICAtIFdpbGNveG9uLCBGcmFuaw0KVDIgIC0gQmlvbWV0cmljcyBCdWxsZXRpbg0KREEgIC0gMTk0NS8vLw0KUFkgIC0gMTk0NQ0KRE8gIC0gMTAuMjMwNy8zMDAxOTY4DQpWTCAgLSAxDQpJUyAgLSA2DQpTUCAgLSA4MA0KRVAgIC0gODMNClNOICAtIDAwOTktNDk4Nw0KVVIgIC0gaHR0cDovL3d3dy5qc3Rvci5vcmcvc3RhYmxlLzMwMDE5NjgNCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">F. Wilcoxon, “Individual Comparisons by Ranking Methods,” <i>Biometrics Bulletin</i>, vol. 1, no. 6, pp. 80–83, 1945.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.2307%2F3001968&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Individual%20Comparisons%20by%20Ranking%20Methods&amp;rft.jtitle=Biometrics%20Bulletin&amp;rft.volume=1&amp;rft.issue=6&amp;rft.aufirst=Frank&amp;rft.aulast=Wilcoxon&amp;rft.au=Frank%20Wilcoxon&amp;rft.date=1945&amp;rft.pages=80%E2%80%9383&amp;rft.issn=0099-4987"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">A logical calculus of the ideas immanent in nervous activity</span> <span class="containertitle">The bulletin of mathematical biophysics</span> (December 1943) <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">W. S. McCulloch and W. Pitts, “<span class="doctitle"><a class="doctitle" href="https://link.springer.com/article/10.1007/BF02478259">A logical calculus of the ideas immanent in nervous activity</a></span>,” <i>The bulletin of mathematical biophysics</i>, vol. 5, no. 4, pp. 115–133, Dec. 1943.</div>
  </div><div class="bib-extra">15658</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__1943 type__journalArticle ""</span></div><div class="bib-venue">The bulletin of mathematical biophysics</div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Because of the “all-or-none” character of nervous activity‚ neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms‚ with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions‚ one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent‚ in the sense that for every net behaving under one assumption‚ there exists another net which behaves under the other and gives the same results‚ although perhaps not in the same time. Various applications of the calculus are discussed.</div></div></div><div class="blink"><a   href="https://link.springer.com/article/10.1007/BF02478259" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{mcculloch1943logical,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {0007-4985, 1522-9602},
	url = {https://link.springer.com/article/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	language = {en},
	number = {4},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	note = {15658},
	pages = {115--133}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gQSBsb2dpY2FsIGNhbGN1bHVzIG9mIHRoZSBpZGVhcyBpbW1hbmVudCBpbiBuZXJ2b3VzIGFjdGl2aXR5DQpBVSAgLSBNY0N1bGxvY2gsIFdhcnJlbiBTLg0KQVUgIC0gUGl0dHMsIFdhbHRlcg0KVDIgIC0gVGhlIGJ1bGxldGluIG9mIG1hdGhlbWF0aWNhbCBiaW9waHlzaWNzDQpBQiAgLSBCZWNhdXNlIG9mIHRoZSDigJxhbGwtb3Itbm9uZeKAnSBjaGFyYWN0ZXIgb2YgbmVydm91cyBhY3Rpdml0eSwgbmV1cmFsIGV2ZW50cyBhbmQgdGhlIHJlbGF0aW9ucyBhbW9uZyB0aGVtIGNhbiBiZSB0cmVhdGVkIGJ5IG1lYW5zIG9mIHByb3Bvc2l0aW9uYWwgbG9naWMuIEl0IGlzIGZvdW5kIHRoYXQgdGhlIGJlaGF2aW9yIG9mIGV2ZXJ5IG5ldCBjYW4gYmUgZGVzY3JpYmVkIGluIHRoZXNlIHRlcm1zLCB3aXRoIHRoZSBhZGRpdGlvbiBvZiBtb3JlIGNvbXBsaWNhdGVkIGxvZ2ljYWwgbWVhbnMgZm9yIG5ldHMgY29udGFpbmluZyBjaXJjbGVzOyBhbmQgdGhhdCBmb3IgYW55IGxvZ2ljYWwgZXhwcmVzc2lvbiBzYXRpc2Z5aW5nIGNlcnRhaW4gY29uZGl0aW9ucywgb25lIGNhbiBmaW5kIGEgbmV0IGJlaGF2aW5nIGluIHRoZSBmYXNoaW9uIGl0IGRlc2NyaWJlcy4gSXQgaXMgc2hvd24gdGhhdCBtYW55IHBhcnRpY3VsYXIgY2hvaWNlcyBhbW9uZyBwb3NzaWJsZSBuZXVyb3BoeXNpb2xvZ2ljYWwgYXNzdW1wdGlvbnMgYXJlIGVxdWl2YWxlbnQsIGluIHRoZSBzZW5zZSB0aGF0IGZvciBldmVyeSBuZXQgYmVoYXZpbmcgdW5kZXIgb25lIGFzc3VtcHRpb24sIHRoZXJlIGV4aXN0cyBhbm90aGVyIG5ldCB3aGljaCBiZWhhdmVzIHVuZGVyIHRoZSBvdGhlciBhbmQgZ2l2ZXMgdGhlIHNhbWUgcmVzdWx0cywgYWx0aG91Z2ggcGVyaGFwcyBub3QgaW4gdGhlIHNhbWUgdGltZS4gVmFyaW91cyBhcHBsaWNhdGlvbnMgb2YgdGhlIGNhbGN1bHVzIGFyZSBkaXNjdXNzZWQuDQpEQSAgLSAxOTQzLzEyLy8NClBZICAtIDE5NDMNCkRPICAtIDEwLjEwMDcvQkYwMjQ3ODI1OQ0KVkwgIC0gNQ0KSVMgIC0gNA0KU1AgIC0gMTE1DQpFUCAgLSAxMzMNCkxBICAtIGVuDQpTTiAgLSAwMDA3LTQ5ODUsIDE1MjItOTYwMg0KVVIgIC0gaHR0cHM6Ly9saW5rLnNwcmluZ2VyLmNvbS9hcnRpY2xlLzEwLjEwMDcvQkYwMjQ3ODI1OQ0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent in nervous activity,” <i>The bulletin of mathematical biophysics</i>, vol. 5, no. 4, pp. 115–133, Dec. 1943.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_id=info%3Adoi%2F10.1007%2FBF02478259&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=A%20logical%20calculus%20of%20the%20ideas%20immanent%20in%20nervous%20activity&amp;rft.jtitle=The%20bulletin%20of%20mathematical%20biophysics&amp;rft.volume=5&amp;rft.issue=4&amp;rft.aufirst=Warren%20S.&amp;rft.aulast=McCulloch&amp;rft.au=Warren%20S.%20McCulloch&amp;rft.au=Walter%20Pitts&amp;rft.date=1943-12&amp;rft.pages=115%E2%80%93133&amp;rft.issn=0007-4985%2C%201522-9602"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">6.4.4.3.1. Seasonal Subseries Plot</span> <span class="containertitle"></span>  <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;"><span class="doctitle"><a class="doctitle" href="https://www.itl.nist.gov/div898/handbook/pmc/section4/ pmc4431.htm"><i>6.4.4.3.1. Seasonal Subseries Plot</i>.</a></span> .</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__ type__book ""</span></div><div class="blinkitems"><div><div class="blink"><a   href="https://www.itl.nist.gov/div898/handbook/pmc/section4/ pmc4431.htm" >link</a></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@book{noauthornodate6.4.4.3.1.,
	title = {6.4.4.3.1. {Seasonal} {Subseries} {Plot}},
	url = {https://www.itl.nist.gov/div898/handbook/pmc/section4/ pmc4431.htm},
	note = {00000}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gQk9PSw0KVEkgIC0gNi40LjQuMy4xLiBTZWFzb25hbCBTdWJzZXJpZXMgUGxvdA0KVVIgIC0gaHR0cHM6Ly93d3cuaXRsLm5pc3QuZ292L2Rpdjg5OC9oYW5kYm9vay9wbWMvc2VjdGlvbjQvIHBtYzQ0MzEuaHRtDQpFUiAgLQ==');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;"><i>6.4.4.3.1. Seasonal Subseries Plot</i>. .</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=6.4.4.3.1.%20Seasonal%20Subseries%20Plot"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Practical Bayesian Optimization of Machine Learning Algorithms</span> <span class="containertitle"></span>  <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. Snoek, H. Larochelle, and R. P. Adams, “<span class="doctitle">Practical Bayesian Optimization of Machine Learning Algorithms</span>,” p. 9.</div>
  </div><span class='bib-kw' style='display:none;'>XG9M3S5V year__ type__journalArticle ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately‚ this tuning is often a “black art” requiring expert experience‚ rules of thumb‚ or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work‚ we consider this problem through the framework of Bayesian optimization‚ in which a learning algorithm’s generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP‚ such as the type of kernel and the treatment of its hyperparameters‚ can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation‚ structured SVMs and convolutional neural networks.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{snoeknodatepractical,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	language = {en},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	pages = {9}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gUHJhY3RpY2FsIEJheWVzaWFuIE9wdGltaXphdGlvbiBvZiBNYWNoaW5lIExlYXJuaW5nIEFsZ29yaXRobXMNCkFVICAtIFNub2VrLCBKYXNwZXINCkFVICAtIExhcm9jaGVsbGUsIEh1Z28NCkFVICAtIEFkYW1zLCBSeWFuIFANCkFCICAtIFRoZSB1c2Ugb2YgbWFjaGluZSBsZWFybmluZyBhbGdvcml0aG1zIGZyZXF1ZW50bHkgaW52b2x2ZXMgY2FyZWZ1bCB0dW5pbmcgb2YgbGVhcm5pbmcgcGFyYW1ldGVycyBhbmQgbW9kZWwgaHlwZXJwYXJhbWV0ZXJzLiBVbmZvcnR1bmF0ZWx5LCB0aGlzIHR1bmluZyBpcyBvZnRlbiBhIOKAnGJsYWNrIGFydOKAnSByZXF1aXJpbmcgZXhwZXJ0IGV4cGVyaWVuY2UsIHJ1bGVzIG9mIHRodW1iLCBvciBzb21ldGltZXMgYnJ1dGVmb3JjZSBzZWFyY2guIFRoZXJlIGlzIHRoZXJlZm9yZSBncmVhdCBhcHBlYWwgZm9yIGF1dG9tYXRpYyBhcHByb2FjaGVzIHRoYXQgY2FuIG9wdGltaXplIHRoZSBwZXJmb3JtYW5jZSBvZiBhbnkgZ2l2ZW4gbGVhcm5pbmcgYWxnb3JpdGhtIHRvIHRoZSBwcm9ibGVtIGF0IGhhbmQuIEluIHRoaXMgd29yaywgd2UgY29uc2lkZXIgdGhpcyBwcm9ibGVtIHRocm91Z2ggdGhlIGZyYW1ld29yayBvZiBCYXllc2lhbiBvcHRpbWl6YXRpb24sIGluIHdoaWNoIGEgbGVhcm5pbmcgYWxnb3JpdGht4oCZcyBnZW5lcmFsaXphdGlvbiBwZXJmb3JtYW5jZSBpcyBtb2RlbGVkIGFzIGEgc2FtcGxlIGZyb20gYSBHYXVzc2lhbiBwcm9jZXNzIChHUCkuIFdlIHNob3cgdGhhdCBjZXJ0YWluIGNob2ljZXMgZm9yIHRoZSBuYXR1cmUgb2YgdGhlIEdQLCBzdWNoIGFzIHRoZSB0eXBlIG9mIGtlcm5lbCBhbmQgdGhlIHRyZWF0bWVudCBvZiBpdHMgaHlwZXJwYXJhbWV0ZXJzLCBjYW4gcGxheSBhIGNydWNpYWwgcm9sZSBpbiBvYnRhaW5pbmcgYSBnb29kIG9wdGltaXplciB0aGF0IGNhbiBhY2hpZXZlIGV4cGVydGxldmVsIHBlcmZvcm1hbmNlLiBXZSBkZXNjcmliZSBuZXcgYWxnb3JpdGhtcyB0aGF0IHRha2UgaW50byBhY2NvdW50IHRoZSB2YXJpYWJsZSBjb3N0IChkdXJhdGlvbikgb2YgbGVhcm5pbmcgYWxnb3JpdGhtIGV4cGVyaW1lbnRzIGFuZCB0aGF0IGNhbiBsZXZlcmFnZSB0aGUgcHJlc2VuY2Ugb2YgbXVsdGlwbGUgY29yZXMgZm9yIHBhcmFsbGVsIGV4cGVyaW1lbnRhdGlvbi4gV2Ugc2hvdyB0aGF0IHRoZXNlIHByb3Bvc2VkIGFsZ29yaXRobXMgaW1wcm92ZSBvbiBwcmV2aW91cyBhdXRvbWF0aWMgcHJvY2VkdXJlcyBhbmQgY2FuIHJlYWNoIG9yIHN1cnBhc3MgaHVtYW4gZXhwZXJ0LWxldmVsIG9wdGltaXphdGlvbiBmb3IgbWFueSBhbGdvcml0aG1zIGluY2x1ZGluZyBsYXRlbnQgRGlyaWNobGV0IGFsbG9jYXRpb24sIHN0cnVjdHVyZWQgU1ZNcyBhbmQgY29udm9sdXRpb25hbCBuZXVyYWwgbmV0d29ya3MuDQpTUCAgLSA5DQpMQSAgLSBlbg0KRVIgIC0=');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. Snoek, H. Larochelle, and R. P. Adams, “Practical Bayesian Optimization of Machine Learning Algorithms,” p. 9.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Practical%20Bayesian%20Optimization%20of%20Machine%20Learning%20Algorithms&amp;rft.aufirst=Jasper&amp;rft.aulast=Snoek&amp;rft.au=Jasper%20Snoek&amp;rft.au=Hugo%20Larochelle&amp;rft.au=Ryan%20P%20Adams&amp;rft.pages=9"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Random Search for Hyper-Parameter Optimization</span> <span class="containertitle"></span>  <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. Bergstra, Y. Bengio, J. BERGSTRA, and Y. BENGIO, “<span class="doctitle">Random Search for Hyper-Parameter Optimization</span>,” p. 25.</div>
  </div><div class="bib-extra">01474</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__ type__journalArticle ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search‚ we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget‚ random search finds better models by effectively searching a larger‚ less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search‚ purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets‚ and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter‚ but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent “High Throughput” methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{bergstranodaterandom,
	title = {Random {Search} for {Hyper}-{Parameter} {Optimization}},
	language = {en},
	author = {Bergstra, James and Bengio, Yoshua and BERGSTRA, JAMES and BENGIO, YOSHUA},
	note = {01474},
	pages = {25}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('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');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. Bergstra, Y. Bengio, J. BERGSTRA, and Y. BENGIO, “Random Search for Hyper-Parameter Optimization,” p. 25.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Random%20Search%20for%20Hyper-Parameter%20Optimization&amp;rft.aufirst=James&amp;rft.aulast=Bergstra&amp;rft.au=James%20Bergstra&amp;rft.au=Yoshua%20Bengio&amp;rft.au=JAMES%20BERGSTRA&amp;rft.au=YOSHUA%20BENGIO&amp;rft.pages=25"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Understanding the diffculty of training deep feedforward neural networks</span> <span class="containertitle"></span>  <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">X. Glorot and Y. Bengio, “<span class="doctitle">Understanding the diffculty of training deep feedforward neural networks</span>,” p. 8.</div>
  </div><div class="bib-extra">00000</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__ type__journalArticle ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained‚ since then several algorithms have been shown to successfully train them‚ with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks‚ to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value‚ which can drive especially the top hidden layer into saturation. Surprisingly‚ we find that saturated units can move out of saturation by themselves‚ albeit slowly‚ and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally‚ we study how activations and gradients vary across layers and during training‚ with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations‚ we propose a new initialization scheme that brings substantially faster convergence.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{glorotnodateunderstanding,
	title = {Understanding the diffculty of training deep feedforward neural networks},
	language = {en},
	author = {Glorot, Xavier and Bengio, Yoshua},
	note = {00000},
	pages = {8}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('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');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">X. Glorot and Y. Bengio, “Understanding the diffculty of training deep feedforward neural networks,” p. 8.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Understanding%20the%20diffculty%20of%20training%20deep%20feedforward%20neural%20networks&amp;rft.aufirst=Xavier&amp;rft.aulast=Glorot&amp;rft.au=Xavier%20Glorot&amp;rft.au=Yoshua%20Bengio&amp;rft.pages=8"></span></div></div></div></div></li><li class="bib-item"><div class="blink"><a class="shortened"   >&#8862;</a> <span class="doctitle-short">Algorithms for Hyper-Parameter Optimization</span> <span class="containertitle"></span>  <div class="bibshowhide" style="padding-left:20px;"><div class="bib-details"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. S. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl, “<span class="doctitle">Algorithms for Hyper-Parameter Optimization</span>,” p. 9.</div>
  </div><div class="bib-extra">00566</div><span class='bib-kw' style='display:none;'>XG9M3S5V year__ type__journalArticle ""</span></div><div class="blinkitems"><div><div class="blink"><a    >Abstract</a><div class="bibshowhide"><div class="abstract">Several recent advances to the state of the art in image classification benchmarks have come from better configurations of existing techniques rather than novel approaches to feature learning. Traditionally‚ hyper-parameter optimization has been the job of humans because they can be very efficient in regimes where only a few trials are possible. Presently‚ computer clusters and GPU processors make it possible to run more trials and we show that algorithmic approaches can find better results. We present hyper-parameter optimization results on tasks of training neural networks and deep belief networks (DBNs). We optimize hyper-parameters using random search and two new greedy sequential methods based on the expected improvement criterion. Random search has been shown to be sufficiently efficient for learning neural networks for several datasets‚ but we show it is unreliable for training DBNs. The sequential algorithms are applied to the most difficult DBN learning problems from [1] and find significantly better results than the best previously reported. This work contributes novel techniques for making response surface models P (y\textbackslashtextbarx) in which many elements of hyper-parameter assignment (x) are known to be irrelevant given particular values of other elements.</div></div></div><div class="blink"><a    >BIB</a><div class="bibshowhide"><div class="bib">@article{bergstranodatealgorithms,
	title = {Algorithms for {Hyper}-{Parameter} {Optimization}},
	language = {en},
	author = {Bergstra, James S and Bardenet, Rémi and Bengio, Yoshua and Kégl, Balázs},
	note = {00566},
	pages = {9}
}</div></div></div><div class="blink"><a  title="Download RIS/Endnote record"  onclick="dwnD('VFkgIC0gSk9VUg0KVEkgIC0gQWxnb3JpdGhtcyBmb3IgSHlwZXItUGFyYW1ldGVyIE9wdGltaXphdGlvbg0KQVUgIC0gQmVyZ3N0cmEsIEphbWVzIFMNCkFVICAtIEJhcmRlbmV0LCBSw6ltaQ0KQVUgIC0gQmVuZ2lvLCBZb3NodWENCkFVICAtIEvDqWdsLCBCYWzDoXpzDQpBQiAgLSBTZXZlcmFsIHJlY2VudCBhZHZhbmNlcyB0byB0aGUgc3RhdGUgb2YgdGhlIGFydCBpbiBpbWFnZSBjbGFzc2lmaWNhdGlvbiBiZW5jaG1hcmtzIGhhdmUgY29tZSBmcm9tIGJldHRlciBjb25maWd1cmF0aW9ucyBvZiBleGlzdGluZyB0ZWNobmlxdWVzIHJhdGhlciB0aGFuIG5vdmVsIGFwcHJvYWNoZXMgdG8gZmVhdHVyZSBsZWFybmluZy4gVHJhZGl0aW9uYWxseSwgaHlwZXItcGFyYW1ldGVyIG9wdGltaXphdGlvbiBoYXMgYmVlbiB0aGUgam9iIG9mIGh1bWFucyBiZWNhdXNlIHRoZXkgY2FuIGJlIHZlcnkgZWZmaWNpZW50IGluIHJlZ2ltZXMgd2hlcmUgb25seSBhIGZldyB0cmlhbHMgYXJlIHBvc3NpYmxlLiBQcmVzZW50bHksIGNvbXB1dGVyIGNsdXN0ZXJzIGFuZCBHUFUgcHJvY2Vzc29ycyBtYWtlIGl0IHBvc3NpYmxlIHRvIHJ1biBtb3JlIHRyaWFscyBhbmQgd2Ugc2hvdyB0aGF0IGFsZ29yaXRobWljIGFwcHJvYWNoZXMgY2FuIGZpbmQgYmV0dGVyIHJlc3VsdHMuIFdlIHByZXNlbnQgaHlwZXItcGFyYW1ldGVyIG9wdGltaXphdGlvbiByZXN1bHRzIG9uIHRhc2tzIG9mIHRyYWluaW5nIG5ldXJhbCBuZXR3b3JrcyBhbmQgZGVlcCBiZWxpZWYgbmV0d29ya3MgKERCTnMpLiBXZSBvcHRpbWl6ZSBoeXBlci1wYXJhbWV0ZXJzIHVzaW5nIHJhbmRvbSBzZWFyY2ggYW5kIHR3byBuZXcgZ3JlZWR5IHNlcXVlbnRpYWwgbWV0aG9kcyBiYXNlZCBvbiB0aGUgZXhwZWN0ZWQgaW1wcm92ZW1lbnQgY3JpdGVyaW9uLiBSYW5kb20gc2VhcmNoIGhhcyBiZWVuIHNob3duIHRvIGJlIHN1ZmZpY2llbnRseSBlZmZpY2llbnQgZm9yIGxlYXJuaW5nIG5ldXJhbCBuZXR3b3JrcyBmb3Igc2V2ZXJhbCBkYXRhc2V0cywgYnV0IHdlIHNob3cgaXQgaXMgdW5yZWxpYWJsZSBmb3IgdHJhaW5pbmcgREJOcy4gVGhlIHNlcXVlbnRpYWwgYWxnb3JpdGhtcyBhcmUgYXBwbGllZCB0byB0aGUgbW9zdCBkaWZmaWN1bHQgREJOIGxlYXJuaW5nIHByb2JsZW1zIGZyb20gWzFdIGFuZCBmaW5kIHNpZ25pZmljYW50bHkgYmV0dGVyIHJlc3VsdHMgdGhhbiB0aGUgYmVzdCBwcmV2aW91c2x5IHJlcG9ydGVkLiBUaGlzIHdvcmsgY29udHJpYnV0ZXMgbm92ZWwgdGVjaG5pcXVlcyBmb3IgbWFraW5nIHJlc3BvbnNlIHN1cmZhY2UgbW9kZWxzIFAgKHlcdGV4dGJhcngpIGluIHdoaWNoIG1hbnkgZWxlbWVudHMgb2YgaHlwZXItcGFyYW1ldGVyIGFzc2lnbm1lbnQgKHgpIGFyZSBrbm93biB0byBiZSBpcnJlbGV2YW50IGdpdmVuIHBhcnRpY3VsYXIgdmFsdWVzIG9mIG90aGVyIGVsZW1lbnRzLg0KU1AgIC0gOQ0KTEEgIC0gZW4NCkVSICAt');return false;">RIS</a></div><div class="blink"><a    >IEEE</a><div class="bibshowhide"><div class="cite"><div class="csl-entry" style="clear: left;">
    <div class="csl-left-margin" style="float: left; padding-right: 0.5em; text-align: right; width: 1em;">[1]</div><div class="csl-right-inline" style="margin: 0 .4em 0 1.5em;">J. S. Bergstra, R. Bardenet, Y. Bengio, and B. Kégl, “Algorithms for Hyper-Parameter Optimization,” p. 9.</div>
  </div></div></div></div><span class="Z3988" title="url_ver=Z39.88-2004&amp;ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fzotero.org%3A2&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Algorithms%20for%20Hyper-Parameter%20Optimization&amp;rft.aufirst=James%20S&amp;rft.aulast=Bergstra&amp;rft.au=James%20S%20Bergstra&amp;rft.au=R%C3%A9mi%20Bardenet&amp;rft.au=Yoshua%20Bengio&amp;rft.au=Bal%C3%A1zs%20K%C3%A9gl&amp;rft.pages=9"></span></div></div></div></div></li></ol></div><div id="zbw_credits" style="text-align:right;">A <a href="https://github.com/davidswelt/zot_bib_web">zot_bib_web</a> bibliography.</div></div></body></html>